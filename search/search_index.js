var __index = {"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"WebKit Overview","text":"<p>WebKit is a cross-platform web browser engine. On iOS and macOS, it powers Safari, Mail, Apple Books, and many other applications.</p>"},{"location":"index.html#getting-up-and-running","title":"Getting Up and Running","text":""},{"location":"index.html#downloading-the-source-code","title":"Downloading the Source Code","text":"<pre><code>git clone https://github.com/WebKit/WebKit.git WebKit\n</code></pre>"},{"location":"index.html#building-webkit","title":"Building WebKit","text":"<p>Compilation instructions are for macOS. For other platforms additional instructions may be found here.</p> <pre><code>cd WebKit\nTools/Scripts/build-webkit\n</code></pre>"},{"location":"index.html#running-minibrowser","title":"Running Minibrowser","text":"<pre><code>Tools/Scripts/run-minibrowser\n</code></pre>"},{"location":"index.html#optional-add-scripts-directory-to-your-path","title":"(Optional) Add Scripts Directory to your PATH","text":"<pre><code>export PATH=$PATH:`pwd`/Tools/Scripts\n</code></pre>"},{"location":"index.html#contribute","title":"Contribute","text":"<p>Congratulations! You\u2019re up and running. Now you can begin coding in WebKit and contribute your fixes and new features to the project.  For details on submitting your code to the project, read Contributing Code.</p>"},{"location":"index.html#trying-the-latest","title":"Trying the Latest","text":"<p>On macOS, download Safari Technology Preview to test the latest version of WebKit.  On Linux, download Epiphany Technology Preview.  On Windows, you will have to build it yourself.</p>"},{"location":"index.html#reporting-bugs","title":"Reporting Bugs","text":"<ol> <li>Search WebKit Bugzilla to see if there is an existing report for the bug you've encountered.</li> <li>Create a Bugzilla account to to report bugs (and to comment on them) if you haven't done so already.</li> <li>File a bug in accordance with our guidelines.</li> </ol> <p>Once your bug is filed, you will receive email when it is updated at each stage in the bug life cycle.  After the bug is considered fixed, you may be asked to download the latest nightly and confirm that the fix works for you.</p>"},{"location":"index.html#staying-in-touch","title":"Staying in Touch","text":"<p>Before getting in touch with WebKit developers using any of the avenues below, make sure that you have checked our page on how to ask questions about WebKit.</p> <p>You can find WebKit developers, testers, and other interested parties on the #WebKit Slack workspace. Join the WebKit slack, and stay in touch.</p>"},{"location":"Build%20%26%20Debug/BuildOptions.html","title":"Building Options","text":"<p>An in depth guide of build options for WebKit.</p>"},{"location":"Build%20%26%20Debug/BuildOptions.html#building-for-apple-platforms","title":"Building for Apple platforms","text":"<p>Install Xcode and its command line tools if you haven't done so already:</p> <ol> <li>Install Xcode Get Xcode from https://developer.apple.com/downloads. To build WebKit for OS X, Xcode 5.1.1 or later is required. To build WebKit for iOS Simulator, Xcode 7 or later is required.</li> <li>Install the Xcode Command Line Tools In Terminal, run the command: <code>xcode-select --install</code></li> </ol> <p>Run the following command to build a macOS debug build with debugging symbols and assertions:</p> <pre><code>Tools/Scripts/build-webkit --debug\n</code></pre> <p>For performance testing, and other purposes, use <code>--release</code> instead.</p>"},{"location":"Build%20%26%20Debug/BuildOptions.html#embedded-builds","title":"Embedded Builds","text":"<p>To build for an embedded platform like iOS, tvOS, or watchOS, pass a platform argument to <code>build-webkit</code>. </p> <p>For example, to build a debug build with debugging symbols and assertions for embedded simulators:</p> <pre><code>Tools/Scripts/build-webkit --debug --&lt;platform&gt;-simulator\n</code></pre> <p>or embedded devices:</p> <pre><code>Tools/Scripts/build-webkit --debug --&lt;platform&gt;-device\n</code></pre> <p>where <code>platform</code> is <code>ios</code>, <code>tvos</code> or <code>watchos</code>.</p>"},{"location":"Build%20%26%20Debug/BuildOptions.html#using-xcode","title":"Using Xcode","text":"<p>You can open <code>WebKit.xcworkspace</code> to build and debug WebKit within Xcode. Select the \"Everything up to WebKit + Tools\" scheme to build the entire project.</p> <p>If you don't use a custom build location in Xcode preferences, you have to update the workspace settings to use <code>WebKitBuild</code> directory.  In menu bar, choose File &gt; Workspace Settings, then click the Advanced button, select \"Custom\", \"Relative to Workspace\", and enter <code>WebKitBuild</code> for both Products and Intermediates.</p>"},{"location":"Build%20%26%20Debug/BuildOptions.html#building-the-gtk-port","title":"Building the GTK Port","text":"<p>For production builds:</p> <pre><code>cmake -DPORT=GTK -DCMAKE_BUILD_TYPE=RelWithDebInfo -GNinja\nninja\nsudo ninja install\n</code></pre> <p>For development builds:</p> <pre><code>Tools/gtk/install-dependencies\nTools/Scripts/update-webkitgtk-libs\nTools/Scripts/build-webkit --gtk --debug\n</code></pre>"},{"location":"Build%20%26%20Debug/BuildOptions.html#building-the-wpe-port","title":"Building the WPE Port","text":"<p>For production builds:</p> <pre><code>cmake -DPORT=WPE -DCMAKE_BUILD_TYPE=RelWithDebInfo -GNinja\nninja\nsudo ninja install\n</code></pre> <p>For development builds:</p> <pre><code>Tools/wpe/install-dependencies\nTools/Scripts/update-webkitwpe-libs\nTools/Scripts/build-webkit --wpe --debug\n</code></pre>"},{"location":"Build%20%26%20Debug/BuildOptions.html#building-windows-port","title":"Building Windows Port","text":"<p>For building WebKit on Windows, see Windows Port.</p>"},{"location":"Build%20%26%20Debug/BuildOptions.html#running-webkit","title":"Running WebKit","text":""},{"location":"Build%20%26%20Debug/BuildOptions.html#with-safari-and-other-macos-applications","title":"With Safari and Other macOS Applications","text":"<p>Run the following command to launch Safari with your local build of WebKit:</p> <pre><code>Tools/Scripts/run-safari --debug\n</code></pre> <p>The <code>run-safari</code> script sets the <code>DYLD_FRAMEWORK_PATH</code> environment variable to point to your build products, and then launches <code>/Applications/Safari.app</code>. <code>DYLD_FRAMEWORK_PATH</code> tells the system loader to prefer your build products over the frameworks installed in <code>/System/Library/Frameworks</code>.</p> <p>To run other applications with your local build of WebKit, run the following command:</p> <pre><code>Tools/Scripts/run-webkit-app &lt;application-path&gt;\n</code></pre>"},{"location":"Build%20%26%20Debug/BuildOptions.html#ios-simulator","title":"iOS Simulator","text":"<p>Run the following command to launch iOS simulator with your local build of WebKit:</p> <pre><code>run-safari --debug --ios-simulator\n</code></pre> <p>In both cases, if you have built release builds instead, use <code>--release</code> instead of <code>--debug</code>.</p>"},{"location":"Build%20%26%20Debug/BuildOptions.html#linux-ports","title":"Linux Ports","text":"<p>If you have a development build, you can use the run-minibrowser script, e.g.:</p> <pre><code>run-minibrowser --debug --wpe\n</code></pre> <p>Pass one of <code>--gtk</code>, <code>--jsc-only</code>, or <code>--wpe</code> to indicate the port to use.</p>"},{"location":"Build%20%26%20Debug/BuildOptions.html#fixing-mysterious-build-or-runtime-errors-after-xcode-upgrades","title":"Fixing mysterious build or runtime errors after Xcode upgrades","text":"<p>If you see mysterious build failures or if you\u2019ve switched to a new version of macOS or Xcode, delete the <code>WebKitBuild</code> directory. <code>make clean</code> may not delete all the relevant files, and building after doing that without deleting the <code>WebKitBuild</code> directory may result in mysterious build or dyld errors.</p>"},{"location":"Build%20%26%20Debug/BuildOptions.html#building-with-address-sanitizer-to-investigate-memory-corruption-bugs","title":"Building with Address Sanitizer to investigate memory corruption bugs","text":"<p>To build Address Sanitizer or ASan builds to analyze security bugs, run <code>Tools/Scripts/set-webkit-configuration --asan --release</code>. This will enable ASan build. If want to attach a debugger, you can also specify <code>--debug</code> instead of <code>--release</code>. Once you don\u2019t need to build or run ASan anymore, you can specify <code>--no-asan</code> in place of <code>--asan</code> to disable ASan. Note that this configuration is saved by creating a file called Asan in the WebKitBuild directory, so if you are trying to do a clean Asan build by deleting the build directory you need to rerun this command.</p>"},{"location":"Build%20%26%20Debug/BuildOptions.html#building-with-compile_commandsjson","title":"Building with compile_commands.json","text":""},{"location":"Build%20%26%20Debug/BuildOptions.html#macos","title":"macOS","text":"<pre><code>make r EXPORT_COMPILE_COMMANDS=YES\ngenerate-compile-commands WebKitBuild/Release\n</code></pre> <p>I would recommend running this command each time you pull the latest code. If you add or remove files during development, recompile with <code>make r EXPORT_COMPILE_COMMANDS=YES</code> and rerun <code>generate-compile-commands WebKitBuild/Release</code>.</p>"},{"location":"Build%20%26%20Debug/BuildOptions.html#linux-and-windows","title":"Linux and Windows","text":"<pre><code>cmake -DCMAKE_EXPORT_COMPILE_COMMANDS=1\n</code></pre>"},{"location":"Build%20%26%20Debug/DebuggingOnTheCommandLine.html","title":"Debugging With GDB/LLDB","text":""},{"location":"Build%20%26%20Debug/DebuggingOnTheCommandLine.html#overview","title":"Overview","text":"<p>Debugging WebKit can be done through command line debuggers like GDB or LLDB.</p>"},{"location":"Build%20%26%20Debug/DebuggingOnTheCommandLine.html#setting-up-your-environment","title":"Setting up your Environment","text":""},{"location":"Build%20%26%20Debug/DebuggingOnTheCommandLine.html#lldb","title":"LLDB","text":"<p>WebKit provides debug helpers under <code>Tools/lldb/lldb_webkit.py</code>. For automatic loading into LLDB on launch, add the line below to <code>~/.lldbinit</code>.</p> <pre><code>command script import {Path to WebKit}/Tools/lldb/lldb_webkit.py\n</code></pre>"},{"location":"Build%20%26%20Debug/DebuggingOnTheCommandLine.html#gdb","title":"GDB","text":"<p><code>Tools/gdb/webkit.py</code> extends GDB with WebKit-specific knowledge. For automatic loading into GDB on launch, add the lines below to <code>~/.gdbinit</code>.</p> <pre><code>python\nimport sys\nsys.path.insert(0, \"{Path to WebKit}/Tools/gdb/\")\nimport webkit\n</code></pre>"},{"location":"Build%20%26%20Debug/DebuggingOnTheCommandLine.html#manually-debugging-webkit","title":"Manually Debugging WebKit","text":"<p>The helper scripts above provide an easy way to start debugging, but a user can choose to manually launch WebKit using GDB or LLDB directly.</p>"},{"location":"Build%20%26%20Debug/DebuggingOnTheCommandLine.html#lldb_1","title":"LLDB","text":"<pre><code>export DYLD_FRAMEWORK_PATH=WebKitBuild/Debug\nlldb -f WebKitBuild/Debug/DumpRenderTree -- test_file.html\n</code></pre>"},{"location":"Build%20%26%20Debug/DebuggingOnTheCommandLine.html#gdb_1","title":"GDB","text":"<pre><code>export DYLD_FRAMEWORK_PATH=WebKitBuild/Debug\ngdb --args WebKitBuild/Debug/DumpRenderTree test_file.html\n</code></pre>"},{"location":"Build%20%26%20Debug/DebuggingWithVS.html","title":"Debugging With Visual Studio","text":""},{"location":"Build%20%26%20Debug/DebuggingWithVS.html#configuring-debugger","title":"Configuring Debugger","text":"<p>Copy \u200bWebKit.natvis to the appropriate directory, which contains custom views for WebKit fundamental types.</p>"},{"location":"Build%20%26%20Debug/DebuggingWithVS.html#debugging-webkit","title":"Debugging WebKit","text":"<p>There are three ways to debugging WebKit with Visual Studio. Opening the generated WebKit.sln, opening an exe file directly, and attaching running WebKit.</p> <p>Invoke build-webkit with <code>--no-ninja --generate-project-only</code> options, and open by <code>devenv WebKitBuild\\Release\\WebKit.sln</code> or <code>devenv WebKitBuild\\Debug\\WebKit.sln</code> on WebKit command prompt.</p> <p>Set MiniBrowser as the solution's StartUp project. Select the MiniBrowser project in the Solution Explorer, then choose <code>Project &gt; Set as StartUp Project</code>. This will cause the project to turn bold in the Solution Explorer.</p> <p>Launch the debugger Choose <code>Debug &gt; Start Debugging</code>.</p> <p>In Ninja builds, there is no solution files. In such case, open the exe file directly.</p> <pre><code>devenv -debugexe .\\WebKitBuild\\Debug\\bin64\\MiniBrowser.exe\n</code></pre>"},{"location":"Build%20%26%20Debug/DebuggingWithVS.html#debugging-multiple-processes","title":"Debugging Multiple Processes","text":"<p>You can attach a single debugger to more than one process. To do this, launch or attach to the first process, then use Tools &gt; Attach to Process\u2026 or Ctrl+Alt+P to attach to the second process. Your breakpoints will apply to both processes.</p> <p>There is a Visual Studio Extension to attach child processes automatically. \u200bIntroducing the Child Process Debugging Power Tool</p> <p>There are two ways to see which process the debugger is currently operating on, and to switch the current process: the Processes window and the Debug Location toolbar.  You can open the Processes window using <code>Debug &gt; Windows &gt; Processes</code> or <code>Ctrl+Shift+Alt+P</code>. You can show the Debug Location toolbar using View &gt; Toolbars &gt; Debug Location.</p> <p>Visual Studio will always pause all processes (i.e., you can't pause just one process). Similarly, Visual Studio will always step all processes when using the Step In/Over/Out commands.</p>"},{"location":"Build%20%26%20Debug/DebuggingWithVS.html#inspecting-webkit2-api-types","title":"Inspecting WebKit2 API types","text":"<p>You can inspect WebKit2 API types in Visual Studio by casting them to their underlying WebKit2 implementation type. For example, say you have a WKMutableDictionaryRef that points to address 0x12345678 and want to see what it contains. You can view its contents using the following watch expression (in either the Watch Window or Quick Watch Window):</p> <pre><code>{,,WebKit}(WebKit::MutableDictionary*)0x12345678\n</code></pre> <p>The same technique will work for other WebKit2 API types as long as you substitute the appropriate type for MutableDictionary above.</p>"},{"location":"Build%20%26%20Debug/DebuggingWithVS.html#miscellaneous-tips","title":"Miscellaneous Tips","text":"<p>Follow the \u200binstructions for using the Microsoft symbol server so that Visual Studio can show you backtraces that involve closed-source components.</p> <p>Adding $err,hr to the Watch Window will show you what ::GetLastError() would return at this moment, and will show you both the numerical error value and the error string associated with it.</p>"},{"location":"Build%20%26%20Debug/DebuggingWithXcode.html","title":"Debugging With Xcode","text":"<p>You can use Xcode to build &amp; debug WebKit. Open <code>WebKit.xcworkspace</code> at the top level directory.</p> <p>In order to make Xcode use build files built by <code>make</code> command above, go to File &gt; Workspace Settings... &gt; Advanced... &gt; Custom &gt; Relative to Workspace and adjust the relative paths of Products and Intermediates to point to <code>WebKitBuild</code> directory.   Note that debugging WebCore code typically requires attaching to the relevant WebContent process, not the application process, which is mostly running code in Source/WebKit/UIProcess. Depending on what you\u2019re debugging, you\u2019d have to attach &amp; debug different processes in the coalition.</p> <p>You may find it useful to use the debug helpers under <code>Tools/lldb/lldb_webkit.py</code>. This can be added to <code>~/.lldbinit</code> for automatic loading into LLDB on launch by adding the line <code>command script import {Path to WebKit}/Tools/lldb/lldb_webkit.py</code>. For more details, see the Wiki article on lldb formatters.</p> <p>When debugging a debug build in LLDB, there are also a few functions that can be called on objects that will dump debugging info.</p> <ul> <li>RenderObject<ul> <li>showNodeTree()</li> <li>showLineTree()</li> <li>showRenderTree()</li> </ul> </li> <li>Node<ul> <li>showTree()</li> <li>showNodePath()</li> <li>showTreeForThis()</li> <li>showNodePathForThis()</li> </ul> </li> </ul>"},{"location":"Build%20%26%20Debug/DebuggingWithXcode.html#debugging-layout-tests","title":"Debugging Layout Tests","text":"<p>The easiest way to debug a layout test is with WebKitTestRunner or DumpRenderTree. In Product &gt; Scheme, select \u201cAll Source\u201d.</p> <p>In Product &gt; Scheme &gt; Edit Scheme, open \u201cRun\u201d tab. Pick WebKitTestRunner or DumpRenderTree, whichever is desired in \u201cExecutable\u201d.</p> <p> Go to Arguments and specify the path to the layout tests being debugged relative to where the build directory is located. e.g. <code>../../LayoutTests/fast/dom/Element/element-traversal.html</code> if <code>WebKitBuild/Debug</code> is the build directory.  You may want to specify OS_ACTIVITY_MODE environmental variable to \u201cdisable\u201d in order to suppress all the system logging that happens during the debugging session.</p> <p>You may also want to specify <code>--no-timeout</code> option to prevent WebKitTestRunner or DumpRenderTree to stop the test after 30 seconds if you\u2019re stepping through code.</p> <p>Once this is done, you can run WebKitTestRunner or DumpRenderTree by going to Product &gt; Perform Action &gt; Run without Building.</p> <p>Clicking on \u201cRun\u201d button may be significantly slower due to Xcode re-building every project and framework each time. You can disable this behavior by going to \u201cBuild\u201d tab and unchecking boxes for all the frameworks involved for \u201cRun\u201d: </p>"},{"location":"Build%20%26%20Debug/DebuggingWithXcode.html#attaching-to-webcontent-process","title":"Attaching to WebContent Process","text":"<p>You may find Xcode fails to attach to WebContent or Networking process in the case of WebKitTestRunner. In those cases, attach a breakpoint in UIProcess code such as <code>TestController::runTest</code> in WebKitTestRunner right before <code>TestInvocation::invoke</code> is called. Once breakpoint is hit in the UIProcess, attach to <code>WebContent.Development</code> or <code>Networking.Development</code> process manually in Xcode via Debug &gt; Attach to Process.</p>"},{"location":"Build%20%26%20Debug/Logging.html","title":"Logging","text":""},{"location":"Build%20%26%20Debug/Logging.html#setup","title":"Setup","text":"<p>Each framework (WebCore, WebKit, WebKitLegacy, WTF) enable their own logging infrastructure independently (though the infrastructure itself is shared). If you want to log a message, <code>#include</code> the relevant framework's <code>Logging.h</code> header. Then, you can use the macros below.</p> <p>Beware that you can't <code>#include</code> multiple framework's <code>Logging.h</code> headers at the same time - they each define a macro <code>LOG_CHANNEL_PREFIX</code> which will conflict with each other. Only <code>#include</code> the <code>Logging.h</code> header from your specific framework.</p> <p>If you want to do more advanced operations, like searching through the list of log channels, <code>#include</code> your framework's <code>LogInitialization.h</code> header. These do not conflict across frameworks, so you can do something like</p> <pre><code>#include \"LogInitialization.h\"\n#include &lt;WebCore/LogInitialization.h&gt;\n#include &lt;WTF/LogInitialization.h&gt;\n</code></pre> <p>Indeed, WebKit does this to initialize all frameworks' log channels during Web Process startup.</p>"},{"location":"Build%20%26%20Debug/Logging.html#logging-messages","title":"Logging messages","text":"<p>There are a few relevant macros for logging messages:</p> <ul> <li><code>LOG()</code>: Log a printf-style message in debug builds. Requires you to name a logging channel to output to.</li> <li><code>LOG_WITH_STREAM()</code> Log an iostream-style message in debug builds. Requires you to name a logging channel to output to.</li> <li><code>RELEASE_LOG()</code>: Just like <code>LOG()</code> but logs in both debug and release builds. Requires you to name a logging channel to output to.</li> <li><code>WTFLogAlways()</code>: Mainly for local debugging, unconditionally output a message. Does not require a logging channel to output to.</li> </ul> <p>Here's an example invocation of <code>LOG()</code>:</p> <pre><code>LOG(MediaQueries, \"HTMLMediaElement %p selectNextSourceChild evaluating media queries\", this);\n</code></pre> <p>That first argument is a log channel. These have 2 purposes:</p> <ul> <li>Individual channels can be enabled/disabled independently (So you can get all the WebGL logging without getting any Loading logging)</li> <li>When multiple channels are enabled, and you're viewing the logs, you can search/filter by the channel</li> </ul> <p>Here's an example invocation of <code>LOG_WITH_STREAM()</code>:</p> <pre><code>LOG_WITH_STREAM(Scrolling, stream &lt;&lt; \"ScrollingTree::commitTreeState - removing unvisited node \" &lt;&lt; nodeID);\n</code></pre> <p>The macro sets up a local variable named <code>stream</code> which the second argument can direct messages to. The second argument is a collection of statements - not expressions like <code>LOG()</code> and <code>RELEASE_LOG()</code>. So, you can do things like this:</p> <pre><code>LOG_WITH_STREAM(TheLogChannel,\n    for (const auto&amp; something : stuffToLog)\n        stream &lt;&lt; \" \" &lt;&lt; something;\n);\n</code></pre> <p>The reason why (most of) these use macros is so the entire thing can be compiled out when logging is disabled. Consider this:</p> <pre><code>LOG(TheLogChannel, \"The result is %d\", someSuperComplicatedCalculation());\n</code></pre> <p>If these were not macros, you'd have to pay for <code>someSuperComplicatedCalculation()</code> whether logging is enabled or not.</p>"},{"location":"Build%20%26%20Debug/Logging.html#enabling-and-disabling-log-channels","title":"Enabling and disabling log channels","text":"<p>Channels are enabled/disabled at startup by passing a carefully crafted string to <code>initializeLogChannelsIfNecessary()</code>. On the macOS and iOS ports, this string comes from the defaults database. On other UNIX systems and Windows, it comes from environment variables.</p> <p>You can read the grammar of this string in <code>initializeLogChannelsIfNecessary()</code>. Here is an example:</p> <pre><code>WebGL -Loading\n</code></pre> <p>You can also specify the string <code>all</code> to enable all logging.</p> <p>On macOS/iOS and Windows, each framework has its own individually supplied string that it uses to enable its own logging channels. On Linux, all frameworks share the same string.</p>"},{"location":"Build%20%26%20Debug/Logging.html#linux","title":"Linux","text":"<p>Set the <code>WEBKIT_DEBUG</code> environment variable.</p> <pre><code>WEBKIT_DEBUG=Scrolling Tools/Scripts/run-minibrowser --gtk --debug\n</code></pre>"},{"location":"Build%20%26%20Debug/Logging.html#macos","title":"macOS","text":"<p>On macOS, you can, for example, enable the <code>Language</code> log channel with these terminal commands:</p> <pre><code>for identifier in com.apple.WebKit.WebContent.Development com.apple.WebKit.WebContent org.webkit.MiniBrowser com.apple.WebKit.WebKitTestRunner org.webkit.DumpRenderTree -g /Users/$USER/Library/Containers/com.apple.Safari/Data/Library/Preferences/com.apple.Safari.plist; do\n    for key in WTFLogging WebCoreLogging WebKitLogging WebKit2Logging; do\n        defaults write ${identifier} \"${key}\" \"Language\"\n    done\ndone\n</code></pre> <p>You may also need to specify these strings to <code>com.apple.WebKit.WebContent.Development</code>, the global domain, or the Safari container, depending on what you're running.</p> <p>You may also pass this key and value as an argument:</p> <pre><code>Tools/Scripts/run-minibrowser --debug -WebCoreLogging Scrolling\n</code></pre>"},{"location":"Build%20%26%20Debug/Logging.html#windows","title":"Windows","text":"<p>Set the <code>WTFLogging</code>, <code>WebCoreLogging</code> or <code>WebKit2Logging</code> environment variables.</p> <p>It outputs logs both to stderr and <code>OutputDebugString</code> on Windows. Console applications (jsc.exe, WebKitTestRunner.exe, etc) can use stderr, but GUI applications (MiniBrowser.exe). Attach a debugger to see the message of <code>OutputDebugString</code>. Use \u200bChild Process Debugging Power Tool to automatically attach child processes.</p>"},{"location":"Build%20%26%20Debug/Logging.html#adding-a-new-log-channel","title":"Adding a new log channel","text":"<p>Simply add a line to your framework's <code>Logging.h</code> header. Depending on how the accompanying <code>Logging.cpp</code> file is set up, you may need to add a parallel line there. That should be all you need. It is acceptable to have log channels in different frameworks with the same name - this is what <code>LOG_CHANNEL_PREFIX</code> is for.</p>"},{"location":"Build%20%26%20Debug/Logging.html#javascriptcore-and-datalog","title":"JavaScriptCore and dataLog","text":"<p>WebKit has another logging infrastructure <code>dataLog</code>. JavaScriptCore is mainly using it. To enable the JSC logging, set a environment variable or give a command switch to <code>jsc</code>. For example, <code>JSC_logGC=2</code> or <code>run-jsc --logGC=2</code>. Give <code>--debug</code> switch to <code>run-jsc</code> script if you build a debug build.</p> <p>Invoking <code>run-jsc --options</code> lists all options and possible values. On Windows, invoke <code>perl Tools/Scripts/run-jsc --options</code> or <code>WebKitBuild/Release/bin64/jsc.exe --options</code>.</p>"},{"location":"Build%20%26%20Debug/Tests.html","title":"Testing","text":"<p>A deep dive into WebKit's Tests.</p>"},{"location":"Build%20%26%20Debug/Tests.html#correctness-testing-in-webkit","title":"Correctness Testing in WebKit","text":"<p>WebKit is really big on test driven development, we have many types of tests.</p> <ul> <li>JavaScript tests - Resides in top-level JSTests directory.     This is the primary method of testing JavaScriptCore. Use <code>Tools/Scripts/run-javascriptcore-tests</code> to run these tests.</li> <li>Layout tests - Resides in top-level LayoutTests directory.     This is the primary method of testing WebCore.     If you\u2019re making code changes to WebCore, you typically run these tests. Use <code>Tools/Scripts/run-webkit-tests</code> to run these.     Pass <code>-1</code> to run tests using WebKitLegacy (a.k.a. WebKit1).     WebKitTestRunner is used to run these tests for WebKit2,     and DumpRenderTree is used to these tests for WebKit1.     There are a few styles of layout tests but all of them have a test file and expected result (ends with -expected.txt),     and the test passes if the test file\u2019s output matches that of the expected result.</li> <li>API tests - Reside in Tools/TestWebKitAPI,     these are GTests that test APIs exposed by JavaScriptCore,     WebKitLegacy, and WebKit layers as well as unit tests for selected WTF classes.     WebKit does not use XCTests.     Use <code>Tools/Scripts/run-api-tests</code> to run these tests.     Because these API tests are sequentially, it\u2019s preferable to write layout tests when possible.</li> <li>Bindings tests - Reside in Source/WebCore/bindings/scripts/test,     these are tests for WebCore\u2019s binding code generator.     Use <code>Tools/Scripts/run-bindings-tests</code> to run these tests.</li> <li>webkitpy tests - Tests for WebKit\u2019s various Python scripts in Tools/Scripts/webkitpy.     Use <code>Tools/Scripts/test-webkitpy</code> to run these tests.</li> <li>webkitperl tests - Tests for WebKit\u2019s various Perl scripts in Tools/Scripts/webkitperl.     Use <code>Tools/Scripts/test-webkitperl</code> to run these tests.</li> </ul>"},{"location":"Build%20%26%20Debug/Tests.html#performance-testing-in-webkit","title":"Performance Testing in WebKit","text":"<p>The WebKit project has a \"no performance regression\" policy. We maintain the performance of the following of the benchmarks and are located under PerformanceTests. If your patch regresses one of these benchmarks even slightly (less than 1%), it will get reverted.</p> <ul> <li>JetStream2 - Measures JavaScript and WASM performance.</li> <li>MotionMark - Measures graphics performance.</li> <li>Speedometer 3 - Measures WebKit\u2019s performance for complex web apps.</li> </ul> <p>The following are benchmarks maintained by Apple's WebKit team but not available to other open source contributors since Apple doesn't have the right to redistribute the content. If your WebKit patch regresses one of these tests, your patch may still get reverted.</p> <ul> <li>RAMification - Apple's internal JavaScript memory benchmark.</li> <li>ScrollPerf - Apple's internal scrolling performance tests.</li> <li>PLT - Apple's internal page load time tests.</li> <li>Membuster / PLUM - Apple's internal memory tests. Membuster for macOS and PLUM for iOS and iPadOS.</li> </ul>"},{"location":"Build%20%26%20Debug/Tests.html#layout-tests-tests-of-the-web-for-the-web","title":"Layout Tests: Tests of the Web for the Web","text":"<p>Layout tests are WebKit tests written using Web technology such as HTML, CSS, and JavaScript, and it\u2019s the primary mechanism by which much of WebCore is tested. Relevant layout test should be ran while you\u2019re making code changes to WebCore and before uploading a patch to bugs.webkit.org. While bugs.webkit.org\u2019s Early Warning System will build and run tests on a set of configurations, individual patch authors are ultimately responsible for any test failures that their patches cause.</p>"},{"location":"Build%20%26%20Debug/Tests.html#test-files-and-expected-files","title":"Test Files and Expected Files","text":""},{"location":"Build%20%26%20Debug/Tests.html#directory-structure","title":"Directory Structure","text":"<p>LayoutTests directory is organized by the category of tests. For example, LayoutTests/accessibility contains accessibility related tests, and LayoutTests/fast/dom/HTMLAnchorElement contains tests for the HTML anchor element.</p> <p>Any file that ends in <code>.html</code>, <code>.htm</code>, <code>.shtml</code>, <code>.xhtml</code>, <code>.mht</code>, <code>.xht</code>, <code>.xml</code>, <code>.svg</code>, or <code>.php</code> is considered as a test unless it\u2019s preceded with <code>-ref</code>, <code>-notref</code>, <code>-expected</code>, or <code>-expected-mismatch</code> (these are used for ref tests; explained later). It\u2019s accompanied by another file of the same name except it ends in <code>-expected.txt</code> or <code>-expected.png</code>. These are called expected results and constitutes the baseline output of a given test. When layout tests are ran, the test runner generates an output in the form of a plain text file and/or an PNG image, and it is compared against these expected results.</p> <p>In the case expected results may differ from one platform to another, the expected results for each test is stored in LayoutTests/platform. The expected result of a given test exists in the corresponding directory in each subdirectory of LayoutTests/platform. For example, the expected result of LayoutTests/svg/W3C-SVG-1.1/animate-elem-46-t.svg for macOS Mojave is located at LayoutTests/platform/mac-mojave/svg/W3C-SVG-1.1/animate-elem-46-t-expected.txt.</p> <p>These platform directories have a fallback order. For example, running tests for WebKit2 on macOS Catalina will use the following fallback path from the most specific to most generic:</p> <ul> <li>platform/mac-catalina-wk2 - Results for WebKit2 on macOS Catalina.</li> <li>platform/mac-catalina - Results for WebKit2 and WebKitLegacy on macOS Catalina.</li> <li>platform/mac-wk2 - Results for WebKit2 on all macOS.</li> <li>platform/mac - Results for all macOS.</li> <li>platform/wk2 - Results for WebKit2 on every operating system.</li> <li>generic - Next to the test file.</li> </ul>"},{"location":"Build%20%26%20Debug/Tests.html#imported-tests","title":"Imported Tests","text":"<p>Tests under LayoutTests/imported are imported from other repositories. They should not be modified by WebKit patches unless the change is made in respective repositories first.</p> <p>Most notable is Web Platform Tests, which are imported under LayoutTests/imported/w3c/web-platform-tests. These are cross browser vendor tests developed by W3C. Mozilla, Google, and Apple all contribute many tests to this shared test repository.</p>"},{"location":"Build%20%26%20Debug/Tests.html#http-tests","title":"HTTP Tests","text":"<p>To open tests under LayoutTests/http or LayoutTests/imported/w3c/web-platform-tests, use Tools/Scripts/open-layout-test with the path to a test.</p> <p>You can also manually start HTTP servers with <code>Tools/Scripts/run-webkit-httpd</code>. To stop the HTTP servers, exit the script (e.g. Control + C on macOS).</p> <p>Tests under LayoutTests/http are accessible at http://127.0.0.1:8000 except tests in LayoutTests/http/wpt, which are available at http://localhost:8800/WebKit/ instead.</p> <p>The Web Platform Tests imported under LayoutTests/imported/w3c/web-platform-tests are accessible under HTTP at http://localhost:8800/ and HTTPS at http://localhost:9443/</p> <p>Note that it's important to use the exact host names such as <code>127.0.0.1</code> and <code>localhost</code> above verbatim since some tests rely on or test same-origin or cross-origin behaviors based on those host names.</p>"},{"location":"Build%20%26%20Debug/Tests.html#test-expectations","title":"Test Expectations","text":"<p>FIXME: Explain how test expectations work.</p>"},{"location":"Build%20%26%20Debug/Tests.html#running-layout-tests","title":"Running Layout Tests","text":"<p>To run layout tests, use <code>Tools/Scripts/run-webkit-tests</code>. It optionally takes file paths to a test file or directory and options on how to run a test. For example, in order to just run <code>LayoutTests/fast/dom/Element/element-traversal.html</code>, do:</p> <pre><code>Tools/Scripts/run-webkit-tests fast/dom/Element/element-traversal.html\n</code></pre> <p>Because there are 50,000+ tests in WebKit, you typically want to run a subset of tests that are relevant to your code change (e.g. <code>LayoutTests/storage/indexeddb/</code> if you\u2019re working on IndexedDB) while developing the code change, and run all layout tests at the end on your local machine or rely on the Early Warning System on bugs.webkit.org for more thorough testing.</p> <p>Specify <code>--debug</code> or <code>--release</code> to use either release or debug build. To run tests using iOS simulator, you can specify either <code>--ios-simulator</code>, <code>--iphone-simulator</code>, or <code>--ipad-simulator</code> based on whichever simulator is desired.</p> <p>By default, <code>run-webkit-tests</code> will run all the tests you specified once in the lexicological order of test paths relative to <code>LayoutTests</code> directory and retry any tests that have failed. If you know the test is going to fail and don\u2019t want retries, specify <code>--no-retry-failures</code>.</p> <p>Because there are so many tests, <code>run-webkit-tests</code> will runs tests in different directories in parallel (i.e. all tests in a single directory is ran sequentially one after another). You can control the number of parallel test runners using <code>--child-processes</code> option.</p> <p><code>run-webkit-tests</code> has many options. Use <code>--help</code> to enumerate all the supported options.</p>"},{"location":"Build%20%26%20Debug/Tests.html#repeating-layout-tests","title":"Repeating Layout Tests","text":"<p>When you\u2019re investigating flaky tests or crashes, it might be desirable to adjust this. <code>--iterations X</code> option will specify the number of times the list of tests are ran. For example, if we are running tests A, B, C and <code>--iterations 3</code> is specified, <code>run-webkit-tests</code> will run: A, B, C, A, B, C, A, B, C. Similarly, <code>--repeat-each</code> option will specify the number of times each test is repeated before moving onto next test. For example, if we\u2019re running tests A, B, C, and <code>--repeat-each 3</code> is specified, <code>run-webkit-tests</code> will run: A, A, A, B, B, B, C, C, C. <code>--exit-after-n-failures</code> option will specify the total number of test failures before <code>run-webkit-tests</code> will stop. In particular, <code>--exit-after-n-failures=1</code> is useful when investigating a flaky failure so that <code>run-webkit-tests</code> will stop when the failure actually happens for the first time.</p>"},{"location":"Build%20%26%20Debug/Tests.html#test-results","title":"Test Results","text":"<p>Whenever tests do fail, run-webkit-tests will store results in <code>WebKitBuild/Debug/layout-test-results</code> mirroring the same directory structure as <code>LayoutTests</code>. For example, the actual output produced for <code>LayoutTests/editing/inserting/typing-001.html</code>, if failed, will appear in <code>WebKitBuild/Debug/layout-test-results/editing/inserting/typing-001-actual.txt</code>. run-webkit-tests also generates a web page with the summary of results in <code>WebKitBuild/Debug/layout-test-results/results.html</code> and automatically tries to open it in Safari using the local build of WebKit.</p> <p>If Safari fails to launch, specify <code>--no-show-results</code> and open results.html file manually.</p>"},{"location":"Build%20%26%20Debug/Tests.html#updating-expected-results","title":"Updating Expected Results","text":"<p>If you\u2019ve updated a test content or test\u2019s output changes with your code change (e.g. more test case passes), then you may have to update <code>-expected.txt</code> file accompanying the test. To do that, first run the test once to make sure the diff and new output makes sense in results.html, and run the test again with <code>--reset-results</code>. This will update the matching <code>-expected.txt</code> file.</p> <p>You may need to manually copy the new result to other -expected.txt files that exist under <code>LayoutTests</code> for other platforms and configurations. Find other <code>-expected.txt</code> files when you\u2019re doing this.</p> <p>When a new test is added, <code>run-webkit-tests</code> will automatically generate new <code>-expected.txt</code> file for your test. You can disable this feature by specifying <code>--no-new-test-results</code> e.g. when the test is still under development.</p>"},{"location":"Build%20%26%20Debug/Tests.html#different-styles-of-layout-tests","title":"Different Styles of Layout Tests","text":"<p>There are multiple styles of layout tests in WebKit.</p>"},{"location":"Build%20%26%20Debug/Tests.html#render-tree-dumps","title":"Render tree dumps","text":"<p>This is the oldest style of layout tests, and the default mode of layout tests. It\u2019s a text serialization of WebKit\u2019s render tree and its output looks like this:</p> <pre><code>layer at (0,0) size 800x600\n  RenderView at (0,0) size 800x600\nlayer at (0,0) size 800x600\n  RenderBlock {HTML} at (0,0) size 800x600\n    RenderBody {BODY} at (8,8) size 784x584\n      RenderInline {A} at (0,0) size 238x18 [color=#0000EE]\n        RenderInline {B} at (0,0) size 238x18\n          RenderText {#text} at (0,0) size 238x18\n            text run at (0,0) width 238: \"the second copy should not be bold\"\n      RenderText {#text} at (237,0) size 5x18\n        text run at (237,0) width 5: \" \"\n      RenderText {#text} at (241,0) size 227x18\n        text run at (241,0) width 227: \"the second copy should not be bold\"\n</code></pre> <p>This style of layout tests is discouraged today because its outputs are highly dependent on each platform, and end up requiring a specific expected result in each platform. But they\u2019re still useful when testing new rendering and layout feature or bugs thereof.</p> <p>These tests also have accompanying <code>-expected.png</code> files but <code>run-webkit-tests</code> doesn't check the PNG output against the expected result by default. To do this check, pass <code>--pixel</code>. Unfortunately, many pixel tests will fail because we have not been updating the expected PNG results a good chunk of the last decade. However, these pixel results might be useful when diagnosing a new test failure. For this reason, <code>run-webkit-tests</code> will automatically generate PNG results when retrying the test, effectively enabling <code>--pixel</code> option for retries.</p>"},{"location":"Build%20%26%20Debug/Tests.html#dumpastext-test","title":"dumpAsText test","text":"<p>These are tests that uses the plain text serialization of the test page as the output (as if the entire page\u2019s content is copied as plain text). All these tests call <code>testRunner.dumpAsText</code> to trigger this behavior. The output typically contains a log of text or other informative output scripts in the page produced. For example, LayoutTests/fast/dom/anchor-toString.html is written as follows:</p> <pre><code>&lt;a href=\"http://localhost/sometestfile.html\" id=\"anchor\"&gt;\nA link!\n&lt;/a&gt;\n&lt;br&gt;\n&lt;br&gt;\n&lt;script&gt;\n    {\n        if (window.testRunner)\n            testRunner.dumpAsText();\n\n        var anchor = document.getElementById(\"anchor\");\n        document.write(\"Writing just the anchor object - \" + anchor);\n\n        var anchorString = String(anchor);\n        document.write(\"&lt;br&gt;&lt;br&gt;Writing the result of the String(anchor) - \" + anchorString);\n\n        var anchorToString = anchor.toString();\n        document.write(\"&lt;br&gt;&lt;br&gt;Writing the result of the anchor's toString() method - \" + anchorToString);\n    }\n&lt;/script&gt;\n</code></pre> <p>and generates the following output:</p> <pre><code>A link! \n\nWriting just the anchor object - http://localhost/sometestfile.html\n\nWriting the result of the String(anchor) - http://localhost/sometestfile.html\n\nWriting the result of the anchor's toString() method - http://localhost/sometestfile.html\n</code></pre>"},{"location":"Build%20%26%20Debug/Tests.html#js-testjs-and-js-test-prejs-tests","title":"js-test.js and js-test-pre.js tests","text":"<p>These are variants of dumpAsText test which uses WebKit\u2019s assertion library: LayoutTests/resources/js-test.js and LayoutTests/resources/js-test-pre.js. It consists of shouldX function calls which takes two JavaScript code snippet which are then executed and outputs of which are compared. js-test.js is simply a new variant of js-test-pre.js that doesn\u2019t require the inclusion of LayoutTests/resources/js-test-post.js at the end. Use js-test.js in new tests, not js-test-pre.js.</p> <p>For example, LayoutTests/fast/dom/Comment/remove.html which tests remove() method on Comment node is written as:</p> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;script src=\"../../../resources/js-test-pre.js\"&gt;&lt;/script&gt;\n&lt;div id=\"test\"&gt;&lt;/div&gt;\n&lt;script&gt;\n\ndescription('This tests the DOM 4 remove method on a Comment.');\n\nvar testDiv = document.getElementById('test');\nvar comment = document.createComment('Comment');\ntestDiv.appendChild(comment);\nshouldBe('testDiv.childNodes.length', '1');\ncomment.remove();\nshouldBe('testDiv.childNodes.length', '0');\ncomment.remove();\nshouldBe('testDiv.childNodes.length', '0');\n\n&lt;/script&gt;\n&lt;script src=\"../../../resources/js-test-post.js\"&gt;&lt;/script&gt;\n</code></pre> <p>with the following expected result (output):</p> <pre><code>This tests the DOM 4 remove method on a Comment.\n\nOn success, you will see a series of \"PASS\" messages, followed by \"TEST COMPLETE\".\n\n\nPASS testDiv.childNodes.length is 1\nPASS testDiv.childNodes.length is 0\nPASS testDiv.childNodes.length is 0\nPASS successfullyParsed is true\n\nTEST COMPLETE\n</code></pre> <p><code>description</code> function specifies the description of this test, and subsequent shouldBe calls takes two strings, both of which are evaluated as JavaScript and then compared.</p> <p>Some old js-test-pre.js tests may put its test code in a separate JS file but we don\u2019t do that anymore to keep all the test code in one place.</p> <p>js-test.js and js-test-pre.js provide all kinds of other assertion and helper functions. Here are some examples:</p> <ul> <li><code>debug(msg)</code> - Inserts a debug / log string in the output.</li> <li><code>evalAndLog(code)</code> - Similar to <code>debug()</code> but evaluates code as JavaScript.</li> <li><code>shouldNotBe(a, b)</code> - Generates <code>PASS</code> if the results of evaluating <code>a</code> and <code>b</code> differ.</li> <li><code>shouldBeTrue(code)</code> - Shorthand for <code>shouldBe(code, 'true')</code>.</li> <li><code>shouldBeFalse(code)</code> - Shorthand for <code>shouldBe(code, 'false')</code>.</li> <li><code>shouldBeNaN(code)</code> - Shorthand for <code>shouldBe(code, 'NaN')</code>.</li> <li><code>shouldBeNull(code)</code> - Shorthand for <code>shouldBe(code, 'null')</code>.</li> <li><code>shouldBeZero(code)</code> - Shorthand for <code>shouldBe(code, '0')</code>.</li> <li><code>shouldBeEqualToString(code, string)</code> - Similar to <code>shouldBe</code> but the second argument is not evaluated as string.</li> <li><code>finishJSTest()</code> - When js-test.js style test needs to do some async work, define the global variable named jsTestIsAsync and set it to true. When the test is done, call this function to notify the test runner (don\u2019t call <code>testRunner.notifyDone</code> mentioned later directly). See an example.</li> </ul> <p>It\u2019s important to note that these shouldX functions only add output strings that say PASS or FAIL. If the expected result also contains the same FAIL strings, then run-webkit-tests will consider the whole test file to have passed.</p> <p>Another way to think about this is that <code>-expected.txt</code> files are baseline outputs, and baseline outputs can contain known failures.</p> <p>There is a helper script to create a template for a new js-test.js test. The following will create new test named <code>new-test.html</code> in LayoutTests/fast/dom:</p> <pre><code>Tools/Scripts/make-new-script-test fast/dom/new-test.html\n</code></pre>"},{"location":"Build%20%26%20Debug/Tests.html#dump-as-markupjs-tests","title":"dump-as-markup.js Tests","text":"<p>A dump-as-markup.js test is yet another variant of dumpAsText test, which uses LayoutTests/resources/dump-as-markup.js. This style of test is used when it\u2019s desirable to compare the state of the DOM tree before and after some operations. For example, many tests under LayoutTests/editing use this style of testing to test complex DOM mutation operations such as pasting HTML from the users\u2019 clipboard. dump-as-markup.js adds <code>Markup</code> on the global object and exposes a few helper functions. Like js-test.js tests, a test description can be specified via <code>Markup.description</code>. The test then involves <code>Markup.dump(node, description)</code> to serialize the state of DOM tree as plain text where <code>element</code> is either a DOM node under which the state should be serialized or its id.</p> <p>For example, LayoutTests/editing/inserting/insert-list-in-table-cell-01.html is written as follows:</p> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;div id=\"container\" contenteditable=\"true\"&gt;&lt;table border=\"1\"&gt;&lt;tr&gt;&lt;td id=\"element\"&gt;fsdf&lt;/td&gt;&lt;td&gt;fsdf&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;gghfg&lt;/td&gt;&lt;td&gt;fsfg&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;\n&lt;script src=\"../editing.js\"&gt;&lt;/script&gt;\n&lt;script src=\"../../resources/dump-as-markup.js\"&gt;&lt;/script&gt;\n&lt;script&gt;\n    Markup.description('Insert list items in a single table cell:');\n\n    var e = document.getElementById(\"element\");\n    setSelectionCommand(e, 0, e, 1);\n    Markup.dump('container', 'Before');\n\n    document.execCommand(\"insertOrderedList\");\n    Markup.dump('container', 'After');\n&lt;/script&gt;\n</code></pre> <p>with the following expected result:</p> <pre><code>Insert list items in a single table cell:\n\nBefore:\n| &lt;table&gt;\n|   border=\"1\"\n|   &lt;tbody&gt;\n|     &lt;tr&gt;\n|       &lt;td&gt;\n|         id=\"element\"\n|         \"&lt;#selection-anchor&gt;fsdf&lt;#selection-focus&gt;\"\n|       &lt;td&gt;\n|         \"fsdf\"\n|     &lt;tr&gt;\n|       &lt;td&gt;\n|         \"gghfg\"\n|       &lt;td&gt;\n|         \"fsfg\"\n\nAfter:\n| &lt;table&gt;\n|   border=\"1\"\n|   &lt;tbody&gt;\n|     &lt;tr&gt;\n|       &lt;td&gt;\n|         id=\"element\"\n|         &lt;ol&gt;\n|           &lt;li&gt;\n|             \"&lt;#selection-anchor&gt;fsdf&lt;#selection-focus&gt;\"\n|             &lt;br&gt;\n|       &lt;td&gt;\n|         \"fsdf\"\n|     &lt;tr&gt;\n|       &lt;td&gt;\n|         \"gghfg\"\n|       &lt;td&gt;\n|         \"fsfg\"\n</code></pre>"},{"location":"Build%20%26%20Debug/Tests.html#testharnessjs-tests","title":"testharness.js Tests","text":"<p>This is yet another variant of dumpAsText test which uses the test harness of Web Platform Tests,  which is W3C\u2019s official tests for the Web. There is an extensive documentation on how this harness works.</p> <p>As mentioned above, do not modify tests in LayoutTests/imported/w3c/web-platform-tests unless the same test changes are made in Web Platform Tests\u2019 primary repository.</p>"},{"location":"Build%20%26%20Debug/Tests.html#reference-tests","title":"Reference Tests","text":"<p>Reference tests are special in that they don\u2019t have accompanying <code>-expected.txt</code> files. Instead, they have a matching or mismatching expected result file. Both the test file and the accompanying matching or mismatching expected result generate PNG outputs. The test passes if the PNG outputs of the test and the matching expected result are the same; the test fails otherwise. For a test with a mismatching expected result, the test passes if the PNG outputs of the test and the mismatching expected result are not the same, and fails if they are the same.</p> <p>A matching expected result or a mismatching expected result can be specified in a few ways:</p> <ul> <li>The file with the same name as the test name except it ends with  <code>-expected.*</code> or <code>-ref.*</code> is a matching expected result for the test.</li> <li>The file with the same name as the test name except it ends with  <code>-expected-mismatch.*</code> or <code>-notref.*</code> is a matching expected result for the test.</li> <li>The file specified by a HTML link element in the test file with <code>match</code> relation: <code>&lt;link rel=match href=X&gt;</code> where X is the relative file path is a matching expected result.</li> <li>The file specified by a HTML link element in the test file with <code>mismatch</code> relation: <code>&lt;link rel=mismatch href=X&gt;</code> where X is the relative file path is a mismatching expected result.</li> </ul> <p>For example, LayoutTests/imported/w3c/web-platform-tests/html/rendering/replaced-elements/images/space.html specifies space-ref.html in the same directory as the matching expected result as follows:</p> <pre><code>&lt;!doctype html&gt;\n&lt;meta charset=utf-8&gt;\n&lt;title&gt;img hspace/vspace&lt;/title&gt;\n&lt;link rel=match href=space-ref.html&gt;\n&lt;style&gt;\nspan { background: blue; }\n&lt;/style&gt;\n&lt;div style=width:400px;&gt;\n&lt;p&gt;&lt;span&gt;&lt;img src=/images/green.png&gt;&lt;/span&gt;\n&lt;p&gt;&lt;span&gt;&lt;img src=/images/green.png hspace=10&gt;&lt;/span&gt;\n&lt;p&gt;&lt;span&gt;&lt;img src=/images/green.png vspace=10&gt;&lt;/span&gt;\n&lt;p&gt;&lt;span&gt;&lt;img src=/images/green.png hspace=10%&gt;&lt;/span&gt;\n&lt;p&gt;&lt;span&gt;&lt;img src=/images/green.png vspace=10%&gt;&lt;/span&gt;\n&lt;/div&gt;\n</code></pre>"},{"location":"Build%20%26%20Debug/Tests.html#test-runners","title":"Test Runners","text":"<p>Most layout tests are designed to be runnable inside a browser but run-webkit-tests uses a special program to run them. Our continuous integration system as well as the Early Warning System uses run-webkit-tests to run layout tests. In WebKit2, this is appropriately named WebKitTestRunner. In WebKit1 or WebKitLegacy, it\u2019s DumpRenderTree, which is named after the very first type of layout tests, which generated the text representation of the render tree.</p>"},{"location":"Build%20%26%20Debug/Tests.html#extra-interfaces-available-in-test-runners","title":"Extra Interfaces Available in Test Runners","text":"<p>Both WebKitTestRunner and DumpRenderTree expose a few extra interfaces to JavaScript on <code>window</code> (i.e. global object) in order to emulate user inputs, enable or disable a feature, or to improve the reliability of testing.</p> <ul> <li>GCController<ul> <li><code>GCController.collect()</code> triggers a synchronous full garbage collection. This function is useful for testing crashes or erroneous premature collection of JS wrappers and leaks.</li> </ul> </li> <li>testRunner<ul> <li>TestRunner interface exposes many methods to control the behaviors of WebKitTestRunner and DumpRenderTree. Some the most commonly used methods are as follows:</li> <li><code>waitUntilDone()</code> / <code>notifyDone()</code> - These functions are useful when writing tests that involve asynchronous tasks     which may require the test to continue running beyond when it finished loading.     <code>testRunner.waitUntilDone()</code> makes WebKitTestRunner and DumpRenderTree not end the test when a layout test has finished loading.     The test continues until <code>testRunner.notifyDone()</code> is called.</li> <li><code>dumpAsText(boolean dumpPixels)</code> - Makes WebKitTestRunner and DumpRenderTree output the plain text of the loaded page instead of the state of the render tree.</li> <li><code>overridePreference(DOMString preference, DOMString value)</code> - Overrides WebKit\u2019s preferences.     For WebKitLegacy, these are defined in Source/WebKitLegacy/mac/WebView/WebPreferences.h for macOS     and Source/WebKitLegacy/win/WebPreferences.h for Windows.</li> </ul> </li> <li>eventSender<ul> <li>Exposes methods to emulate mouse, keyboard, and touch actions. Use ui-helpers.js script instead of directly calling methods on this function. This will ensure the test will be most compatible with all the test configurations we have.</li> </ul> </li> <li>UIScriptController<ul> <li>Exposes methods to emulate user inputs like eventSender mostly on iOS WebKit2.  Use ui-helpers.js script instead of directly calling methods on this function.  This will ensure the test will be most compatible with all the test configurations we have.</li> </ul> </li> <li>textInputController<ul> <li>Exposes methods to test input methods.</li> </ul> </li> </ul> <p>Additionally, WebCore/testing exposes a few testing hooks to test its internals:</p> <ul> <li>internals<ul> <li>Exposes various hooks into WebCore that shouldn\u2019t be part of WebKit or WebKitLegacy API.</li> </ul> </li> <li>internals.settings<ul> <li>Exposes various WebCore settings and let tests override them. Note that WebKit layer code may depend on preferences in UI process and the aforementioned <code>testRunner.overridePreference</code> may need to be used instead. It\u2019s in fact preferable to override the equivalent preference via <code>testRunner.overridePreference</code> unless you know for sure WebKit or WebKitLegacy layer of code isn\u2019t affected by the setting you\u2019re overriding.</li> </ul> </li> </ul>"},{"location":"Build%20%26%20Debug/Tests.html#enabling-or-disabling-a-feature-in-test-runners","title":"Enabling or Disabling a Feature in Test Runners","text":"<p>FIXME: Mention test-runner-options</p>"},{"location":"Build%20%26%20Debug/Tests.html#test-harness-scripts","title":"Test Harness Scripts","text":"<p>FIXME: Write about dump-as-markup.js, and ui-helper.js</p>"},{"location":"Build%20%26%20Debug/Tests.html#investigating-test-failures-observed-on-bots","title":"Investigating Test Failures Observed on Bots","text":"<p>There are multiple tools to investigate test failures happening on our continuous integration system (build.webkit.org). The most notable is flakiness dashboard: results.webkit.org</p> <p>FIXME: Write how to investigate a test failure.</p>"},{"location":"Build%20%26%20Debug/Tests.html#dive-into-api-tests","title":"Dive into API tests","text":"<p>FIXME: Talk about how to debug API tests.</p>"},{"location":"Deep%20Dive/DOM.html","title":"DOM","text":"<p>A deep dive into the Document Object Model.</p>"},{"location":"Deep%20Dive/DOM.html#introduction","title":"Introduction","text":"<p>Document Object Model (often abbreviated as DOM) is the tree data structured resulted from parsing HTML. It consists of one or more instances of subclasses of Node and represents the document tree structure. Parsing a simple HTML like this:</p> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;body&gt;hi&lt;/body&gt;\n&lt;/html&gt;\n</code></pre> <p>Will generate the following six distinct DOM nodes:</p> <ul> <li>Document<ul> <li>DocumentType</li> <li>HTMLHtmlElement<ul> <li>HTMLHeadElement</li> <li>HTMLBodyElement<ul> <li>Text with the value of \u201chi\u201d</li> </ul> </li> </ul> </li> </ul> </li> </ul> <p>Note that HTMLHeadElement (i.e. <code>&lt;head&gt;</code>) is created implicitly by WebKit per the way HTML parser is specified.</p> <p>Broadly speaking, DOM node divides into the following categories:</p> <ul> <li>Container nodes such as Document, Element, and DocumentFragment.</li> <li>Leaf nodes such as DocumentType, Text, and Attr.</li> </ul> <p>Document node, as the name suggests a single HTML, SVG, MathML, or other XML document, and is the owner of every node in the document. It is the very first node in any document that gets created and the very last node to be destroyed.</p> <p>Note that a single web page may consist of multiple documents since iframe and object elements may contain a child frame, and form a frame tree. Because JavaScript can open a new window under user gestures and have access back to its opener, multiple web pages across multiple tabs might be able to communicate with one another via JavaScript API such as postMessage.</p>"},{"location":"Deep%20Dive/DOM.html#nodes-type-and-state-flags","title":"Node\u2019s Type and State flags","text":"<p>Each node has a set of <code>TypeFlag</code>, which are set at construction time and immutable, and a set of <code>StateFlag</code>, which can be set or unset throughout <code>Node</code>\u2019s lifetime. Node also makes use of <code>EventTargetFlag</code> for indicating ownership and relationship with other objects. For example, <code>TypeFlag::IsElement</code> is set whenever a <code>Node</code> is a subclass of <code>Element</code>. <code>StateFlag::IsParsingChildren</code> is set whenever a <code>Node</code> is in the state of its child nodes being parsed. <code>EventTargetFlag::IsConnected</code> is set whenever a <code>Node</code> is connected. These flags are updated by each subclass of <code>Node</code> throughout its lifetime. Note that these flags are set or unset within a specific function. For example, <code>EventTargetFlag::IsConnected</code> is set in <code>Node::insertedIntoAncestor</code>. It means that any code which runs prior to <code>Node::insertedIntoAncestor</code> running on a given <code>Node</code> will observe an outdated value of <code>EventTargetFlag::IsConnected</code>.</p>"},{"location":"Deep%20Dive/DOM.html#insertion-and-removal-of-dom-nodes","title":"Insertion and Removal of DOM Nodes","text":"<p>In order to construct a DOM tree, we create a DOM <code>Node</code> and insert it into a <code>ContainerNode</code> such as <code>Document</code> and <code>Element</code>. An insertion of a node starts with a validation, then removal of the node from its old parent if there is any. Either of these two steps can synchronously execute JavaScript via mutation events and therefore can synchronously mutate tree\u2019s state. Because of that, we need to check the validity again before proceeding with the insertion.</p> <p>An actual insertion of a DOM <code>Node</code> is implemented using <code>executeNodeInsertionWithScriptAssertion</code> or <code>executeParserNodeInsertionIntoIsolatedTreeWithoutNotifyingParent</code>. To start off, these functions instantiate a RAII-style object <code>ScriptDisallowedScope</code>, which forbids JavaScript execution during its lifetime, do the insertion, then notify the child and its descendant with <code>insertedIntoAncestor</code>. Note that <code>insertedIntoAncestor</code> can be called when a given <code>Node</code> becomes connected to a <code>Document</code>, or it\u2019s inserted into a disconnected subtree. It\u2019s not correct to assume that <code>this</code> <code>Node</code> is always connected to a <code>Document</code> in <code>insertedIntoAncestor</code>. To run code only when a <code>Node</code> becomes connected to a document, check <code>InsertionType</code>\u2019s <code>connectedToDocument</code> boolean. It\u2019s also not necessarily true that this <code>Node</code>\u2019s immediate parent node changed. It could be this <code>Node</code>\u2019s ancestor that got inserted into a new parent. To run code only when this <code>Node</code>\u2019s immediate parent had changed, check if node\u2019s parent node matches <code>parentOfInsertedTree</code>. There are cases in which code must run whenever its <code>TreeScope</code> (<code>ShadowRoot</code> or <code>Document</code>) had changed. In this case, check <code>InsertionType</code>\u2019s <code>treeScopeChanged</code> boolean. In all cases, it\u2019s vital that no code invoked by <code>insertedIntoAncestor</code> attempts to execute JavaScript synchronously, for example, by dispatching an event. Doing so will result in a release assert (i.e. crash). If an element must dispatch events or otherwise execute arbitrary author JavaScript, return <code>NeedsPostInsertionCallback</code> from <code>insertedIntoAncestor</code>. This will result in a call to <code>didFinishInsertingNode</code> which unlike <code>insertedIntoAncestor</code> allows script execution (it gets called only after <code>ScriptDisallowedScope</code> has been out of scope). But note that the tree\u2019s state may have been mutated by other scripts between when <code>insertedIntoAncestor</code> is called and by when <code>didFinishInsertingNode</code> is called so it\u2019s not safe to assume any tree state condition which was true during <code>insertedIntoAncestor</code> to be true in <code>didFinishInsertingNode</code>. It\u2019s also not safe to leave Node in an inconsistent state at the end of <code>insertedIntoAncestor</code> because JavaScript may invoke any API on such a Node between <code>insertedIntoAncestor</code> and <code>didFinishInsertingNode</code>. After invoking <code>insertedIntoAncestor</code>, these functions invoke <code>childrenChanged</code> on the new parent. This function has the first opportunity to execute any JavaScript in response to a child node being inserted. <code>HTMLScriptElement</code>, for example, may execute its script in its <code>childrenChanged</code>. Finally, the functions will invoke <code>didFinishInsertingNode</code> on <code>Node</code>s which returned <code>NeedsPostInsertionCallback</code> from its <code>insertedIntoAncestor</code> and trigger mutation events such as <code>DOMNodeInsertedEvent</code>.</p> <p>The removal of a DOM <code>Node</code> from its parent is implemented using <code>ContainerNode::removeAllChildrenWithScriptAssertion</code> and <code>ContainerNode::removeChildWithScriptAssertion</code>. These functions first dispatch mutation events and check if child\u2019s parent is still the same container node. If it\u2019s not, we stop and exit early. Next, they disconnect any subframes in the subtree to be removed. These functions then instantiate a RAII-style object <code>ScriptDisallowedScope</code>, which forbids JavaScript execution during its lifetime like the insertion counterparts, and notify <code>Document</code> of the node\u2019s removal so that objects such as <code>NodeIterator</code> and <code>Range</code> can be updated. The functions will then do the removal and notify the child and its descendant with <code>removedFromAncestor</code>. Note that <code>removedFromAncestor</code> can be called when a given <code>Node</code> becomes disconnected from a <code>Document</code>, or it\u2019s removed from an already disconnected subtree. It\u2019s not correct to assume that <code>this</code> <code>Node</code> used to be connected to a <code>Document</code> in <code>removedFromAncestor</code>. To run code only when a <code>Node</code> becomes disconnected from a document, check <code>RemovalType</code>\u2019s <code>disconnectedFromDocument</code> boolean. It\u2019s also not necessarily true that this <code>Node</code>\u2019s immediate parent node changed. It could be this <code>Node</code>\u2019s ancestor that got removed from its old parent. To run code only when this <code>Node</code>\u2019s immediate parent had changed, check if node\u2019s parent node is <code>nullptr</code>. To run code whenever its <code>TreeScope</code> (<code>ShadowRoot</code> or <code>Document</code>) had changed, check <code>RemovalType</code>\u2019s <code>treeScopeChanged</code> boolean. In all cases, it\u2019s vital that no code invoked by <code>removedFromAncestor</code> attempts to execute JavaScript synchronously, for example, by dispatching an event. Doing so will result in a release assert (i.e. crash). If an element must dispatch events or otherwise execute arbitrary author JavaScript, queue a task to do so. After invoking <code>removedFromAncestor</code>, these functions invoke <code>childrenChanged</code> on the old parent.</p> <p>Additionally, certain <code>StateFlag</code> and <code>EventTargetFlag</code> might be outdated in <code>insertedIntoAncestor</code> and <code>removedFromAncestor</code>. For example, <code>EventTargetFlag::IsConnected</code> flag is not set or unset until <code>Node::insertedIntoAncestor</code> or <code>Node::removedFromAncestor</code> is called. Accessing other node\u2019s states and member functions are even trickier. Because <code>insertedIntoAncestor</code> or <code>removedFromAncestor</code> may not have been called on such nodes, functions like <code>getElementById</code> and <code>rootNode</code> will return wrong results for those nodes. Code which runs inside these functions must carefully avoid these pitfalls.</p>"},{"location":"Deep%20Dive/MemoryManagement.html","title":"Memory Management","text":"<p>A deep dive into the memory management system utilized by WebKit.</p>"},{"location":"Deep%20Dive/MemoryManagement.html#overview","title":"Overview","text":"<p>In WebKit, when an object is owned by another object, we typically use <code>std::unique_ptr</code> to express that ownership. WebKit uses two primary management strategies when objects in other cases: garbage collection and reference counting.</p>"},{"location":"Deep%20Dive/MemoryManagement.html#reference-counting-in-webkit","title":"Reference counting in WebKit","text":""},{"location":"Deep%20Dive/MemoryManagement.html#overview_1","title":"Overview","text":"<p>Most of WebCore objects are not managed by JavaScriptCore\u2019s garbage collector. Instead, we use reference counting. We have two referencing counting pointer types: <code>RefPtr</code> and <code>Ref</code>. RefPtr is intended to behave like a C++ pointer whereas Ref is intended to behave like a C++ reference, meaning that the former can be set to <code>nullptr</code> but the latter cannot.</p> <pre><code>Ref&lt;A&gt; a1; // This will result in compilation error.\nRefPtr&lt;A&gt; a2; // This is okay.\nRef&lt;A&gt; a3 = A::create(); // This is okay.\na3-&gt;f(); // Calls f() on an instance of A.\nA* a4 = a3.ptr();\na4 = a2.get();\n</code></pre> <p>Unlike C++\u2018s<code>std::shared_ptr</code>, the implementation of referencing counting is a part of a managed object. The requirements for an object to be used with <code>RefPtr</code> and <code>Ref</code> is as follows:</p> <ul> <li>It implements <code>ref()</code> and <code>deref()</code> member functions</li> <li>Each call to <code>ref()</code> and <code>deref()</code> will increment and decrement its internal reference counter</li> <li>The initial call to <code>ref()</code> is implicit in <code>new</code>,     after the object had been allocated and the constructor has been called upon;     i.e. meaning that the reference count starts at 1.</li> <li>When <code>deref()</code> is called when its internal reference counter reaches 0, \u201cthis\u201d object is destructed and deleted.</li> </ul> <p>There is a convenience super template class, <code>RefCounted&lt;T&gt;</code>, which implements this behavior for any inherited class T automatically.</p>"},{"location":"Deep%20Dive/MemoryManagement.html#how-to-use-refptr-and-ref","title":"How to use RefPtr and Ref","text":"<p>When an object which implements the semantics required by RefPtr and Ref is created via new, we must immediately adopt it into <code>Ref</code> type using <code>adoptRef</code> as follows:</p> <pre><code>class A : public RefCounted&lt;A&gt; {\npublic:\n    int m_foo;\n\n    int f() { return m_foo; }\n\n    static Ref&lt;A&gt; create() { return adoptRef(*new A); }\nprivate:\n    A() = default;\n};\n</code></pre> <p>This will create an instance of <code>Ref</code> without calling <code>ref()</code> on the newly created object, avoiding the unnecessary increment from 0 to 1. WebKit\u2019s coding convention is to make the constructor private and add a static <code>create</code> function which returns an instance of a ref counted object after adopting it. </p> <p>Note that returning RefPtr or Ref is efficient thanks to copy elision in C++11, and the following example does not create a temporary Ref object using copy constructor):</p> <pre><code>Ref&lt;A&gt; a = A::create();\n</code></pre> <p>When passing the ownership of a ref-counted object to a function, use rvalue reference with <code>WTFMove</code> (equivalent to <code>std::move</code> with some safety checks), and use a regular reference when there is a guarantee for the caller to keep the object alive as follows:</p> <pre><code>class B {\npublic:\n    void setA(Ref&lt;A&gt;&amp;&amp; a) { m_a = WTFMove(a); }\nprivate:\n    Ref&lt;A&gt; m_a;\n};\n\n...\n\nvoid createA(B&amp; b) {\n    b.setA(A::create());\n}\n</code></pre> <p>Note that there is no <code>WTFMove</code> on <code>A::create</code> due to copy elision.</p>"},{"location":"Deep%20Dive/MemoryManagement.html#forwarding-ref-and-deref","title":"Forwarding ref and deref","text":"<p>As mentioned above, objects that are managed with <code>RefPtr</code> and <code>Ref</code> do not necessarily have to inherit from <code>RefCounted</code>. One common alternative is to forward <code>ref</code> and <code>deref</code> calls to another object which has the ownership. For example, in the following example, <code>Parent</code> class owns <code>Child</code> class. When someone stores <code>Child</code> in <code>Ref</code> or <code>RefPtr</code>, the referencing counting of <code>Parent</code> is incremented and decremented on behalf of <code>Child</code>. Both <code>Parent</code> and <code>Child</code> are destructed when the last <code>Ref</code> or <code>RefPtr</code> to either object goes away.</p> <pre><code>class Parent : RefCounted&lt;Parent&gt; {\npublic:\n    static Ref&lt;Parent&gt; create() { return adoptRef(*new Parent); }\n\n    Child&amp; child() {\n        if (!m_child)\n            m_child = makeUnique&lt;Child&gt;(*this);\n        return m_child\n    }\n\nprivate:\n    std::unique_ptr&lt;Child&gt; m_child;    \n};\n\nclass Child {\npublic:\n    ref() { m_parent.ref(); }\n    deref() { m_parent.deref(); }\n\nprivate:\n    Child(Parent&amp; parent) : m_parent(parent) { }\n    friend class Parent;\n\n    Parent&amp; m_parent;\n}\n</code></pre>"},{"location":"Deep%20Dive/MemoryManagement.html#reference-cycles","title":"Reference Cycles","text":"<p>A reference cycle occurs when an object X which holds <code>Ref</code> or <code>RefPtr</code> to another object Y which in turns owns X by <code>Ref</code> or <code>RefPtr</code>. For example, the following code causes a trivial memory leak because A holds a <code>Ref</code> of B, and B in turn holds <code>Ref</code> of the A:</p> <pre><code>class A : RefCounted&lt;A&gt; {\npublic:\n    static Ref&lt;A&gt; create() { return adoptRef(*new A); }\n    B&amp; b() {\n        if (!m_b)\n            m_b = B::create(*this);\n        return m_b.get();\n    }\nprivate:\n    Ref&lt;B&gt; m_b;\n};\n\nclass B : RefCounted&lt;B&gt; {\npublic:\n    static Ref&lt;B&gt; create(A&amp; a) { return adoptRef(*new B(a)); }\n\nprivate:\n    B(A&amp; a) : m_a(a) { }\n    Ref&lt;A&gt; m_a;\n};\n</code></pre> <p>We need to be particularly careful in WebCore with regards to garbage collected objects because they often keep other ref counted C++ objects alive without having any <code>Ref</code> or <code>RefPtr</code> in C++ code. It\u2019s almost always incorrect to strongly keep JS value alive in WebCore code because of this.</p>"},{"location":"Deep%20Dive/MemoryManagement.html#protectedthis-pattern","title":"ProtectedThis Pattern","text":"<p>Because many objects in WebCore are managed by tree data structures, a function that operates on a node of such a tree data structure can end up deleting itself (<code>this</code> object). This is highly undesirable as such code often ends up having a use-after-free bug.</p> <p>To prevent these kinds of bugs, we often employ a strategy of adding <code>protectedThis</code> local variable of <code>Ref</code> or <code>RefPtr</code> type, and store <code>this</code> object as follows:</p> <pre><code>ExceptionOr&lt;void&gt; ContainerNode::removeChild(Node&amp; oldChild)\n{\n    // Check that this node is not \"floating\".\n    // If it is, it can be deleted as a side effect of sending mutation events.\n    ASSERT(refCount() || parentOrShadowHostNode());\n\n    Ref&lt;ContainerNode&gt; protectedThis(*this);\n\n    // NotFoundError: Raised if oldChild is not a child of this node.\n    if (oldChild.parentNode() != this)\n        return Exception { NotFoundError };\n\n    if (!removeNodeWithScriptAssertion(oldChild, ChildChange::Source::API))\n        return Exception { NotFoundError };\n\n    rebuildSVGExtensionsElementsIfNecessary();\n    dispatchSubtreeModifiedEvent();\n\n    return { };\n}\n</code></pre> <p>In this code, the act of removing <code>oldChild</code> can execute arbitrary JavaScript and delete <code>this</code> object. As a result, <code>rebuildSVGExtensionsElementsIfNecessary</code> or <code>dispatchSubtreeModifiedEvent</code> might be called after <code>this</code> object had already been free\u2019ed if we didn\u2019t have <code>protectedThis</code>, which guarantees that this object\u2019s reference count is at least 1 (because Ref\u2019s constructor increments the reference count by 1).</p> <p>This pattern can be used for other objects that need to be protected from destruction inside a code block. In the following code, <code>childToRemove</code> was passed in using C++ reference. Because this function is going to remove this child node from <code>this</code> container node, it can get destructed while the function is still running. To prevent from having any chance of use-after-free bugs, this function stores it in Ref (<code>protectedChildToRemove</code>) which guarantees the object to be alive until the function returns control back to the caller:</p> <pre><code>ALWAYS_INLINE bool ContainerNode::removeNodeWithScriptAssertion(Node&amp; childToRemove, ChildChangeSource source)\n{\n    Ref&lt;Node&gt; protectedChildToRemove(childToRemove);\n    ASSERT_WITH_SECURITY_IMPLICATION(childToRemove.parentNode() == this);\n    {\n        ScriptDisallowedScope::InMainThread scriptDisallowedScope;\n        ChildListMutationScope(*this).willRemoveChild(childToRemove);\n    }\n    ..\n</code></pre> <p>Also see Darin\u2019s RefPtr Basics for further reading.</p>"},{"location":"Deep%20Dive/MemoryManagement.html#weak-pointers-in-webkit","title":"Weak Pointers in WebKit","text":"<p>In some cases, it\u2019s desirable to express a relationship between two objects without necessarily tying their lifetime. In those cases, <code>WeakPtr</code> is useful. Like std::weak_ptr, this class creates a non-owning reference to an object. There is a lot of legacy code which uses a raw pointer for this purpose, but there is an ongoing effort to always use WeakPtr instead so do that in new code you\u2019re writing.</p> <p>To create a <code>WeakPtr</code> to an object, we need to make its class inherit from <code>CanMakeWeakPtr</code> as follows:</p> <pre><code>class A : CanMakeWeakPtr&lt;A&gt; { }\n\n...\n\nfunction foo(A&amp; a) {\n    WeakPtr&lt;A&gt; weakA = a;\n}\n</code></pre> <p>Dereferencing a <code>WeakPtr</code> will return <code>nullptr</code> when the referenced object is deleted. Because creating a <code>WeakPtr</code> allocates an extra <code>WeakPtrImpl</code> object, you\u2019re still responsible to dispose of <code>WeakPtr</code> at appropriate time.</p>"},{"location":"Deep%20Dive/MemoryManagement.html#weakhashset","title":"WeakHashSet","text":"<p>While ordinary <code>HashSet</code> does not support having <code>WeakPtr</code> as its elements, there is a specialized <code>WeakHashSet</code> class, which supports referencing a set of elements weakly. Because <code>WeakHashSet</code> does not get notified when the referenced object is deleted, the users / owners of <code>WeakHashSet</code> are still responsible for deleting the relevant entries from the set. Otherwise, WeakHashSet will hold onto <code>WeakPtrImpl</code> until <code>computeSize</code> is called or rehashing happens.</p>"},{"location":"Deep%20Dive/MemoryManagement.html#weakhashmap","title":"WeakHashMap","text":"<p>Like <code>WeakHashSet</code>, <code>WeakHashMap</code> is a specialized class to map a WeakPtr key with a value. Because <code>WeakHashMap</code> does not get notified when the referenced object is deleted, the users / owners of <code>WeakHashMap</code> are still responsible for deleting the relevant entries from the map. Otherwise, the memory space used by <code>WeakPtrImpl</code> and its value will not be free'ed up until next rehash or amortized cleanup cycle arrives (based on the total number of read or write operations).</p>"},{"location":"Deep%20Dive/MemoryManagement.html#reference-counting-of-dom-nodes","title":"Reference Counting of DOM Nodes","text":"<p><code>Node</code> is a reference counted object but with a twist. It has a separate boolean flag indicating whether it has a parent node or not. A <code>Node</code> object is not deleted so long as it has a reference count above 0 or this boolean flag is set. The boolean flag effectively functions as a <code>RefPtr</code> from a parent <code>Node</code> to each one of its child <code>Node</code>. We do this because <code>Node</code> only knows its first child and its last child and each sibling nodes are implemented as a doubly linked list to allow efficient insertion and removal and traversal of sibling nodes.</p> <p>Conceptually, each <code>Node</code> is kept alive by its root node and external references to it, and we use the root node as an opaque root of each <code>Node</code>'s JS wrapper. Therefore the JS wrapper of each <code>Node</code> is kept alive as long as either the node itself or any other node which shares the same root node is visited by the garbage collector.</p> <p>On the other hand, a <code>Node</code> does not keep its parent or any of its shadow-including ancestor <code>Node</code> alive either by reference counting or via the boolean flag even though the JavaScript API requires this to be the case. In order to implement this DOM API behavior, WebKit will create a JS wrapper for each <code>Node</code> which is being removed from its parent if there isn't already one. A <code>Node</code> which is a root node (of the newly removed subtree) is an opaque root of its JS wrapper, and the garbage collector will visit this opaque root if there is any JS wrapper in the removed subtree that needs to be kept alive. In effect, this keeps the new root node and all its descendant nodes alive if the newly removed subtree contains any node with a live JS wrapper, preserving the API contract.</p> <p>It's important to recognize that storing a <code>Ref</code> or a <code>RefPtr</code> to another <code>Node</code> in a <code>Node</code> subclass or an object directly owned by the Node can create a reference cycle, or a reference that never gets cleared. It's not guaranteed that every node is disconnected from a <code>Document</code> at some point in the future, and some <code>Node</code> may always have a parent node or a child node so long as it exists. Only permissible circumstances in which a <code>Ref</code> or a <code>RefPtr</code> to another <code>Node</code> can be stored in a <code>Node</code> subclass or other data structures owned by it is if it's temporally limited. For example, it's okay to store a <code>Ref</code> or a <code>RefPtr</code> in an enqueued event loop task. In all other circumstances, <code>WeakPtr</code> should be used to reference another <code>Node</code>, and JS wrapper relationships such as opaque roots should be used to preserve the lifecycle ties between <code>Node</code> objects.</p> <p>It's equally crucial to observe that keeping C++ Node object alive by storing <code>Ref</code> or <code>RefPtr</code> in an enqueued event loop task does not keep its JS wrapper alive, and can result in the JS wrapper of a conceptually live object to be erroneously garbage collected. To avoid this problem, use <code>GCReachableRef</code> instead to temporarily hold a strong reference to a node over a period of time. For example, <code>HTMLTextFormControlElement::scheduleSelectEvent()</code> uses <code>GCReachableRef</code> to fire an event in an event loop task:</p> <pre><code>void HTMLTextFormControlElement::scheduleSelectEvent()\n{\n    document().eventLoop().queueTask(TaskSource::UserInteraction, [protectedThis = GCReachableRef { *this }] {\n        protectedThis-&gt;dispatchEvent(Event::create(eventNames().selectEvent, Event::CanBubble::Yes, Event::IsCancelable::No));\n    });\n}\n</code></pre> <p>Alternatively, we can make it inherit from an active DOM object, and use one of the following functions to enqueue a task or an event:</p> <ul> <li><code>queueTaskKeepingObjectAlive</code></li> <li><code>queueCancellableTaskKeepingObjectAlive</code></li> <li><code>queueTaskToDispatchEvent</code></li> <li><code>queueCancellableTaskToDispatchEvent</code></li> </ul> <p><code>Document</code> node has one more special quirk because every <code>Node</code> can have access to a document via <code>ownerDocument</code> property whether Node is connected to the document or not. Every document has a regular reference count used by external clients and referencing node count. The referencing node count of a document is the total number of nodes whose <code>ownerDocument</code> is the document. A document is kept alive so long as its reference count and node referencing count is above 0. In addition, when the regular reference count is to become 0, it clears various states including its internal references to owning Nodes to sever any reference cycles with them. A document is special in that sense that it can store <code>RefPtr</code> to other nodes. Note that whilst the referencing node count acts like <code>Ref</code> from each <code>Node</code> to its owner <code>Document</code>, storing a <code>Ref</code> or a <code>RefPtr</code> to the same document or any other document will create a reference cycle and should be avoided unless it's temporally limited as noted above.</p>"},{"location":"Deep%20Dive/Architecture/AddingNewJSApi.html","title":"Adding JS APIs","text":""},{"location":"Deep%20Dive/Architecture/AddingNewJSApi.html#overview","title":"Overview","text":"<p>To introduce a new JavaScript API in WebCore,  first identify the directory under which to implement this new API, and introduce corresponding Web IDL files (e.g., \"dom/SomeAPI.idl\").</p> <p>New IDL files should be listed in Source/WebCore/DerivedSources.make so that the aforementioned perl script can generate corresponding JS.cpp and JS.h files. Add these newly generated JS*.cpp files to Source/WebCore/Sources.txt in order for them to be compiled.</p> <p>Also, add the new IDL file(s) to Source/WebCore/CMakeLists.txt.</p> <p>Remember to add these files to WebCore's Xcode project as well.</p> <p>For example, this commit introduced <code>IdleDeadline.idl</code> and added <code>JSIdleDeadline.cpp</code> to the list of derived sources to be compiled.</p>"},{"location":"Deep%20Dive/Architecture/JSWrappers.html","title":"JS Wrappers and IDL Files","text":""},{"location":"Deep%20Dive/Architecture/JSWrappers.html#overview","title":"Overview","text":"<p>In addition to typical C++ translation units (.cpp) and C++ header files (.cpp) along with some Objective-C and Objective-C++ files, WebCore contains hundreds of Web IDL (.idl) files. Web IDL is an interface description language and it's used to define the shape and the behavior of JavaScript API implemented in WebKit.</p> <p>When building WebKit, a perl script generates appropriate C++ translation units and C++ header files corresponding to these IDL files under <code>WebKitBuild/Debug/DerivedSources/WebCore/</code> where <code>Debug</code> is the current build configuration (e.g. it could be <code>Release-iphonesimulator</code> for example).</p> <p>These auto-generated files along with manually written files Source/WebCore/bindings are called JS DOM binding code and implements JavaScript API for objects and concepts whose underlying shape and behaviors are written in C++.</p> <p>For example, C++ implementation of Node is Node class and its JavaScript interface is implemented by <code>JSNode</code> class. The class declaration and most of definitions are auto-generated at <code>WebKitBuild/Debug/DerivedSources/WebCore/JSNode.h</code> and <code>WebKitBuild/Debug/DerivedSources/WebCore/JSNode.cpp</code> for debug builds. It also has some custom, manually written, bindings code in Source/WebCore/bindings/js/JSNodeCustom.cpp. Similarly, C++ implementation of Range interface is Range class whilst its JavaScript API is implemented by the auto-generated JSRange class (located at <code>WebKitBuild/Debug/DerivedSources/WebCore/JSRange.h</code> and <code>WebKitBuild/Debug/DerivedSources/WebCore/JSRange.cpp</code> for debug builds) We call instances of these JSX classes JS wrappers of X.</p> <p>These JS wrappers exist in what we call a <code>DOMWrapperWorld</code>. Each <code>DOMWrapperWorld</code> has its own JS wrapper for each C++ object. As a result, a single C++ object may have multiple JS wrappers in distinct <code>DOMWrapperWorld</code>s. The most important <code>DOMWrapperWorld</code> is the main <code>DOMWrapperWorld</code> which runs the scripts of web pages WebKit loaded while other <code>DOMWrapperWorld</code>s are typically used to run code for browser extensions and other code injected by applications that embed WebKit.  JSX.h provides <code>toJS</code> functions which creates a JS wrapper for X in a given global object\u2019s <code>DOMWrapperWorld</code>, and toWrapped function which returns the underlying C++ object. For example, <code>toJS</code> function for Node is defined in Source/WebCore/bindings/js/JSNodeCustom.h.</p> <p>When there is already a JS wrapper object for a given C++ object, <code>toJS</code> function will find the appropriate JS wrapper in a hash map of the given <code>DOMWrapperWorld</code>. Because a hash map lookup is expensive, some WebCore objects inherit from ScriptWrappable, which has an inline pointer to the JS wrapper for the main world if one was already created.</p>"},{"location":"Deep%20Dive/Architecture/JSWrappers.html#js-wrapper-lifecycle-management","title":"JS Wrapper Lifecycle Management","text":"<p>As a general rule, a JS wrapper keeps its underlying C++ object alive by means of reference counting in JSDOMWrapper temple class from which all JS wrappers in WebCore inherits. However, C++ objects do not keep their corresponding JS wrapper in each world alive by the virtue of them staying alive as such a circular dependency will result in a memory leak.</p> <p>There are two primary mechanisms to keep JS wrappers alive in WebCore:</p> <ul> <li>Visit Children - When JavaScriptCore\u2019s garbage collection visits some JS wrapper during     the marking phase,     visit another JS wrapper or JS object that needs to be kept alive.</li> <li>Reachable from Opaque Roots - Tell JavaScriptCore\u2019s garbage collection that a JS wrapper is reachable     from an opaque root which was added to the set of opaque roots during marking phase.</li> </ul>"},{"location":"Deep%20Dive/Architecture/JSWrappers.html#visit-children","title":"Visit Children","text":"<p>Visit Children is the mechanism we use when a JS wrapper needs to keep another JS wrapper or JS object alive.</p> <p>For example, <code>ErrorEvent</code> object uses this method in Source/WebCore/bindings/js/JSErrorEventCustom.cpp to keep its \"error\" IDL attribute as follows:</p> <pre><code>template&lt;typename Visitor&gt;\nvoid JSErrorEvent::visitAdditionalChildren(Visitor&amp; visitor)\n{\n    wrapped().originalError().visit(visitor);\n}\n\nDEFINE_VISIT_ADDITIONAL_CHILDREN(JSErrorEvent);\n</code></pre> <p>Here, <code>DEFINE_VISIT_ADDITIONAL_CHILDREN</code> macro generates template instances of visitAdditionalChildren which gets called by the JavaScriptCore's garbage collector. When the garbage collector visits an instance <code>ErrorEvent</code> object, it also visits <code>wrapped().originalError()</code>, which is the JavaScript value of \"error\" attribute:</p> <pre><code>class ErrorEvent final : public Event {\n...\n    const JSValueInWrappedObject&amp; originalError() const { return m_error; }\n    SerializedScriptValue* serializedError() const { return m_serializedError.get(); }\n...\n    JSValueInWrappedObject m_error;\n    RefPtr&lt;SerializedScriptValue&gt; m_serializedError;\n    bool m_triedToSerialize { false };\n};\n</code></pre> <p>Note that <code>JSValueInWrappedObject</code> uses <code>Weak</code>, which does not keep the referenced object alive on its own. We can't use a reference type such as <code>Strong</code> which keeps the referenced object alive on its own since the stored JS object may also have this <code>ErrorEvent</code> object stored as its property. Because the garbage collector has no way of knowing or clearing the <code>Strong</code> reference or the property to <code>ErrorEvent</code> in this hypothetical version of <code>ErrorEvent</code>, it would never be able to collect either object, resulting in a memory leak.</p> <p>To use this method of keeping a JavaScript object or wrapper alive, add <code>JSCustomMarkFunction</code> to the IDL file, then introduce JS*Custom.cpp file under Source/WebCore/bindings/js and implement <code>template&lt;typename Visitor&gt; void JS*Event::visitAdditionalChildren(Visitor&amp; visitor)</code> as seen above for <code>ErrorEvent</code>.</p> <p>visitAdditionalChildren is called concurrently while the main thread is running. Any operation done in visitAdditionalChildren needs to be multi-thread safe. For example, it cannot increment or decrement the reference count of a <code>RefCounted</code> object or create a new <code>WeakPtr</code> from <code>CanMakeWeakPtr</code> since these WTF classes are not thread safe.</p>"},{"location":"Deep%20Dive/Architecture/JSWrappers.html#opaque-roots","title":"Opaque Roots","text":"<p>Reachable from Opaque Roots is the mechanism we use when we have an underlying C++ object and want to keep JS wrappers of other C++ objects alive.</p> <p>To see why, let's consider a <code>StyleSheet</code> object. So long as this object is alive, we also need to keep the DOM node returned by the <code>ownerNode</code> attribute. Also, the object itself needs to be kept alive so long as the owner node is alive since this [<code>StyleSheet</code> object] can be accessed via <code>sheet</code> IDL attribute of the owner node. If we were to use the visit children mechanism, we need to visit every JS wrapper of the owner node whenever this <code>StyleSheet</code> object is visited by the garbage collector, and we need to visit every JS wrapper of the <code>StyleSheet</code> object whenever an owner node is visited by the garbage collector. But in order to do so, we need to query every <code>DOMWrapperWorld</code>'s wrapper map to see if there is a JavaScript wrapper. This is an expensive operation that needs to happen all the time, and creates a tie coupling between <code>Node</code> and <code>StyleSheet</code> objects since each JS wrapper objects need to be  aware of other objects' existence. </p> <p>Opaque roots solves these problems by letting the garbage collector know that a particular JavaScript wrapper needs to be kept alive so long as the gargabe collector had encountered specific opaque root(s) this JavaScript wrapper cares about even if the garbage collector didn't visit the JavaScript wrapper directly. An opaque root is simply a <code>void*</code> identifier the garbage collector keeps track of during each marking phase, and it does not conform to a specific interface or behavior. It could have been an arbitrary integer value but <code>void*</code> is used out of convenience since pointer values of live objects are unique.</p> <p>In the case of a <code>StyleSheet</code> object, <code>StyleSheet</code>'s JavaScript wrapper tells the garbage collector that it needs to be kept alive because an opaque root it cares about has been encountered whenever <code>ownerNode</code> is visited by the garbage collector.</p> <p>In the most simplistic model, the opaque root for this case will be the <code>ownerNode</code> itself. However, each <code>Node</code> object also has to keep its parent, siblings, and children alive. To this end, each <code>Node</code> designates the root node as its opaque root. Both <code>Node</code> and <code>StyleSheet</code> objects use this unique opaque root as a way of communicating with the gargage collector.</p> <p>For example, <code>StyleSheet</code> object informs the garbage collector of this opaque root when it's asked to visit its children in JSStyleSheetCustom.cpp:</p> <pre><code>template&lt;typename Visitor&gt;\nvoid JSStyleSheet::visitAdditionalChildren(Visitor&amp; visitor)\n{\n    visitor.addOpaqueRoot(root(&amp;wrapped()));\n}\n</code></pre> <p>Here, <code>void* root(StyleSheet*)</code> returns the opaque root of the <code>StyleSheet</code> object as follows:</p> <pre><code>inline void* root(StyleSheet* styleSheet)\n{\n    if (CSSImportRule* ownerRule = styleSheet-&gt;ownerRule())\n        return root(ownerRule);\n    if (Node* ownerNode = styleSheet-&gt;ownerNode())\n        return root(ownerNode);\n    return styleSheet;\n}\n</code></pre> <p>And then in <code>JSStyleSheet.cpp</code> (located at <code>WebKitBuild/Debug/DerivedSources/WebCore/JSStyleSheet.cpp</code> for debug builds) <code>JSStyleSheetOwner</code> (a helper JavaScript object to communicate with the garbage collector) tells the garbage collector that <code>JSStyleSheet</code> should be kept alive so long as the garbage collector had encountered this <code>StyleSheet</code>'s opaque root:</p> <pre><code>bool JSStyleSheetOwner::isReachableFromOpaqueRoots(JSC::Handle&lt;JSC::Unknown&gt; handle, void*, AbstractSlotVisitor&amp; visitor, const char** reason)\n{\n    auto* jsStyleSheet = jsCast&lt;JSStyleSheet*&gt;(handle.slot()-&gt;asCell());\n    void* root = WebCore::root(&amp;jsStyleSheet-&gt;wrapped());\n    if (UNLIKELY(reason))\n        *reason = \"Reachable from jsStyleSheet\";\n    return visitor.containsOpaqueRoot(root);\n}\n</code></pre> <p>Generally, using opaque roots as a way of keeping JavaScript wrappers involve two steps:</p> <ol> <li>Add opaque roots in <code>visitAdditionalChildren</code>.</li> <li>Return true in <code>isReachableFromOpaqueRoots</code> when relevant opaque roots are found.</li> </ol> <p>The first step can be achieved by using the aforementioned <code>JSCustomMarkFunction</code> with <code>visitAdditionalChildren</code>. Alternatively and more preferably, <code>GenerateAddOpaqueRoot</code> can be added to the IDL interface to auto-generate this code. For example, AbortController.idl makes use of this IDL attribute as follows:</p> <pre><code>[\n    Exposed=(Window,Worker),\n    GenerateAddOpaqueRoot=signal\n] interface AbortController {\n    [CallWith=ScriptExecutionContext] constructor();\n\n    [SameObject] readonly attribute AbortSignal signal;\n\n    [CallWith=GlobalObject] undefined abort(optional any reason);\n};\n</code></pre> <p>Here, <code>signal</code> is a public member function funtion of the underlying C++ object:</p> <pre><code>class AbortController final : public ScriptWrappable, public RefCounted&lt;AbortController&gt; {\n    WTF_MAKE_ISO_ALLOCATED(AbortController);\npublic:\n    static Ref&lt;AbortController&gt; create(ScriptExecutionContext&amp;);\n    ~AbortController();\n\n    AbortSignal&amp; signal();\n    void abort(JSDOMGlobalObject&amp;, JSC::JSValue reason);\n\nprivate:\n    explicit AbortController(ScriptExecutionContext&amp;);\n\n    Ref&lt;AbortSignal&gt; m_signal;\n};\n</code></pre> <p>When <code>GenerateAddOpaqueRoot</code> is specified without any value, it automatically calls <code>opaqueRoot()</code> instead.</p> <p>Like visitAdditionalChildren, adding opaque roots happen concurrently while the main thread is running. Any operation done in visitAdditionalChildren needs to be multi-thread safe. For example, it cannot increment or decrement the reference count of a <code>RefCounted</code> object or create a new <code>WeakPtr</code> from <code>CanMakeWeakPtr</code> since these WTF classes are not thread safe.</p> <p>The second step can be achived by adding <code>CustomIsReachable</code> to the IDL file and implementing <code>JS*Owner::isReachableFromOpaqueRoots</code> in JS*Custom.cpp file. Alternatively and more preferably, <code>GenerateIsReachable</code> can be added to IDL file to automatically generate this code with the following values:</p> <ul> <li>No value - Adds the result of calling <code>root(T*)</code> on the underlying C++ object of type T as the opaque root.</li> <li><code>Impl</code> - Adds the underlying C++ object as the opaque root.</li> <li><code>ReachableFromDOMWindow</code> - Adds a <code>DOMWindow</code>     returned by <code>window()</code> as the opaque root.</li> <li><code>ReachableFromNavigator</code> - Adds a <code>Navigator</code>     returned by <code>navigator()</code> as the opaque root.</li> <li><code>ImplDocument</code> - Adds a <code>Document</code>     returned by <code>document()</code> as the opaque root.</li> <li><code>ImplElementRoot</code> - Adds the root node of a <code>Element</code>     returned by <code>element()</code> as the opaque root.</li> <li><code>ImplOwnerNodeRoot</code> - Adds the root node of a <code>Node</code>     returned by <code>ownerNode()</code> as the opaque root.</li> <li><code>ImplScriptExecutionContext</code> - Adds a <code>ScriptExecutionContext</code>     returned by <code>scriptExecutionContext()</code> as the opaque root.</li> </ul> <p>Similar to visiting children or adding opaque roots, whether an opaque root is reachable or not is checked in parallel. However, it happens while the main thread is paused unlike visiting children or adding opaque roots, which happen concurrently while the main thread is running. This means that any operation done in <code>JS*Owner::isReachableFromOpaqueRoots</code> or any function called by GenerateIsReachable cannot have thread unsafe side effects such as incrementing or decrementing the reference count of a <code>RefCounted</code> object or creating a new <code>WeakPtr</code> from <code>CanMakeWeakPtr</code> since these WTF classes' mutation operations are not thread safe.</p>"},{"location":"Deep%20Dive/Architecture/JSWrappers.html#active-dom-objects","title":"Active DOM Objects","text":"<p>Visit children and opaque roots are great way to express lifecycle relationships between JS wrappers but there are cases in which a JS wrapper needs to be kept alive without any relation to other objects. Consider <code>XMLHttpRequest</code>. In the following example, JavaScript loses all references to the <code>XMLHttpRequest</code> object and its event listener but when a new response gets received, an event will be dispatched on the object, re-introducing a new JavaScript reference to the object. That is, the object survives garbage collection's mark and sweep cycles without having any ties to other \"root\" objects.</p> <pre><code>function fetchURL(url, callback)\n{\n    const request = new XMLHttpRequest();\n    request.addEventListener(\"load\", callback);\n    request.open(\"GET\", url);\n    request.send();\n}\n</code></pre> <p>In WebKit, we consider such an object to have a pending activity. Expressing the presence of such a pending activity is a primary use case of <code>ActiveDOMObject</code>.</p> <p>By making an object inherit from <code>ActiveDOMObject</code> and annotating IDL as such, WebKit will automatically generate <code>isReachableFromOpaqueRoot</code> function which returns true whenever <code>ActiveDOMObject::hasPendingActivity</code> returns true even though the garbage collector may not have encountered any particular opaque root to speak of in this instance.</p> <p>In the case of <code>XMLHttpRequest</code>, <code>hasPendingActivity</code> will return true so long as there is still an active network activity associated with the object. Once the resource is fully fetched or failed, it ceases to have a pending activity. This way, JS wrapper of <code>XMLHttpRequest</code> is kept alive so long as there is an active network activity.</p> <p>There is one other related use case of active DOM objects, and that's when a document enters the back-forward cache and when the entire page has to pause for other reasons.</p> <p>When this happens, each active DOM object associated with the document gets suspended. Each active DOM object can use this opportunity to prepare itself to pause whatever pending activity; for example, <code>XMLHttpRequest</code> will stop dispatching <code>progress</code> event and media elements will stop playback. When a document gets out of the back-forward cache or resumes for other reasons, each active DOM object gets resumed. Here, each object has the opportunity to resurrect the previously pending activity once again.</p>"},{"location":"Deep%20Dive/Architecture/JSWrappers.html#creating-a-pending-activity","title":"Creating a Pending Activity","text":"<p>There are a few ways to create a pending activity on an active DOM objects.</p> <p>When the relevant Web standards says to queue a task to do some work, one of the following member functions of <code>ActiveDOMObject</code> should be used:</p> <ul> <li><code>queueTaskKeepingObjectAlive</code></li> <li><code>queueCancellableTaskKeepingObjectAlive</code></li> <li><code>queueTaskToDispatchEvent</code></li> <li><code>queueCancellableTaskToDispatchEvent</code></li> </ul> <p>These functions will automatically create a pending activity until a newly enqueued task is executed.</p> <p>Alternatively, <code>makePendingActivity</code> can be used to create a pending activity token for an active DOM object. This will keep a pending activity on the active DOM object until all tokens are dead.</p> <p>Finally, when there is a complex condition under which a pending activity exists, an active DOM object can override <code>virtualHasPendingActivity</code> member function and return true whilst such a condition holds. Note that <code>virtualHasPendingActivity</code> should return true so long as there is a possibility of dispatching an event or invoke JavaScript in any way in the future. In other words, a pending activity should exist while an object is doing some work in C++ well before any event dispatching is scheduled. Anytime there is no pending activity, JS wrappers of the object can get deleted by the garbage collector.</p>"},{"location":"Deep%20Dive/Architecture/Storage.html","title":"Storage","text":""},{"location":"Deep%20Dive/Architecture/Storage.html#overview","title":"Overview","text":"<p>In network process, WebStorage data is managed by WebKit::StorageManager class. Each StorageManager owns one or many StorageNameSpaces, and each StorageNamespace owns one or more StorageAreas. Each StorageArea corresponds to one storage map (one localStorage or sessionStorage object), using either a SQLite database or a in-memory map as backend. Now that we have NetworkStorageManager, which manages storage by origin. We can merge StorageManager with NetworkStorageManager, since localStorage and sessionStorage are not shared between different origins.</p>"},{"location":"Deep%20Dive/Architecture/Storage.html#hierarchy","title":"Hierarchy","text":"<ul> <li>NetworkStorageManager (manage storage for a session, owns one or more OriginStorageManagers)</li> <li>OriginStorageManager (manage storage for an origin, owns one LocalStorageManager and one SessionStorageManager)</li> <li>LocalStorageManager / SessionStorageManager (manage LocalStorage and SessionStorage, owns one or more StorageAreaBases)</li> <li>MemoryStorageArea / SQLiteStorageArea (inherits StorageAreaBases; manage one local or session storage, like Webkit::StorageArea)</li> </ul>"},{"location":"Deep%20Dive/Architecture/Storage.html#notes","title":"Notes","text":"<p>The StorageNamespace layer was removed. For SessionStorage, different StorageNamespaces means different web pages, and same origin can have different sessionStorages on different pages, so we keep a StorageNamespace &lt;=&gt; StorageArea map in SessionStorageManager. For LocalStorage, different StorageNamespaces means different page groups. Our original plan was the same origin can have different localStorages in different page groups, but our old implementation actually made all page groups point to the same database file. To keep existing behavior that all page groups with same origin share the storage, we now keep one local StorageArea and one transient StorageArea per LocalStorageManager.</p>"},{"location":"Deep%20Dive/Architecture/WebKit2.html","title":"WebKit2","text":"<p>WebKit\u2019s Multi-Process Architecture</p>"},{"location":"Deep%20Dive/Architecture/WebKit2.html#overview","title":"Overview","text":"<p>In order to safeguard the rest of the system and allow the application to remain responsive even if the user had loaded web page that infinite loops or otherwise hangs, the modern incarnation of WebKit uses multi-process architecture. Web pages are loaded in its own WebContent process. Multiple WebContent processes can share a browsing session, which lives in a shared network process. In addition to handling all network accesses, this process is also responsible for managing the disk cache and Web APIs that allow websites to store structured data such as Web Storage API and IndexedDB API:  Because a WebContent process can Just-in-Time compile arbitrary JavaScript code loaded from the internet, meaning that it can write to memory that gets executed, this process is tightly sandboxed. It does not have access to any file system unless the user grants an access, and it does not have direct access to the underlying operating system\u2019s clipboard, microphone, or video camera even though there are Web APIs that grant access to those features. Instead, UI process brokers such requests.</p> <p>FIXME: How is IPC setup</p> <p>FIXME: How to add / modify an IPC message</p>"},{"location":"Deep%20Dive/Build/AddingNewFile.html","title":"Adding a New File","text":"<p>How to add a new file to WebKit</p>"},{"location":"Deep%20Dive/Build/AddingNewFile.html#overview","title":"Overview","text":"<p>To add a new header file or a translation unit (e.g. <code>.cpp</code>, <code>.m</code>, or <code>.mm</code>), open WebKit.xcworkspace and add respective files in each directory.</p> <p>Make sure to uncheck the target membership so that it\u2019s not compiled as a part of the framework in xcodebuild. Instead, add the same file in Sources.txt file that exists in each subdirectory of Source. e.g. Source/WebCore/Sources.txt for WebCore. This will ensure the newly added file is compiled as a part of unified sources.  When a header file in WTF is used in WebCore, or a header file in WebCore is used in WebKit or WebKitLegacy, we need to export the file to those projects. To do that, turn on the target membership in respective framework as set the membership to \u201cPrivate\u201d as seen below. This will ensure the relevant header file is exported from WTF / WebCore to other downstream projects like WebKitLegacy. </p> <p>Non-cocoa ports, like WPE and GTK, use CMake for building the project. If the header is not platform-specific, you might  want to add an entry for it into the relevant Headers.cmake. For example, if a WebCore header is included by WebKit, you  will need to list the header in Source/WebCore/Headers.cmake.</p> <p>FIXME: Mention WTF_EXPORT_PRIVATE and WEBCORE_EXPORT.</p>"},{"location":"Deep%20Dive/Build/CI.html","title":"Continuous Integration","text":""},{"location":"Deep%20Dive/Build/CI.html#overview","title":"Overview","text":"<p>WebKit\u2019s CI (continuous integration) infrastructure is located at build.webkit.org.</p> <p>The CI will build and test commits from WebKit in chronological order and report test results to results.webkit.org. Due to chronological ordering, results may require several hours to complete during peak times.</p>"},{"location":"Deep%20Dive/Build/CI.html#dashboard","title":"Dashboard","text":"<p>We also have a dashboard to monitor the health of build.webkit.org at build.webkit.org/dashboard. If you observe that some bots are offline, or otherwise not processing your patch, please notify webkit-dev@webkit.org.</p>"},{"location":"Deep%20Dive/Build/CI.html#results","title":"Results","text":"<p>This dashboard isn't great for investigating individual test failures; results.webkit.org is a better tool for such investigations. It keeps track of individual test status by configuration over time. You can search individual tests by name or look at the historical results of entire test suites. These results will link back to the test runs in Buildbot which are associated with a specific failure. See layout tests section for more details on how to use these tools to investigate test failures observed on bots.</p>"},{"location":"Deep%20Dive/Build/CI.html#ci-artifacts","title":"CI Artifacts","text":"<p>The test results and build artifacts are available to download for every CI run. Upon opening the specific run you can find the layout test results available for download under <code>layout-test</code>. The binaries used for the run can be found using the link in <code>download-build-product</code>. Other tests like <code>run-api-tests</code> also have their logs available for download in their respective sections.</p>"},{"location":"Deep%20Dive/Build/ConditionalCompilation.html","title":"Conditional Compilation","text":""},{"location":"Deep%20Dive/Build/ConditionalCompilation.html#overview","title":"Overview","text":"<p>Every translation unit in WebKit starts by including \u201cconfig.h\u201d. This file defines a set of C++ preprocessor macros used to enable or disable code based on the target operating system, platform, and whether a given feature is enabled or disabled.</p> <p>For example, the following <code>#if</code> condition says that the code inside of it is only compiled if SERVICE_WORKER feature is enabled:</p> <pre><code>#if ENABLE(SERVICE_WORKER)\n...\n#endif\n</code></pre> <p>Similarly, the following <code>#if</code> condition will enable the in-between code only on macOS:</p> <pre><code>#if PLATFORM(MAC)\n...\n#endif\n</code></pre> <p>For code which should be enabled in iOS, watchOS, tvOS, and Mac Catalyst we use <code>PLATFORM(IOS_FAMILY)</code>. For each specific variant of iOS family, we also have <code>PLATFORM(IOS)</code>, <code>PLATFORM(WATCHOS)</code>, <code>PLATFORM(APPLETV)</code>, and <code>PLATFORM(MACCATALYST)</code>.</p> <p>The following <code>#if</code> condition will enable the in-between code only if CoreGraphics is used:</p> <pre><code>#if USE(CG)\n...\n#endif\n</code></pre> <p>Finally, if a certain piece of code should only be enabled in an operating system newer than some version, we use  <code>__IPHONE_OS_VERSION_MIN_REQUIRED</code> or <code>__MAC_OS_X_VERSION_MIN_REQUIRED</code>. For example, the following #if enables the in-between code only on macOS 10.14 (macOS Mojave) or above:</p> <pre><code>#if PLATFORM(MAC) &amp;&amp; __MAC_OS_X_VERSION_MIN_REQUIRED &gt;= 101400\n...\n#endif\n</code></pre>"},{"location":"Deep%20Dive/Build/UnifiedBuilds.html","title":"Unified Build System","text":"<p>An overview of how the WebKit build system is structured.</p>"},{"location":"Deep%20Dive/Build/UnifiedBuilds.html#overview","title":"Overview","text":"<p>In order to reduce the compilation time, which used to take 40+ minutes on a fully loaded 2018 15\u201c MacBook Pro, we bundle up multiple C++ translation units (.cpp files) and compile them as a single translation unit. This is a common technique known as Unified Sources or Unified Builds.</p> <p>Unified Sources are generated under <code>WebKitBuild/X/DerivedSources</code> where X is the name of build configuration such as <code>Debug</code> and <code>Release-iphonesimulator</code>. For example, <code>WebKitBuild/Debug/DerivedSources/WebCore/unified-sources/UnifiedSource116.cpp</code> may look like this:</p> <pre><code>#include \"dom/Document.cpp\"\n#include \"dom/DocumentEventQueue.cpp\"\n#include \"dom/DocumentFragment.cpp\"\n#include \"dom/DocumentMarkerController.cpp\"\n#include \"dom/DocumentParser.cpp\"\n#include \"dom/DocumentSharedObjectPool.cpp\"\n#include \"dom/DocumentStorageAccess.cpp\"\n#include \"dom/DocumentType.cpp\"\n</code></pre>"},{"location":"Deep%20Dive/Build/UnifiedBuilds.html#build-failures-with-unified-sources","title":"Build Failures with Unified Sources","text":"<p>Because of Unified Sources, it is possible that adding a new file will cause a new build failure on some platforms. This happens because if <code>UnifiedSource1.cpp</code> contains <code>a.cpp</code>, <code>b.cpp</code>, <code>c.cpp</code>, then <code>#include</code> in <code>a.cpp</code> could have pulled in some header files that <code>c.cpp</code> needed. When you add <code>b2.cpp</code>, and <code>c.cpp</code> moves to <code>UnifiedSource2.cpp</code>, <code>c.cpp</code> no longer benefits from <code>a.cpp</code> \u201caccidentally\u201d satisfying <code>c.cpp</code>\u2019s header dependency. When this happens, you need to add a new <code>#include</code> to <code>c.cpp</code> as it was supposed to be done in the first place.</p>"},{"location":"Deep%20Dive/GitHub/GitConfig.html","title":"Git Config","text":"<p>The WebKit project outlines a simplified recommended setup. This section outlines in greater detail other configuration options certain contributors may prefer.</p>"},{"location":"Deep%20Dive/GitHub/GitConfig.html#remotes","title":"Remotes","text":""},{"location":"Deep%20Dive/GitHub/GitConfig.html#forking","title":"Forking","text":"<p>Since <code>git</code> is a decentralized version control system, a local copy can work with any remote that has the same set of shas. GitHub pull requests take advantage of this. After running <code>git-webkit setup</code>, the <code>.git/config</code> in the local WebKit repository should look something like this:</p> <pre><code>[remote \"origin\"]\n    url = https://github.com/WebKit/WebKit.git\n    fetch = +refs/heads/*:refs/remotes/origin/*\n[remote \"&lt;username&gt;\"]\n    url = https://github.com/&lt;username&gt;/WebKit.git\n    fetch = +refs/heads/*:refs/remotes/&lt;username&gt;/*\n[remote \"fork\"]\n    url = https://github.com/&lt;username&gt;/WebKit.git\n    fetch = +refs/heads/*:refs/remotes/fork/*\n</code></pre> <p>Now, if a contributor runs <code>git push fork eng/some-branch</code>, <code>eng/some-branch</code> will be pushed to the remote <code>fork</code>, which should correspond to that contributor's personal fork of the WebKit project on GitHub. Likewise, running <code>git checkout remotes/fork/eng/some-branch</code> will checkout <code>eng/some-branch</code> according to that contributor's <code>fork</code> remote.</p> <p><code>git-webkit setup</code> also configures a remote with a contributors GitHub username. This is because if other contributors would like to checkout code from a pull request which they do not own, contributors will need to add this:</p> <pre><code>[remote \"&lt;username&gt;\"]\n    url = https://github.com/&lt;username&gt;/WebKit.git\n    fetch = +refs/heads/*:refs/remotes/&lt;username&gt;/*\n</code></pre> <p>to their <code>.git/config</code> and run <code>git checkout remotes/&lt;username&gt;/eng/some-branch</code>. This is what <code>git-webkit checkout pr-#</code> and EWS machines do to retrieve a contributor's change.</p>"},{"location":"Deep%20Dive/GitHub/GitConfig.html#configuration-options","title":"Configuration Options","text":"<p><code>git-webkit setup</code> automatically sets or prompts the contributor to define a number of <code>git</code> configuration options. Most contributors  should use the defaults recommended by <code>git-webkit setup</code>. This section defines, in detail, what an option does and why the WebKit project recommends a certain setting.</p>"},{"location":"Deep%20Dive/GitHub/GitConfig.html#useremail","title":"user.email","text":"<p>Prompts:</p> <pre><code>Set '&lt;email&gt;' as the git user email for this repository\nEnter git user email for this repository: \n</code></pre> <p>The <code>user.email</code> option is usually configured globally, and will become the contact information in <code>git</code> when authoring or committing a change. This is also the part of a commit that GitHub uses when attributing commits to specific users. The email a contributor defines here should be one of that contributor's emails in GitHub so that changes are correctly attributed to the contributor.</p> <p>The WebKit project asks contributors to define this value for their WebKit repository specifically because a contributor's reported email may change over time, and may even differ between machines. <code>git-webkit setup</code>'s prompt is an effort to make contributors think about what their reported contact information for this specific checkout should be.</p> <p>Note that the author and committer listed in a <code>git</code> commit can easily be spoofed, so <code>user.email</code> plays no part in authentication. It is strictly for communication to other contributors.</p>"},{"location":"Deep%20Dive/GitHub/GitConfig.html#username","title":"user.name","text":"<p>Prompts:</p> <pre><code>Set '&lt;name&gt;' as the git user name for this repository\nEnter git user name for this repository: \n</code></pre> <p>The <code>user.name</code> option is usually configured globally, and will become the listed name in <code>git</code> when authoring or committing a change. The name a contributor defines here should be one that individual expects other contributors to use when interacting with them.</p> <p>Note that the author and committer listed in a <code>git</code> commit can easily be spoofed, so <code>user.name</code> plays no part in authentication. It is strictly for communication to other contributors.</p>"},{"location":"Deep%20Dive/GitHub/GitConfig.html#pullrebase","title":"pull.rebase","text":"<p>When a contributor is updating a branch from a remote, a local branch may have commits that do not exist on the remote. This usually happens when a contributor is committing local changes. <code>git</code> supports \"rebasing\" and \"merging\" in these cases.</p> <p>\"rebasing\" means updating the local branch reference to match the remote and then re-applying local commits on top of the tip of the updated branch. For changes which are small relative to the size of the repository, this is the cleanest method of applying local changes to an updated branch.</p> <p>\"merging\" means creating a new \"merge commit\" which has both the most recent commit from the newly updated remote and the most recent local commit as its parents. This technique is useful if the number and magnitude of local commits are large relative to the size of the repository. Note that many project explicitly ban pushing merge commits because they can make bisection and reasoning about continuous integration difficult.</p> <p>The <code>pull.rebase</code> configuration will automatically use a <code>rebase</code> workflow when running <code>git pull</code>. The WebKit project strongly recommends a <code>rebase</code> workflow and does not allow merge commits on <code>main</code> and other protected branches.</p>"},{"location":"Deep%20Dive/GitHub/GitConfig.html#colorstatuscolordiffcolorbranch","title":"color.status/color.diff/color.branch","text":"<p>Prompts:</p> <pre><code>Auto-color status, diff, and branch for this repository?\n</code></pre> <p>Applies coloring to various <code>git</code> commands, most notably <code>git log</code> and <code>git diff</code>. A number of <code>git-webkit</code> commands also use this configuration setting when deciding when to display color. Most users will want to use <code>auto</code>, although contributors who are colorblind may wish to either customize these colors or disable them completely.</p>"},{"location":"Deep%20Dive/GitHub/GitConfig.html#diff","title":"diff.*","text":"<p><code>diff</code> options will apply to different file types and modify the output of <code>git diff</code> to be more human-readable.</p>"},{"location":"Deep%20Dive/GitHub/GitConfig.html#coreeditor","title":"core.editor","text":"<p>Prompts:</p> <pre><code>Pick a commit message editor for this repository:\n    1) [default]\n    2) Sublime\n    3) vi\n    4) open\n</code></pre> <p>When creating or editing commit messages, <code>git</code> will invoke an external editor. By default on most systems, this is <code>vi</code>. The <code>core.editor</code> option lets a user of <code>git</code> change what editor they would like to use globally or within a repository. Note that the invocation of the editor should block until the user closes the editor.</p>"},{"location":"Deep%20Dive/GitHub/GitConfig.html#merge","title":"merge.*","text":"<p><code>git</code> does basic automatic conflict resolution, but certain types of files may be difficult to resolve with what <code>git</code> provides. Specifying a <code>merge.driver</code> for a category of files can help automatically resolve conflicts in these files when running <code>git</code> commands, most notable, <code>git pull</code>. This is most common with frequently changing versioning files or ChangeLogs.</p>"},{"location":"Deep%20Dive/GitHub/GitConfig.html#webkit-options","title":"WebKit Options","text":"<p>[<code>git-webkit</code>] respects a few options that are specific to the <code>webkitscmpy</code> library. <code>git-webkit setup</code> does automatically configure some of these, <code>metadata/project_config</code> also contains a few default values for the project.</p>"},{"location":"Deep%20Dive/GitHub/GitConfig.html#webkitscmpypull-request","title":"webkitscmpy.pull-request","text":"<p>When responding to review feedback, contributors can either append commits to their original changes or force push and overwrite existing commits. <code>git-webkit pull-request</code> supports both workflows, and the <code>webkitscmpy.pull-request</code> option can be set to either <code>overwrite</code> or <code>append</code> to control which workflow <code>git-webkit</code> assumes a contributor is using.</p>"},{"location":"Deep%20Dive/GitHub/GitConfig.html#webkitscmpyhistory","title":"webkitscmpy.history","text":"<p>Prompts:</p> <pre><code>Would you like to create new branches to retain history when you overwrite\na pull request branch?\n    1) [when-user-owned]\n    2) disabled\n    3) always\n    4) never\n</code></pre> <p>Managing pull requests often involves force pushing. This may result in historical changes being lost as a contributor responds to feedback. <code>git-webkit</code> supports saving old branches for the duration of a pull request. Some projects may wish to aggressively disable this option with <code>never</code> because contributors do not own user-specific forks. <code>when-user-owned</code> is generally considered the default option, which will create history branches only when a contributor owns a remote fork and is using the <code>overwrite</code> workflow.</p>"},{"location":"Deep%20Dive/GitHub/GitHub.html","title":"GitHub","text":""},{"location":"Deep%20Dive/GitHub/GitHub.html#overview","title":"Overview","text":"<p>WebKit officially switched over to using GitHub for managing source code in June of 2022. The announcement can be found on the WebKit blog here.</p> <p>WebKit uses a pretty standard git workflow with some custom additions on top.</p>"},{"location":"Deep%20Dive/GitHub/Merging.html","title":"Merging","text":"<p>This page explains the proper way to set test expectations, as well as highlights the differences between <code>safe-merge-queue</code>, <code>merge-queue</code>, and <code>unsafe-merge-queue</code>. There will also be a section that gives tips on what to do when you need to edit an existing pull-request, and how to cleanly re-submit that.</p>"},{"location":"Deep%20Dive/GitHub/Merging.html#test-gardening","title":"Test Gardening","text":"<p>Since direct commit access is limited only to repository administers, this will change the prior workflow of Test Gardening/Setting test expectations. As such the new process is outlined as follows: </p> <ol> <li> <p>From a clean up-to-date checkout, create a new branch using<code>Tools/Scripts/git-webkit branch</code>. It will ask you for either your bug URL, or for just a title. Giving it the bug URL will title the branch the same as the title of your bug. You can manually title it as well, if you prefer. </p> </li> <li> <p>Mark your expectations/ make your changes, followed by <code>git add .</code> and <code>git commit</code>  Changelogs are no longer needed for this, but you will still need to add a commit message.</p> </li> <li> <p>Your commit message MUST contain the term 'Unreviewed' in order for your pull-request to be committed successfully. It is generally recommended that you use the term <code>Unreviewed test gardening.</code> in place of <code>reviewed by nobody. (OOPS)</code> for any commit that is setting any kind of test expectations.</p> </li> </ol> <p>An example template for your commit message looks like:</p> <p></p> <ol> <li> <p>Once your expectations and commit message have been added, you can then submit your pull-request by running <code>Tools/Scripts/git-webkit pr</code>. From there your pull-request will be created, and submitted to WebKit Pull Requests. A link directly to your pull-request will be in your terminal output.</p> </li> <li> <p>Go to your pull-request with the provided link. Then on the right side of the page under the <code>labels</code> tab you will add <code>unsafe-merge-queue</code> just like the example below: </p> </li> <li> <p>After adding the <code>unsafe-merge-queue</code> label, your pull-request will attempt to be committed. This should take 1-3 minutes, and should commit without issue given that you followed the steps above. If there is an issue with your pull-request then the commit will fail with an error.</p> </li> </ol>"},{"location":"Deep%20Dive/GitHub/Merging.html#safe-merge-queue","title":"Safe-Merge-Queue","text":"<p><code>Safe-Merge-Queue</code> is a great way to verify that all builds and tests succeed and land the pull request without manual intervention. <code>Safe-Merge-Queue</code> checks the status of pull requests with the label every 15 minutes. Once all EWS tests pass, <code>Safe-Merge-Queue</code> will automatically land your pull request for you.</p>"},{"location":"Deep%20Dive/GitHub/Merging.html#merge-queue","title":"Merge-Queue","text":"<p>The <code>Merge-Queue</code> is used in conjunction with <code>Tools/Scripts/git-webkit land</code>. When a pull-request goes through merge-queue it will build and test it. After you have pushed your pull-request to WebKit Pull Requests, if you manually add the <code>Merge-Queue</code> label, EWS will determine if your commit has any stylistic issues and if your commit builds on macOS and passes WK2 layout tests. You should use <code>Merge-Queue</code> if you want your pull-request to go through these tests before landing. Once testing is completed, and if a reviewer has approved your pull-request, the <code>Merge-Queue</code> will automatically land your pull-request for you.</p>"},{"location":"Deep%20Dive/GitHub/Merging.html#unsafe-merge-queue","title":"Unsafe-Merge-Queue","text":"<p>When using <code>Unsafe-Merge-Queue</code> a style check will be run on your pull-request, and then it will attempt to commit the pull-request without building it or testing it. As long as there are no stylistic or formatting issues, <code>Unsafe-Merge-Queue</code> will commit your change in approx. 1-3 minutes. <code>Unsafe-Merge-Queue</code> should ONLY be used for very simple, basic changes that can't really break anything too badly, or setting expectations for tests.</p>"},{"location":"Deep%20Dive/GitHub/Migration.html","title":"Migration","text":"<p>WebKit has moved away from Subversion to Git, and contributors will need to migrate their local checkouts and workflows to GitHub. To determine which migration workflow you need, run the following command in your WebKit repository:</p> <pre><code>git remote -v\n</code></pre> <p>If you see something like:</p> <pre><code>fatal: not a git repository (or any parent up to mount point /Volumes)\nStopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n</code></pre> <p>you have a Subversion checkout and need to migrate to a GitHub checkout.</p> <p>If you see something like:</p> <pre><code>origin    https://git.webkit.org/git/WebKit-https (fetch)\norigin    https://git.webkit.org/git/WebKit-https (push)\n</code></pre> <p>or:</p> <pre><code>origin    https://git.webkit.org/git/WebKit (fetch)\norigin    https://git.webkit.org/git/WebKit (push)\n</code></pre> <p>you have an obsolete git mirror for how to deal with patches uploaded to bugzilla.) and need to migrate to a GitHub checkout.</p> <p>If you see something like:</p> <pre><code>origin    git@github.com:WebKit/WebKit.git (fetch)\norigin    git@github.com:WebKit/WebKit.git (push)\n</code></pre> <p>you already have a GitHub checkout, and can skip to setting up your checkout.</p>"},{"location":"Deep%20Dive/GitHub/Migration.html#subversion","title":"Subversion","text":"<p>Migrating from a Subversion checkout involves first cloning the WebKit repository with <code>git clone https://github.com/WebKit/WebKit.git &lt;local path&gt;</code>. A SSH based clone will work as well, but will require an SSH key to be uploaded.</p> <p>After your clone completes, set up your checkout. If you don't have local changes in your Subversion checkout, no more migration work is required. If you do have local changes, use <code>Tools/Scripts/svn-create-patch</code> to save those changes in a local <code>.patch</code> file and then run <code>Tools/Scripts/svn-apply</code> to apply that patch to your new GitHub clone. See below for how to deal with patches uploaded to bugzilla.</p>"},{"location":"Deep%20Dive/GitHub/Migration.html#obsolete-mirror","title":"Obsolete Mirror","text":"<p>The repository hosted via github.com/WebKit/WebKit has different commits from the ones hosted on git.webkit.org. While it is possible convert an existing checkout, the WebKit team recomends that you freshly clone the WebKit repository with <code>git clone https://github.com/WebKit/WebKit.git &lt;local path&gt;</code>. A SSH based clone will work as well, but will require an SSH key to be uploaded.</p> <p>After your clone completes, set up your checkout. The WebKit team has not built automation to assist in migrating branches from git.webkit.org checkouts to github.com/WebKit/WebKit ones. However, the content on disk in both repositories is identical. That means that something like <code>git -C &lt;oldpath&gt; diff HEAD~1 | git -C &lt;newpath&gt; apply</code> will apply a commit from one checkout to the other, assuming the diff of the commit you're moving applies to commit you have checked out.</p> <p>See below for how to deal with patches uploaded to bugzilla.</p>"},{"location":"Deep%20Dive/GitHub/Migration.html#webkit-patch","title":"webkit-patch","text":"<p>While <code>Tools/Scripts/webkit-patch</code> is being replaced by <code>Tools/Scripts/git-webkit</code> for developement workflows, <code>webkit-patch</code> does continue to work on GitHub based checkouts. In particular:</p> <pre><code>Tools/Scripts/webkit-patch apply-from-bug 238981\n</code></pre> <p>will apply the patch uploaded to bug 238981 to a user's local checkout, even if that checkout is a GitHub based checkout. Simlarly, this command:</p> <pre><code>Tools/Scripts/webkit-patch upload\n</code></pre> <p>will upload local changes to the bugzilla bug mentioned in modified <code>ChangeLog</code> files, even if those local changes are committed to a pull request branch.</p>"},{"location":"Deep%20Dive/GitHub/Migration.html#webkit-patch-reverse-look-up","title":"webkit-patch Reverse Look-up","text":"<p>The table bellow includes a number of common <code>webkit-patch</code> commands and their <code>git-webkit</code> equivalences.</p> <code>webkit-patch</code> <code>git-webkit</code> Description <code>webkit-patch apply-attachment</code> <code>git-webkit checkout pr-#</code> Get another contributor's unlanded change <code>webkit-patch clean</code> <code>git-webkit clean</code> Discard uncommitted local changes <code>webkit-patch create-revert &lt;revisions&gt;</code> <code>git-webkit revert --pr &lt;hash/identifier&gt;</code> Upload a proposal to revert a landed change <code>webkit-patch help -a</code> <code>git-webkit --help</code> Print program help message <code>webkit-patch land</code> <code>git-webkit land</code> Land a local change via Commit/Merge Queue <code>webkit-patch land-unsafe</code> <code>git-webkit land --unsafe</code> Land a local change manually/via Unsafe Merge Queue <code>webkit-patch prepare-revert &lt;revisions&gt;</code> <code>git-webkit revert &lt;hash/identifier&gt;</code> Revert a landed change locally <code>webkit-patch setup-git-clone</code> <code>git-webkit setup</code> Configure a local checkout for development <code>webkit-patch upload</code> <code>git-webkit pr</code> Upload a change for review"},{"location":"Deep%20Dive/GitHub/PullRequests.html","title":"Pull Requests","text":"<p>The WebKit project has a number of expectations for how pull requests are formatted. These expectations are codified in <code>Tools/Scripts/git-webkit pr</code>, which contributors can run after drafting a change locally to configure a pull request. The WebKit project recommends you follow and leverage our tools. If contributors would like to use alternative tools, this document explains what <code>Tools/Scripts/git-webkit pr</code> is doing and how WebKit expects pull requests to be formatted.</p>"},{"location":"Deep%20Dive/GitHub/PullRequests.html#bug-tracking","title":"Bug Tracking","text":"<p>The first step of most pull requests is creating a bug. While it is not expected that every pull request has a unique bug, it is expected that every landed commit can be linked back to a bug report. In particular, pull requests which are regressions or follow-up fixes often reference the bug the original commit references.</p>"},{"location":"Deep%20Dive/GitHub/PullRequests.html#branching","title":"Branching","text":"<p>Pull request branches are owned by their author, which is why <code>git-webkit setup</code> creates a personal fork of WebKit. This means that the WebKit project cannot enforce branching idioms, although there are some suggestions the WebKit team has so that other contributors can more easily access proposed changes. <code>Tools/Scripts/git-webkit pr</code> derives it's <code>eng</code> prefixed branch from the bug title of the bug associated with a pull request.</p> <p>We suggest that pull request branch names are prefixed by <code>eng/</code> or <code>dev/</code> so that contributors are clear which branches contain production code when they add other user's forks as remotes. Notably, EWS is unable to apply changes which come from the branch they are targeting (ie, EWS cannot apply a change from <code>Contributor/WebKit:main</code> onto <code>WebKit/WebKit:main</code>), so in order to be reviewed, changes must come from a different branch.</p>"},{"location":"Deep%20Dive/GitHub/PullRequests.html#commit-messages","title":"Commit Messages","text":"<p>The WebKit project heavily relies on commit messages to defend project performance and correctness along with using them to manage releases. In support of this, the WebKit project mandates the following in every commit message:</p> <ul> <li>Bug title</li> <li>Bug url</li> <li>Reviewer (or explicit reason why a change is unreviewed)</li> <li>High level explanation (optional)</li> <li>Files changes, what was changed, and why</li> </ul> <p><code>git-webkit setup</code> configures <code>.git/prepare-commit-msg</code> such that your commit message template is formatted to the standards of the WebKit project.</p> <p>In addition to pull request commits having verbose commit messages, the WebKit project also expects the content of commit messages in the pull request description. This is so that reviewers can provide feedback on the commit message itself.</p>"},{"location":"Deep%20Dive/GitHub/PullRequests.html#reviewing","title":"Reviewing","text":"<p>Commits generally require the approval of a reviewer, although there are narrow exceptions for test expectation gardening and build fixes. Reviewers will approve pull requests through the GitHub UI by marking pull requests as \"Approved.\" Note that approval from someone who is not a reviewer will not be recognized by the Merge-Queues.</p>"},{"location":"Deep%20Dive/GitHub/PullRequests.html#landing","title":"Landing","text":"<p>Only repository administrators have direct commit access, and this is reserved for repairing infrastructure issues. Pull requests should be landed through the Merge-Queues, which is achieved by applying the <code>safe-merge-queue</code>, <code>merge-queue</code>, or <code>unsafe-merge-queue</code> label to your pull request.</p> <p>Each queue makes sure a change is reviewed by adding the name of all reviewers who have approved a pull request to the commit message. It will then check the commit message for <code>Reviewed by</code>.</p> <p>Note that the Merge-Queues will reject pull requests labeled by contributors who are not committers.</p>"},{"location":"Deep%20Dive/GitHub/SourceControl.html","title":"Source Control","text":"<p>The information outlined in this section is intended for a future where Git is the source of truth for the project.</p>"},{"location":"Deep%20Dive/GitHub/SourceControl.html#commit-representation","title":"Commit Representation","text":"<p>The WebKit project heavily relies on a linear, ordered history on the <code>main</code> branch to track regressions in the project. Historically, Subversion's revisions were used across our commit messages, bug tracking and services to achieve this goal. Our migration to <code>git</code> has required a new solution, because while <code>git</code> is capable on enforcing a linear and ordered history (so long as merge commits are banned), <code>git</code> commits are traditionally represented as hashes, which are not trivially orderable the way Subversion's revisions are.</p> <p>The WebKit teams has instead adopted a system where commits are represented based on their relationship to the default branch and number of ancestors they have, we have dubbed this representation the commit identifier. Most tooling accepts <code>git</code> hashes, Subversion revisions and identifiers, although the <code>Tools/Scripts/git-webkit</code> script can convert between the three representations locally, if the need arises.</p> <p>To use this commit representation for local development, <code>Tools/Scripts/git-webkit</code> implements a <code>blame</code> and <code>log</code> sub-command that include commit identifiers and Subversion revisions, if available.</p> <pre><code>Tools/Scripts/git-webkit blame Makefile\n\n230258@main (Keith Rollin    2020-10-08 19:10:32 +0000  1) MODULES = Source Tools\n184786@main (Jonathan Bedard 2017-02-02 18:42:02 +0000  2) \n229628@main (Keith Rollin    2020-09-22 18:37:51 +0000  3) define build_target_for_each_module\n229628@main (Keith Rollin    2020-09-22 18:37:51 +0000  4)      for dir in $(MODULES); do \\\n229628@main (Keith Rollin    2020-09-22 18:37:51 +0000  5)              ${MAKE} $@ -C $$dir PATH_FROM_ROOT=$(PATH_FROM_ROOT)/$${dir}; \\\n229628@main (Keith Rollin    2020-09-22 18:37:51 +0000  6)              exit_status=$$?; \\\n229628@main (Keith Rollin    2020-09-22 18:37:51 +0000  7)              [ $$exit_status -ne 0 ] &amp;&amp; exit $$exit_status; \\\n229628@main (Keith Rollin    2020-09-22 18:37:51 +0000  8)      done; true\n229628@main (Keith Rollin    2020-09-22 18:37:51 +0000  9) endef\n...\n</code></pre> <pre><code>Tools/Scripts/git-webkit log\n\ncommit 240867@main (989ff515ce6e103271072dd1b397ac43572a910c, r281493)\nAuthor: Adrian Perez de Castro &lt;aperez@igalia.com&gt;\nDate:   Tue Aug 24 13:48:35 2021 +0000\n\n    Non-unified build fixes, late August 2021\n    https://bugs.webkit.org/show_bug.cgi?id=229440\n\n    Unreviewed non-unified build fixes.\n...\n</code></pre>"},{"location":"Deep%20Dive/GitHub/SourceControl.html#identifiers","title":"Identifiers","text":"<p>Identifiers are the term the WebKit Team uses to refer to the representation of a commit our team has developed which uniquely identifies a commit based on that commit's relationship to the default branch and the number of ancestors that commit has.</p> <p>Commit identifiers are of the following form:</p> <pre><code>&lt;branch-point&gt;.&lt;number&gt;@&lt;branch&gt;\n</code></pre> <p>where the <code>branch</code> is the name of a <code>git</code> branch the commit is on, the <code>number</code> is the number of ancestors that commit has (if the commit is on the default branch) or the number of ancestors that commit has since diverging from the default branch (if the commit is not on the default branch) and <code>branch-point</code> optionally displays the number of ancestors on the default branch a commit has (only relevant for commits not on the default branch).</p> <p>The timeline bellow shows what identifiers looks like with multiple branches:</p> <pre><code>                      \u2014\u2014\u2014\u2014 o \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014 o\n                    /      |               |\n                   /  101.2@branch-b  101.3@branch-b\n                  \u2014\u2014\u2014\u2014\u2014\u2014\u2014 o \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014 o\n                /         |              |\n           /   101.1@branch-a  101.2@branch-a\n\u2014\u2014\u2014 o \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014 o \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014 o \u2014\u2014\u2014\u2014\u2014\u2014\u2014 o \u2014\u2014\u2014\u2014\u2014\u2014\u2014 o\n    |          |          |         |         |\n 100@main   101@main   102@main  103@main  104@main\n</code></pre> <p>It is worth noting that commits, especially those on branches, have multiple valid identifiers. In the above example, <code>101.1@branch-a</code> could also be referred to as <code>101.1@branch-b</code> and <code>101@main</code> could be referred to as <code>101.0@branch-a</code>. The WebKit team has defined the canonical identifier for a given commit to be that commit's identifier on the least specific branch that commit is on. The metric for branch specificity is outlined in <code>Tools/Scripts/libraries/webkitscmpy/webkitscmpy/scm_base.py</code>, but can essentially be thought of like this:</p> <pre><code>default branch                               (least specific)\nproduction branches\n    \"a\" branch name\n    \"z\" branch name\ndevelopment branches (eng/*, dev/*. ect.)\n    \"a\" branch name\n    \"z\" branch name                          (most specific)\n</code></pre> <p>Conversion between native <code>git</code> refs and identifiers can be done with <code>Tools/Scripts/git-webkit find</code>:</p> <pre><code>Tools/Scripts/git-webkit find safari-611-branch\n\nTitle: Unreviewed build fix, rdar://problem/76412930\nAuthor: Russell Epstein &lt;repstein@apple.com&gt;\nDate: Fri Apr 16 14:34:51 2021\nRevision: 276171\nHash: 67dd5465d8f5\nIdentifier: 232923.433@remotes/fork/safari-611-branch\n</code></pre> <pre><code>git-webkit find 232923.400@safari-611-branch\n\nTitle: Revert \"Cherry-pick r271794. rdar://problem/76375364\"\nAuthor: Commit Queue &lt;commit-queue@webkit.org&gt;\nDate: Thu Apr 15 13:15:48 2021\nRevision: 276064\nHash: dd1f0d38426c\nIdentifier: 232923.400@remotes/fork/safari-611-branch\n</code></pre> <p>Or through commits.webkit.org if no checkout is available:</p> <pre><code>curl https://commits.webkit.org/safari-612-branch/json\n\n{\n    \"author\": {\n        \"emails\": [\n            \"repstein@apple.com\"\n        ],\n        \"name\": \"Russell Epstein\"\n    },\n    \"branch\": \"safari-612-branch\",\n    \"hash\": \"76f038bbe2889a3714c6176b3c9e35b404c57e35\",\n    \"identifier\": \"240672.6@safari-612-branch\",\n    \"message\": \"Versioning.\\n\\nWebKit-7612.2.1\\n\\nCanonical link: https://commits.webkit.org/240672.6@safari-612-branch\\ngit-svn-id: https://svn.webkit.org/repository/webkit/branches/safari-612-branch@281269 268f45cc-cd09-0410-ab3c-d52691b4dbfc\",\n    \"order\": 0,\n    \"repository_id\": \"webkit\",\n    \"revision\": 281269,\n    \"timestamp\": 1629406217\n}\n</code></pre>"},{"location":"Deep%20Dive/GitHub/SourceControl.html#branch-management","title":"Branch Management","text":"<p>The WebKit project aims to keep branches clean, development should be primarily done on forks of the repository owned by developers instead of on the WebKit repository itself. Branches pushed to the WebKit repository should either be production branches or temporary branches owned by automation.</p>"},{"location":"Deep%20Dive/GitHub/SourceControl.html#production-branches","title":"Production Branches","text":"<p>Most WebKit development should be done on <code>main</code>, which is our default branch. Note that Subversion's <code>trunk</code> branch tracked the same set of commits that the modern <code>main</code> branch does. <code>main</code> is protected by Commit Queue, as outlined in the Permissions heading.</p> <p>Other production branches are managed by specific platforms as part of their release cycle. Most notably, the <code>safari-*-branch</code> set of branches correspond to versions of WebKit released by Apple.</p>"},{"location":"Deep%20Dive/GitHub/SourceControl.html#temporary-branches","title":"Temporary Branches","text":"<p>Branches may be added, temporarily, by automation and contributors interacting with automation. The branches are expected to be deleted within 48 hours of being added to the project. Commit Queue is the most notable example of this. Branches named <code>commit-queue/*</code> and <code>fast-commit-queue/*</code> represent code to be committed to a production branch after passing a verification process. (NB, work in progress)</p>"},{"location":"Deep%20Dive/GitHub/SourceControl.html#merge-commits","title":"Merge Commits","text":"<p>The WebKit project forbids merge-commits on production branches.</p> <p>Merge-commits are a type of commit where a <code>git</code> commit may have multiple parents, the history of a merge commit looks something like this:</p> <pre><code>                  \u2014\u2014\u2014 o \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014 o \u2014\u2014\u2014\u2014\n                /                           \\\n           /                             \\ \n\u2014\u2014\u2014 o \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014 o \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014 o \u2014\u2014\u2014\u2014\u2014\u2014\u2014 o \u2014\u2014\u2014\u2014\u2014\u2014\u2014 o \u2014\u2014\u2014\u2014\u2014\u2014\u2014 o\n</code></pre> <p>Merge-commits make bisection difficult, and make it hard for humans to reason about the code in a specific commit when working backwards. As a result, all production branches forbid merge commits.</p>"},{"location":"Deep%20Dive/GitHub/SourceControl.html#permissions","title":"Permissions","text":"<p>Only administers and Commit Queue have direct access to <code>main</code>. Instead, committers are granted access to push branches named <code>commit-queue/*</code> and <code>fast-commit-queue</code>, which are then checked before being rebased and landed on <code>main</code> (NB, work in progress).</p> <p>Branches matching <code>safari-*-branch</code> are managed by Apple's Integrators.</p>"},{"location":"Deep%20Dive/GitHub/SourceControl.html#canonicalization","title":"Canonicalization","text":"<p>To make identifiers easier to user, Commit Queue adds those identifiers to commit messages via a link to commits.webkit.org. We call this process \"canonicalization.\" In addition to adding identifiers to commit messages, canonicalization attempts to parse the commit message to correctly attribute changes which may be authored and committed by different contributors.</p> <p>The task of canonicalization is owned by Commit Queue and is done immediately before pushing changes to a production branch. Canonicalization should not be preformed on non-production branches.</p>"},{"location":"Deep%20Dive/JSC/JSCObjectAccessOptimization.html","title":"Object Access Optimization","text":"<p>Summary</p>"},{"location":"Deep%20Dive/JSC/JSCObjectAccessOptimization.html#overview","title":"Overview","text":"<p>FIXME: Need to Write</p> <p>Note that a pretty good summary of how we optimize code is in \u200bhttps://www.webkit.org/blog/3362/introducing-the-webkit-ftl-jit/.</p>"},{"location":"Deep%20Dive/JSC/JSCTypeInference.html","title":"Type Inference","text":"<p>Summary</p>"},{"location":"Deep%20Dive/JSC/JSCTypeInference.html#overview","title":"Overview","text":"<p>Type inference is achieved by profiling values, predicting what types operations will produce based on those profiles, inserting type checks based on the type predictions, and then attempting to construct type proofs about the types of values based on the type checks.</p> <p>Consider this example to motivate type inference, and optimization, of JavaScript:</p> <pre><code>o.x * o.x + o.y * o.y\n</code></pre> <p>Say that in the context where this code is used, 'o' is an object, and it indeed has properties 'x' and 'y', and those properties are nothing special - in particular, they are not accessors. Let's also say that 'o.x' and 'o.y' usually return doubles, but may sometimes return integers - JavaScript does not have a built-in notion of an integer value, but for efficiency JavaScriptCore will represent most integers as int32 rather than as double. To understand both the problem of type inference, and its solution in JavaScriptCore, it's useful to first consider what a JavaScript engine would have to do if it had no information about 'o', 'o.x', or 'o.y':</p> <p>The expression 'o.x' has to first check if 'o' has any special handling of property access. It may be a DOM object, and DOM objects may intercept accesses to their properties in non-obvious ways. If it doesn't have special handling, the engine must look up the property named \"x\" (where \"x\" is literally a string) in the object. Objects are just tables mapping strings to either values or accessors. If it maps to an accessor, the accessor must be called. If it isn't an accessor, then the value is returned. If \"x\" is not found in 'o', then the process repeats for o's prototype. The inference required for optimizing the object access is not covered in this section. The binary multiply operation in the expression 'o.x * o.x' has to first check the types of its operands. Either operand may be an object, in which case its 'valueOf' method must be called. Either operand may be a string, in which case it must be converted to a number. Once both operands are appropriately converted to numbers (or if they were numbers already), the engine must check if they are both integers; if so, then an integer multiply is performed. This may overflow, in which case a double multiply is performed instead. If either operand is a double, then both are converted to doubles, and a double multiply is performed. Thus 'o.x * o.x' may return either an integer or a double. There is no way of proving, for the generic JavaScript multiply, what kind of number it will return and how that number will be represented. The binary addition operation in the expression 'o.x * o.x + o.y * o.y' has to proceed roughly as the multiply did, except it has to consider the possibility that its operands are strings, in which case a string concatenation is performed. In this case, we could statically prove that this isn't the case - multiply must have returned a number. But still, addition must perform checks for integers versus doubles on both operands, since we do not know which of those types was returned by the multiplication expressions. As a result, the addition may also return either an integer, or a double. The intuition behind JavaScriptCore's type inference is that we can say with good probability what type a numerical operation (such as addition or multiplication) will return, and which path it will take, if we could guess what types flowed into that operation. This forms a kind of induction step that applies to operations that don't generally operate over the heap: if we can predict their inputs, then we can predict their outputs. But induction requires a base case. In the case of JavaScript operations, the base cases are operations that get non-local values: for example, loading a value from the heap (as in 'o.x'), accessing an argument to a function, or using a value returned from a function call. For simplicity, we refer to all such non-local values as heap values, and all operations that place heap values into local variables as heap operations. For arguments, we treat the function prologue as a \"heap operation\" of sorts, which \"loads\" the arguments into the argument variables. We bootstrap our inductive reasoning about type predictions by using value profiling: both the LLInt and Baseline JIT will record the most recent value seen at any heap operation. Each heap operation has exactly one value profile bucket associated with it, and each value profile bucket will store exactly one recent value.</p> <p>A straw-man view of JavaScriptCore's type inference is that we convert each value profile's most recent value into a type, and then apply the induction step to propagate this type through all operations in the function. This gives us type predictions at each value-producing operation in the function. All variables become predictively typed as well.</p> <p>In reality, JavaScriptCore includes in each value profile a second field, which is a type that bounds a random subset of the values seen. This type uses the SpeculatedType (or SpecType for short) type system, which is implemented in \u200bSpeculatedType.h. The type of each value profile starts out as SpecNone - i.e. the type corresponding to no values. You can also think of this as bottom (see \u200bLattice), or the \"contradiction\". When the Baseline JIT's execution counter exceeds a threshold (see JIT::emitOptimizationCheck in \u200bJIT.cpp), it will produce a new type to bound both the last type, and the most recent value. It may also then choose to invoke the DFG, or it may choose to let the baseline code run more. Our heuristics favor the latter, meaning that when DFG compilation kicks in, each value profile will typically have a type that bounds multiple different values.</p> <p>The propagation of SpecTypes derived at value profiles to all operations and variables in a function is performed using a standard \u200bforward data flow formulation, implemented as a flow-insensitive fixpoint. This is one of the first phases of DFG compilation, and is only activated once the Baseline JIT decides, based on its execution count, that a function would be better off executing optimized code. See \u200bDFGPredictionPropagationPhase.cpp.</p> <p>After each value in a function is labeled with a predicted type, we insert speculative type checks based on those predictions. For example, in a numeric operation (like 'o.x * o.y'), we insert speculate-double checks on the operands of the multiplication. If a speculation check fails, execution is diverted from optimized DFG code back to the Baseline JIT. This has the effect of proving the type for subsequent code within the DFG. Consider how the simple addition operation 'a + b' will be decomposed, if 'a' and 'b' both had SpecInt32 as their predicted type:</p> <pre><code>check if a is Int32 -&gt; else OSR exit to Baseline JIT\ncheck if b is Int32 -&gt; else OSR exit to Baseline JIT\nresult = a + b // integer addition\ncheck if overflow -&gt; else OSR exit to Baseline JIT\n</code></pre> <p>After this operation completes, we know that:</p> <ul> <li>'a' is an integer.</li> <li>'b' is an integer.</li> <li>the result is an integer.</li> </ul> <p>Any subsequent operations on either 'a' or 'b' do not need to check their types. Likewise for operations on the result. The elimination of subsequent checks is achieved by a second data flow analysis, called simply the DFG CFA. Unlike the prediction propagation phase, which is concerned with constructing type predictions, the CFA is concerned with constructing type proofs. The CFA, found in \u200bDFGCFAPhase.cpp and \u200bDFGAbstractInterpreterInlines.cpp, follows a flow-sensitive forward data flow formulation. It also implements sparse conditional constant propagation, which gives it the ability to sometimes prove that values are constants, as well as proving their types.</p> <p>Putting this together, the expression 'o.x * o.x + o.y * o.y' will only require type checks on the value loaded from 'o.x' and the value loaded from 'o.y'. After that, we know that the values are doubles, and we know that we only have to emit a double multiply path followed by a double addition. When combined with type check hoisting, DFG code will usually execute a type check at most once per heap load.</p>"},{"location":"Deep%20Dive/JSC/JavaScriptCore.html","title":"JavaScriptCore","text":"<p>JavaScriptCore is the built-in JavaScript engine for WebKit, which implements \u200bECMAScript as in \u200bECMA-262 specification.</p>"},{"location":"Deep%20Dive/JSC/JavaScriptCore.html#overview","title":"Overview","text":"<p>JavaScriptCore is often referred with different names, such as \u200bSquirrelFish and \u200bSquirrelFish Extreme. Within the context of Safari, Nitro and Nitro Extreme (the marketing terms from Apple) are also commonly used. However, the name of the project and the library is always JavaScriptCore.</p> <p>JavaScriptCore source code resides in WebKit source tree, it's under \u200bSource/JavaScriptCore directory.</p>"},{"location":"Deep%20Dive/JSC/JavaScriptCore.html#core-engine","title":"Core Engine","text":"<p>JavaScriptCore is an optimizing virtual machine. JavaScriptCore consists of the following building blocks: lexer, parser, start-up interpreter (LLInt), baseline JIT, a low-latency optimizing JIT (DFG), and a high-throughput optimizing JIT (FTL).</p>"},{"location":"Deep%20Dive/JSC/JavaScriptCore.html#lexer","title":"Lexer","text":"<p>Lexer is responsible for the \u200blexical analysis, i.e. breaking down the script source into a series of tokens. JavaScriptCore lexer is hand-written, it is mostly in \u200bparser/Lexer.h and \u200bparser/Lexer.cpp.</p>"},{"location":"Deep%20Dive/JSC/JavaScriptCore.html#parser","title":"Parser","text":"<p>Parser carries out the \u200bsyntactic analysis, i.e. consuming the tokens from the lexer and building the corresponding syntax tree. JavaScriptCore uses a hand-written \u200brecursive descent parser, the code is in \u200bparser/JSParser.h and \u200bparser/JSParser.cpp.</p>"},{"location":"Deep%20Dive/JSC/JavaScriptCore.html#llint","title":"LLInt","text":"<p>LLInt, short for Low Level Interpreter, executes the bytecodes produced by the parser. The bulk of the Low Level Interpreter is in \u200bllint/. The LLInt is written in a portable assembly called offlineasm (see \u200bofflineasm/, which can compile to x86, ARMv7, and C. The LLInt is intended to have zero start-up cost besides lexing and parsing, while obeying the calling, stack, and register conventions used by the just-in-time compilers. For example, calling a LLInt function works \"as if\" the function was compiled to native code, except that the machine code entrypoint is actually a shared LLInt prologue. The LLInt includes optimizations such as inline caching to ensure fast property access.</p>"},{"location":"Deep%20Dive/JSC/JavaScriptCore.html#baseline","title":"Baseline","text":"<p>Baseline JIT kicks in for functions that are invoked at least 6 times, or take a loop at least 100 times (or some combination - like 3 invocations with 50 loop iterations total). Note, these numbers are approximate; the actual heuristics depend on function size and current memory pressure. The LLInt will on-stack-replace (OSR) to the JIT if it is stuck in a loop; as well all callers of the function are relinked to point to the compiled code as opposed to the LLInt prologue. The Baseline JIT also acts as a fall-back for functions that are compiled by the optimizing JIT: if the optimized code encounters a case it cannot handle, it bails (via an OSR exit) to the Baseline JIT. The Baseline JIT is in \u200bjit/. The Baseline JIT also performs sophisticated polymorphic inline caching for almost all heap accesses.</p> <p>Both the LLInt and Baseline JIT collect light-weight profiling information to enable speculative execution by the next tier of execution (the DFG). Information collected includes recent values loaded into arguments, loaded from the heap, or loaded from a call return. Additionally, all inline caching in the LLInt and Baseline JIT is engineered to enable the DFG to scrape type information easily: for example the DFG can detect that a heap access sometimes, often, or always sees a particular type just by looking at the current state of an inline cache; this can be used to determine the most profitable level of speculation. A more thorough overview of type inference in JavaScriptCore is provided in the next section.</p>"},{"location":"Deep%20Dive/JSC/JavaScriptCore.html#dfg","title":"DFG","text":"<p>DFG JIT kicks in for functions that are invoked at least 60 times, or that took a loop at least 1000 times. Again, these numbers are approximate and are subject to additional heuristics. The DFG performs aggressive type speculation based on profiling information collected by the lower tiers. This allows it to forward-propagate type information, eliding many type checks. Sometimes the DFG goes further and speculates on values themselves - for example it may speculate that a value loaded from the heap is always some known function in order to enable inlining. The DFG uses deoptimization (we call it \"OSR exit\") to handle cases where speculation fails. Deoptimization may be synchronous (for example, a branch that checks that the type of a value is that which was expected) or asynchronous (for example, the runtime may observe that the shape or value of some object or variable has changed in a way that contravenes assumptions made by the DFG). The latter is referred to as \"watchpointing\" in the DFG codebase. Altogether, the Baseline JIT and the DFG JIT share a two-way OSR relationship: the Baseline JIT may OSR into the DFG when a function gets hot, and the DFG may OSR to the Baseline JIT in the case of deoptimization. Repeated OSR exits from the DFG serve as an additional profiling hint: the DFG OSR exit machinery records the reason of the exit (including potentially the values that failed speculation) as well as the frequency with which it occurred; if an exit is taken often enough then reoptimization kicks in: callers are relinked to the Baseline JIT for the affected function, more profiling is gathered, and then the DFG may be later reinvoked. Reoptimization uses exponential back-off to defend against pathological code. The DFG is in \u200bdfg/.</p>"},{"location":"Deep%20Dive/JSC/JavaScriptCore.html#ftl","title":"FTL","text":"<p>FTL JIT kicks in for functions that are invoked thousands of times, or loop tens of thousands of times. See FTLJIT for more information.</p> <p>At any time, functions, eval blocks, and global code in JavaScriptCore may be executing in a mix of the LLInt, Baseline JIT, DFG, and FTL. In the extreme case of a recursive function, there may even be multiple stack frames where one frame is in the LLInt, another is in the Baseline JIT, while another still is in the DFG or even FTL; even more extreme are cases where one stack frame is executing an old DFG or FTL compilation and another is executing a new DFG or FTL compilation because recompilation kicked in but execution did not yet return to the old DFG/FTL code. These four engines are designed to maintain identical execution semantics, and so even if multiple functions in a JavaScript program are executing in a mix of these engines the only perceptible effect ought to be execution performance.</p>"},{"location":"Deep%20Dive/Layout%20%26%20Rendering/DisplayTree.html","title":"LFC Display Tree","text":"<p>Layout Formatting Context display tree and related functionality (aka \"LFC Display\").</p>"},{"location":"Deep%20Dive/Layout%20%26%20Rendering/DisplayTree.html#basic-concepts","title":"Basic concepts","text":"<p>The display tree is intended to be a fully-resolved, isolated tree that is suitable for painting and hit-testing.</p> <p>Fully-resolved means that all style values have been resolved to the values use for painting. For example, colors in the display tree are those which result from the application of visited link rules, and from the application of <code>-apple-color-filter</code>.</p> <p>Isolated means that a display tree is a stand-alone data structure that can be painted without reference to data structures share with layout. For example, the display tree does not use <code>RenderStyle</code> once built, but constructs its own display styles at tree-building time.</p> <p>All the geometry in the display tree is already in painting coordinates: pixel snapping happens at tree building time.</p> <p>In general the tree has been designed to push as much of the complexity to tree-building time as possible, so that painting is fast.</p> <p>The tree objects themselves are intentionally lightweight, mostly data-only objects. Complex tree-walking code lives in external objects that know how to do certain things, like paint the box tree.</p> <p>Boxes in the display tree are intended to be fairly generic, and not have properties which are very CSS- or SVG-specific. It's hoped that SVG and CSS rendering share many of the same box classes.</p> <p>There isn't necessarily a 1:1 relationship between <code>Layout::Box</code> and <code>Display::Box</code>. The display tree may elide boxes that paint nothing, or may create extra boxes for functionality like scrolling.</p>"},{"location":"Deep%20Dive/Layout%20%26%20Rendering/DisplayTree.html#relevant-classes","title":"Relevant classes","text":""},{"location":"Deep%20Dive/Layout%20%26%20Rendering/DisplayTree.html#displayview","title":"Display::View","text":"<p>Represents a presentation of a display tree. Is the entry point for hit-testing.</p> <p>Currently <code>FrameView</code> owns a <code>Display::View</code>.</p>"},{"location":"Deep%20Dive/Layout%20%26%20Rendering/DisplayTree.html#displaytree","title":"Display::Tree","text":"<p>Owns the hierarchy of <code>Display::Box</code> objects via <code>Display::StackingItem</code> objects, and associated tree-related data structures. It should be possible for a <code>Display::Tree</code> to exist independently of a <code>Display::View</code>.</p>"},{"location":"Deep%20Dive/Layout%20%26%20Rendering/DisplayTree.html#displaytreebuilder","title":"Display::TreeBuilder","text":"<p><code>Display::TreeBuilder</code> is a short-lived class that knows how to build a <code>Display::StackingItem</code>/<code>Display::Box</code> tree from a <code>Layout::Box</code> tree, <code>LayoutState</code> and style. It maintains various bits of state in a stack as it's building the display tree in order to track containing blocks etc. <code>Display::TreeBuilder</code> knows about hierarchy, not geometry.</p>"},{"location":"Deep%20Dive/Layout%20%26%20Rendering/DisplayTree.html#displayboxfactory","title":"Display::BoxFactory","text":"<p><code>Display::BoxFactory</code> actually creates the <code>Display::Box</code> objects and their related objects like <code>Display::BoxGeometry</code>. It exists to keep the style and geometry code out of <code>Display::TreeBuilder</code>. It only ever deals with a single box at a time.</p>"},{"location":"Deep%20Dive/Layout%20%26%20Rendering/DisplayTree.html#displaybox","title":"Display::Box","text":"<p><code>Display::Box</code> is the basic building block for the display tree; it represents some rectangular region, often with painted content. There are subclasses for text, images etc. <code>Display::Box</code> has sibling pointers and a parent pointer. <code>Display::ContainerBox</code> is used for boxes with children, so has a pointer to its first child.</p> <p>The hierarchy of <code>Display::Box</code> objects forms a tree, but it's not a complete tree, because each <code>Display::StackingItem</code> owns a part of the tree, and the box objects in those subtrees are not connected.</p>"},{"location":"Deep%20Dive/Layout%20%26%20Rendering/DisplayTree.html#displaystackingitem","title":"Display::StackingItem","text":"<p>This class exists to represent parts of the box tree that are painted atomically, as specified in CSS 2.2 Appendix E. There is always a <code>Display::StackingItem</code> at the root (via which the <code>Display::Tree</code> owns the actual box tree), and CSS positioning and other styles which cause CSS stacking context trigger the creation of a <code>Display::StackingItem</code>. <code>Display::StackingItem</code> objects that represent CSS stacking contexts have positive and negative z-order lists which contain owning references to the descendant <code>Display::StackingItem</code> objects. These lists are built and sorted at tree-building time.</p>"},{"location":"Deep%20Dive/Layout%20%26%20Rendering/DisplayTree.html#displaystyle","title":"Display::Style","text":"<p><code>Display::Style</code> is the display tree's equivalent of <code>RenderStyle</code> but with some important differences. Every <code>Display::Box</code> has a <code>Display::Style</code>, so it should be relatively small. It must only contain data that is independent of box geometry; i.e. it must not contain any <code>Length</code> values (which can hide things like <code>calc()</code> values that must have been already resolved). It should be shareable between boxes with the same appearance, but possibly different sizes. All colors should be their final used values, i.e. after resolving <code>visitedDependentColor()</code> and applying <code>-apple-color-filter</code>.</p>"},{"location":"Deep%20Dive/Layout%20%26%20Rendering/DisplayTree.html#displayboxraregeometry-displayboxdecorationdata","title":"Display::BoxRareGeometry / Display::BoxDecorationData","text":"<p>These classes store geometry-dependent data specific to a single <code>Display::Box</code>, and are only allocated for boxes that have the relevant styles (like visible borders, or border-radius).</p> <p><code>Display::BoxDecorationData</code> stores information about backgrounds and borders.</p> <p><code>Display::BoxRareGeometry</code> stores border-radius (here because a box with overflow:hidden clips to its border-radius even without any border), and transforms.</p>"},{"location":"Deep%20Dive/Layout%20%26%20Rendering/DisplayTree.html#displaycsspainter","title":"Display::CSSPainter","text":"<p><code>Display::CSSPainter</code> knows how to walk the trees of <code>Display::StackingItem</code> and their <code>Display::Box</code> objects in order to implement CSS painting. It knows how to apply clipping, opacity and filters.</p> <p>For painting individual boxes, <code>Display::CSSPainter</code> makes use of <code>Display::BoxDecorationPainter</code> which knows how to paint CSS borders and backgrounds.</p> <p><code>Display::CSSPainter</code> objects exist on the stack only during painting; they are not long-lived.</p> <p>It's intended that SVG would be painted by a <code>Display::SVGPainter</code> class at some point.</p> <p>Hit-testing is very similar to painting, so that code lives in <code>Display::CSSPainter</code> too. Maybe it needs a different name.</p>"},{"location":"Deep%20Dive/Layout%20%26%20Rendering/DisplayTree.html#painting","title":"Painting","text":"<p>The entry point to painting some subset of the display tree is a <code>GraphicsLayerClient</code>'s <code>paintContents()</code> paint callback. Eventually the display tree will be presented via a hierarchy of GraphicsLayers; for now, there is a single GraphicsLayer at the root.</p> <p>To paint the tree, the static <code>CSSPainter::paintTree()</code> function is called, and <code>Display::CSSPainter</code> traverses the <code>Display::StackingItem</code> hierarchy and paints their <code>Display::Box</code> objects, making use of <code>Display::BoxDecorationPainter</code> on boxes with box decorations.</p> <p>At CSS/SVG boundaries a new painter object will be created on the stack to paint the inner content.</p> <p>The destination <code>GraphicsContext</code> is passed around as an argument to painting functions, wrapped up in a <code>PaintingContext</code> struct that also has the <code>deviceScaleFactor</code> and will probably include a dirty rect. </p>"},{"location":"Deep%20Dive/Layout%20%26%20Rendering/DisplayTree.html#geometry","title":"Geometry","text":"<pre><code>Note: this is subject to change. The code needs to make it hard to use the absolute rects in the wrong way.\n</code></pre> <p>Display boxes store their position and size as an absolute (i.e. document-relative) <code>FloatRect</code> (already pixel-snapped). CSS box-model boxes, which have things like borders and border-radius, also store rects for their padding and content boxes. Thus no paint offsets have to be tracked during painting.</p> <p>Some boxes act as coordinate boundaries; namely those with transforms, and those which are affected by scrolling. The \"absolute\" rects are computed before accounting for scrolling and transforms, so code that needs to map a box's rect into view coordinate will need to take those into account.</p>"},{"location":"Deep%20Dive/Layout%20%26%20Rendering/DisplayTree.html#clipping","title":"Clipping","text":"<p>Clipping is a source of complexity in the display tree because CSS clipping does not trigger CSS stacking context; the clipping tree follows the containing block tree, which is different from paint order. For painting normal in-flow content we can just apply clipping at paint time, but when painting out-of-flow content, we have to be able to compute the clip from ancestors. Such out-of-flow content is always represented by the root box of a <code>Display::StackingItem</code>.</p> <p>At tree-building time <code>Display::TreeBuilder</code> keeps track of containing blocks, so for boxes which initiate out-of-order painting (i.e. those which are the root box of a <code>Display::StackingItem</code>) <code>Display::BoxFactory</code> can ask the containing block for the clip that should be applied.</p> <p>Clips are store in <code>Display::BoxClip</code> which tracks an intersection rect, and, if necessary, a set of rounded rects needed to apply border-radius clips from ancestors. <code>Display::BoxClip</code> objects are shared between parent/child <code>Display::BoxModelObject</code> objects that share the same clip (i.e. have no overflow of their own).</p>"},{"location":"Deep%20Dive/Layout%20%26%20Rendering/DisplayTree.html#scrolling","title":"Scrolling","text":"<pre><code>Note: this is preliminary\n</code></pre> <p>A <code>Layout::Box</code> with <code>overflow:auto</code> or <code>overflow:scroll</code> is represented with a hierarchy of three display boxes:</p> <pre><code>container box\n    scrolling container box\n        scrolled contents box\n\n</code></pre> <p>(and possibly additional boxes for scrollbars and the scroll corner).</p> <p>The container box exists to paint box decorations on the scrolling element. The \"scrolling container\" box has geometry of the container's content box, and exists to represent the scroll offset. The \"scrolled contents\" box paints the scrolled content, and has the size of the layout overflow.</p> <p>It is a goal of the display tree that scrolling should not invalidate any geometry on the subtree (thus avoiding post-scroll tree walks to fix up geometry, which are known to be a source of performance issues in rendering code).</p> <pre><code>Note: explain how scrolling interacts with positioned elements.\n</code></pre>"},{"location":"Deep%20Dive/Layout%20%26%20Rendering/DisplayTree.html#compositing","title":"Compositing","text":"<pre><code>Note: write me\n</code></pre>"},{"location":"Deep%20Dive/Libpas/Internals.html","title":"Internals","text":"<p>Deep dive into the internals of libpas.</p>"},{"location":"Deep%20Dive/Libpas/Internals.html#introduction","title":"Introduction","text":"<p>This document describes how libpas works as of 247029@main, so a bit ahead of where WebKit was as of 246842@main. Libpas is a fast and memory-efficient memory allocation toolkit capable of supporting many heaps at once, engineered with the hopes that someday it'll be used for comprehensive isoheaping of all malloc/new callsites in C/C++ programs.</p> <p>Since WebKit 186504@main, we've been steadily enabling libpas as a replacement for WebKit's bmalloc and MetaAllocator. This has so far added up to a ~2% Speedometer2 speed-up and a ~8% memory improvement (on multiple memory benchmarks). Half of the speed-up comes from replacing the MetaAllocator, which was JavaScriptCore's old way of managing executable memory. Now, JSC uses libpas's jit_heap to manage executable memory. The other half of the speed-up comes from replacing everything that bmalloc provided -- the fastMalloc API, the Gigacage API, and the IsoHeap&lt;&gt; API. All of the memory improvement comes from replacing bmalloc (the MetaAllocator was already fairly memory-efficient).</p> <p>This document is structured as follows. First I describe the goals of libpas; these are the things that a malloc-like API created out of libpas should be able to expose as fast and memory-efficient functions. Then I describe the coding style. Next I tell all about the design. Finally I talk about how libpas is tested.</p>"},{"location":"Deep%20Dive/Libpas/Internals.html#goals-of-libpas","title":"Goals of Libpas","text":"<p>Libpas tries to be:</p> <ul> <li>Fast. The goal is to beat bmalloc performance on single-threaded code. Bmalloc was previously the fastest   known malloc for WebKit.</li> <li>Scalable. Libpas is meant to scale well on multi-core devices.</li> <li>Memory-efficient. The goal is to beat bmalloc memory usage across the board. Part of the strategy for memory   efficiency is consistent use of first-fit allocation.</li> <li>External metadata. Libpas never puts information about a free object inside that object. The metadata is   always elsewhere. So, there's no way for a use-after-free to corrupt libpas's understanding of memory.</li> <li>Multiple heap configurations. Not all programs want the same time-memory trade-off. Some malloc users have   very bizarre requirements, like what JavaScriptCore does with its ExecutableAllocator. The goal is to support   all kinds of special allocator needs simultaneously in one library.</li> <li>Boatloads of heaps. Libpas was written with the dream of obviating the need for ownership type systems or   other compiler approaches to fixing the type-safety of use-after-frees. This means that we need one heap per   type, and be 100% strict about it. So, libpas supports tons of heaps.</li> <li>Type-awareness. Sometimes, malloc decisions require knowing what the type's size and alignment are, like when   deciding how to split and coalesce memory. Libpas is designed to avoid type errors arising from the malloc's   \"skewed\" reuse of memory.</li> <li>Common free. Users of libpas isoheaps don't have to know the heap of an object when they free it. All objects   should funnel into the same free function. One kind of exception to this requirement is stuff like   ExecutableAllocator, which needs a malloc, but is fine with not calling a common free function.</li> <li>Cages. WebKit uses virtual memory reservations called cages, in which case WebKit allocates the virtual   memory and the malloc has to associate that memory with some heap. Libpas supports multiple kinds of cages.</li> </ul>"},{"location":"Deep%20Dive/Libpas/Internals.html#libpas-style","title":"Libpas Style","text":"<p>Libpas is written in C. Ultimately, I chose C because I felt that the language provides better support for extremely low-level code:</p> <ul> <li>C++ is usually my favorite, because it makes code easier to write, but for libpas, I wanted something easier   to read. It's easier to read C when auditing for subtle bugs, because there's nothing hidden. C doesn't have   stuff like destructor invocations or operator overloading, which result in surprising effectfulness in   otherwise innocent-looking code. Memory management code like libpas has to be read a lot, so C is better.</li> <li>C makes casting between pointers and integers very simple with its style of cast operator. It feels weird to   use the C cast operator in C++, so when I have to do a lot of uintptr_t'ing, I prefer C.</li> </ul> <p>C lets you do most of what C++ can if you rely on <code>always_inline</code>. This didn't used to be the case, but modern C compilers will meat-grind the code with repeated application of the following things:</p> <ul> <li>Inlining any <code>always_inline</code> call except if it's recursive or the function uses some very weird features that   libpas doesn't use (like goto pointer).</li> <li>Copy-propagating the values from the callsite into the function that uses the value.</li> </ul> <p>Consequently, passing a function pointer (or struct of function pointers), where the pointer points to an <code>always_inline</code> function and the callee is <code>always_inline</code> results in specialization akin to template monomorphization. This works to any depth; the compiler won't be satisfied until there are no more <code>always_inline</code> function calls. This fortuitous development in compilers allowed me to write very nice template code in C. Libpas achieves templates in C using config structs that contain function pointers -- sometimes to <code>always_inline</code> functions (when we want specialization and inlining) and sometimes to out-of-line functions (when we want specialization but not inlining). Additionally, the C template style allows us to have true polymorphic functions. Lots of libpas slow paths are huge and not at all hot. We don't want that code specialized for every config. Luckily, this works just fine in C templates -- those polymorphic functions just pass around a pointer to the config they are using, and dynamically load and call things in that config, almost exactly the same way that the specialized code would do. This saves a lot of code size versus C++ templates.</p> <p>Most of libpas is written in an object-oriented style. Structs are used to create either by-value objects or heap-allocated objects. It's useful to think of these as classes, but a loose way since there are many ways to do classes in C, and libpas uses whatever techniques are best on a per-class basis. But heap allocated objects have a clear convention: for a class named <code>foo</code>, we would call the struct <code>pas_foo</code>, and for a method <code>bar</code> on <code>foo</code>, we would call the function <code>pas_foo_bar</code> and have the first parameter be <code>pas_foo*</code>. The function that creates instances of <code>foo</code> is called <code>pas_foo_create</code> (or <code>pas_foo_create_blah_blah</code> in case of overloading) and returns a <code>pas_foo*</code>. The function that destroys <code>foo</code> objects is called <code>pas_foo_destroy</code> and takes a <code>pas_foo*</code>. </p> <p>Libpas classes are usually implemented in files called <code>pas_foo.h</code> (the header that defines the struct and a subset of functions), <code>pas_foo_inlines.h</code> (the header that defines inline functions of <code>foo</code> that require calling functions declared in headers that <code>pas_foo.h</code> can't include), and <code>pas_foo.c</code> (the implementations of <code>foo</code> functions that can be out-of-line).</p> <p>Some libpas \"classes\" are singletons. The standard way of implementing a singleton in libpas is that there is really no struct, only global variables and functions that are declared in the header. See <code>pas_page_malloc</code> or <code>pas_debug_heap</code> for examples of singletons.</p> <p>Not everything in libpas is a class. In cases where a bunch of not-class-like things can be grouped together in a way that makes sense, we usually do something like a singleton. In cases where a function can't easily be grouped together with some class, even a singleton, we name the file it's in after the function. There are lots of examples of this, like <code>pas_deallocate</code> or <code>pas_get_page_base</code>. Sometimes this gets fun, like <code>pas_get_page_base_and_kind_for_small_other_in_fast_megapage.h</code>.</p> <p>Finally, libpas avoids abbreviations even more so than WebKit usually does. Functions that have a quirky meaning typically have a long name that tells the story. The point is to make it easy to appreciate the subtlety of the algorithm when reading the code. This is the kind of code where complex situations should look complex at any abstraction level.</p>"},{"location":"Deep%20Dive/Libpas/Internals.html#design-of-libpas","title":"Design of Libpas","text":"<p>Libpas is organized into roughly eleven areas:</p> <ol> <li>Heap configurations. This is the way that we tell libpas how to organize a heap. Heap configurations can     control a lot. They can change obvious things like the minalign and page size, but also more crazy things,     like how to find a page header given a page and vice-versa.</li> <li>The large heaps. This is a first-fit heap based on arrays, cartesian trees, and hashtables. The large     heap has excellent type safety support and can be safely (though not efficiently) used for small objects.</li> <li>Metacircularity. Libpas uses malloc-like APIs internally for managing its state. These are provided by the     so-called bootstrap, utility, and immortal heaps.</li> <li>The segregated heaps and TLCs (thread-local caches). Libpas has a super fast simple segregated storage slab     allocator. It supports type safety and is the most commonly used kind of heap.</li> <li>The bitfit heaps. This is a fast and memory-efficient type-unsafe heap based on slabs and bitmaps.</li> <li>The scavenger. Libpas performs a bunch of periodic bookkeeping tasks in a scavenger thread. This includes,     but is not limited to, returning memory to the OS.</li> <li>Megapages and page header tables. Libpas has multiple clever tricks for rapidly identifying which kind of     heap an object belongs to. This includes an arithmetic hack called megapages and some lock-free hashtables.</li> <li>The enumerator. Libpas supports malloc heap enumeration APIs.</li> <li>The basic configuration template, used to create the <code>bmalloc_heap</code> API that is used as a replacement for     all of bmalloc's functionality.</li> <li>The JIT heap config.</li> <li>The fast paths. The various heaps, TLCs, megapages and page header tables are glued together by fast paths     provided for allocation, deallocation, and various utility functions.</li> </ol>"},{"location":"Deep%20Dive/Libpas/Internals.html#heap-configurations","title":"Heap Configurations","text":"<p>The <code>pas_heap_config</code> struct defines all of the configurable behaviors of a libpas heap. This includes things like how the heap gets its memory, what size classes use segregated, bitfit, or large allocators, and a bunch of other things.</p> <p>Heap configs are passed by-value to functions that are meant to be specialized and inlined. To support this, the convention for defining a heap config is that you create a macro (like <code>BMALLOC_HEAP_CONFIG</code>) that gives a heap config literal expression. So, a call like <code>pas_get_allocation_size(ptr, BMALLOC_HEAP_CONFIG)</code> will give you an optimized fast path for getting the allocation size of objects in bmalloc. This works because such fast paths are <code>always_inline</code>.</p> <p>Heap configs are passed by-pointer to functions that are not meant to be specialized. To support this, all heap configs also have a global variable like <code>bmalloc_heap_config</code>, so we can do things like <code>pas_large_heap_try_deallocate(base, &amp;bmalloc_heap_config)</code>.</p> <p>Heap configs can have up to two segregated page configs (<code>config.small_segregated_config</code> and <code>config.medium_segregated_config</code>) and up to three bitfit page configs (<code>config.small_bitfit_config</code>, <code>config.medium_bitfit_config</code>, and <code>config.marge_bitfit_config</code>). Any of the page configs can be disabled, though weird things might happen if the smallest ones are disabled (rather than disabling the bigger ones). Page configs (<code>pas_segregated_page_config</code>, <code>pas_bitfit_page_config</code>, and the common supertype, <code>pas_page_base_config</code>) get used in much the same way as heap configs -- either by-value for specialized and inlined functions or by-pointer for unspecialized functions.</p> <p>Heap and page configs also support specialized-but-not-inlined functions. These are supported using additional function pointers in those configs that are filled in using macros -- so you don't need to fill them in explicitly when creating your own config, like <code>BMALLOC_HEAP_CONFIG</code> or <code>JIT_HEAP_CONFIG</code>. The macros fill them in to point at never_inline functions that call some specialized and inlined function with the config passed as a constant. This means for example that:</p> <pre><code>BMALLOC_HEAP_CONFIG.specialized_local_allocator_try_allocate_small_segregated_slow(...);\n</code></pre> <p>Is an out-of-line direct function call to the specialization of <code>pas_local_allocator_try_allocate_small_segregated_slow</code>. And this would be a virtual call to the same function:</p> <pre><code>pas_heap_config* config = ...;\nconfig-&gt;specialized_local_allocator_try_allocate_small_segregated_slow(...);\n</code></pre> <p>Note that in many cases where you have a <code>pas_heap_config</code>, you are in specialized code and the heap config is a known constant at compile to, so then:</p> <pre><code>config.specialized_local_allocator_try_allocate_small_segregated_slow(...);\n</code></pre> <p>Is an out-of-line direct function call.</p>"},{"location":"Deep%20Dive/Libpas/Internals.html#the-large-heaps","title":"The Large Heaps","text":"<p>Libpas's large heaps serve multiple purposes:</p> <ul> <li>Everything is bootstrapped on large heaps. When segregated and bitfit heaps allocate memory, they do so from   some large heap.</li> <li>Segregated and bitfit heaps have object size ceilings in the tens or hundreds of kilobytes. So, objects that   are too large for the other heaps get allocated from large heaps.</li> </ul> <p>Large heaps are broken into two parts:</p> <ol> <li>The large free heap. In libpas jargon, a free heap is a heap that requires that deallocation passes the    object size, requires that the freed object size matches the allocated object size for that object, and makes    no guarantees about what kind of mess you'll get yourself into if you fail to obey that rule.</li> <li>The large map. This maps object pointer to size and heap.</li> <li>The large heap. This is an abstraction over both (1) and (2).</li> </ol> <p>Large free heaps just maintain a free-list; they know nothing about allocated objects. But combined with the large map, the large heaps provide a user-friendly deallocation API: you just need the object pointer, and the large map figures out the rest, including identifying which free heap the object should be deallocated into, and the size to pass to that free heap.</p> <p>Large heaps operate under a single global lock, called the heap lock. Most libpas heaps use fine-grained locking or avoid locking entirely. But for the large heap, libpas currently just uses one lock.</p>"},{"location":"Deep%20Dive/Libpas/Internals.html#large-free-heap","title":"Large Free Heap","text":"<p>Large free heaps are built out of a generic algorithm that doesn't know how to represent the free-list and gets instantiated with either of two free-list representations, simple and fast. The simple large free heap uses an unordered array of coalesced free object descriptions. The fast large free heap uses a cartesian tree of coalesced free object descriptions.</p> <p>A free object description is represented by the <code>pas_large_free</code> object in libpas; let's just call it a large free for brevity. Large frees can tell you the beginning and end of a free chunk of memory. They can also tell if the memory is already known to be zero and what the type skew of the free memory is. The large heap can be used to manage arrays of some type that is either larger than the heap's minimum alignment, or that is smaller than and not a divisor of the alignment. Especially when this is combined with <code>memalign</code>, the free heap will have to track free memory that isn't type-aligned. Just consider a type of size 1000 that is allocated with alignment 16384. The rules of memalign say that the size must be 16384 in that case. Assuming that the free heap had 32768 contiguous bytes of free memory to begin with, it will now have 16384 bytes that starts with a type skew of 384 bytes. The type skew, or <code>offset_in_type</code> as libpas calls it, is the offset of the beginning of the large free inside the heap's type. In extremely complex cases, this means that finding where the first valid object address is inside a large free for some type and alignment requires computing the offset least common multiple (see <code>pas_coalign</code>), which relies on the right bezout coefficient of the extended GCD of the type size and alignment (see <code>pas_extended_gcd</code>).</p> <p>Large frees support an API for coalescing (merging as libpas calls it) and splitting. The generic large free heap handles searching through large frees to find the first one that matches an allocation request for some size and alignment. It also handles coalescing freed memory back into the heap, by searching for adjacent free memory. The searches are through a struct of function pointers that may be implemented either efficiently (like the simple large free heap's O(n) search through an unordered array) or efficiently (like the O(1)-ish or O(log n)-ish operations on the cartesian tree in the fast large free heap). The generic algorithm uses the C generic idiom so there are no actual function pointer calls at runtime.</p> <p>Large free heaps allow you to give them callbacks for allocating and deallocating memory. The allocation callback will get called if you ask a large free heap for memory and it doesn't have it. That allocation callback could get the memory from the OS, or it could get it from some other heap. The deallocation callback is for those cases where the large free heap called the allocation callback and then decided it wanted to give some fraction of that memory back. Both callbacks are optional (can be NULL), though the case of a NULL allocation callback and non-NULL deallocation callback is not useful since the deallocation callback only gets called on the path where we had an allocation callback.</p> <p>Note that large free heaps do not do anything to decommit their free memory. All decommit of memory in large free heaps is accomplished by the large sharing pool, which is part of the scavenger.</p>"},{"location":"Deep%20Dive/Libpas/Internals.html#large-map","title":"Large Map","text":"<p>The large map is a hashtable that maps object addresses to large entries, which contain the size of the object and the heap it belongs to. The large map has a fairly complex hashtable algorithm because of my past attempts at making the large heap at least somewhat efficient even for small objects. But it's conceptually a simple part of the overall algorithm. It's also legal to ask the large map about objects it doesn't know about, in which case, like a normal hashtable, it will just tell you that it doesn't know about your object. Combined with the way that segregated and bitfit heaps use megapage tables and page header tables, this means that libpas can do fall-through to another malloc for objects that libpas doesn't manage.</p> <p>Note that it might be OK to remove the small object optimizations in the large map. On the other hand, they are reliable, and they aren't known to increase the cost of the algorithm. Having that capability means that as part of tuning the algorithm, it's more safe than it would otherwise be to try putting some small objects into the large heap to avoid allocating the data structures required for operating a segregated or bitfit heap.</p>"},{"location":"Deep%20Dive/Libpas/Internals.html#the-large-heap","title":"The Large Heap","text":"<p>The large free heap and large map are combined into a high-level API with the <code>pas_large_heap</code>. In terms of state, this is just a <code>pas_large_free_heap</code> plus some data to help with small-object optimizations in the large map. The functions of the large heap do a lot of additional work:</p> <ul> <li>They give the free heap an allocator for getting new memory. The large heap routes memory allocation   requests to the heap config's allocation callback.</li> <li>They ensure that each free heap allocation ends up in the large map.</li> <li>They implement deallocation by removing something from the large map and then deallocating it into the free   heap.</li> <li>They provide integration with the scavenger's large sharing pool so that free memory can be decommitted.</li> </ul> <p>The large heap is always used as a member of the <code>pas_heap</code> object. It's useful to think of <code>pas_large_heap</code> as never being a distinct object; it's more of a way of compartmentalizing <code>pas_heap</code>. The heap object also contains a segregated heap and some other stuff.</p>"},{"location":"Deep%20Dive/Libpas/Internals.html#metacircularity","title":"Metacircularity","text":"<p>I'm used to programming with dynamically allocated objects. This lets me build arrays, trees, hashtables, look-up tables, and all kinds of lock-free data structures. So, I wanted libpas's internals to be able to allocate objects just like any other kind of algorithm would do. But libpas is engineered so that it can be a \"bottom of the world\" malloc -- where it is the implementation of <code>malloc</code> and <code>free</code> and cannot rely on any memory allocation primitives other than what the kernel provides. So, libpas uses its own allocation primitives for its own objects that it uses to implement those primitives. This is bootstrapped as follows:</p> <ul> <li>The bootstrap heap is a simple large free heap. A simple large free heap needs to be able to allocate   exactly one variable-length array of large frees. The bootstrap heap has hacks to allow itself to allocate   that array out of itself. This trick then gives us a complete malloc implementation for internal use by   libpas, albeit one that is quite slow, can only be used under the heap lock, and requires us to know the   object's size when freeing it. All other simple large free heaps allocate their free lists from the bootstrap   heap. The bootstrap heap is the only heap in libpas that asks the OS for memory. All other heaps ask either   the bootstrap heap for memory, or they ask one of the other heaps.</li> <li>The compact reservation is 128MB of memory that libpas uses for objects that can be pointed at with 24-bit   (3 byte) pointers assuming 8-byte alignment. Libpas needs to manage a lot of heaps, and that requires a lot   of internal meta-data, and having compact pointers reduces the cost of doing this. The compact reservation is   allocated from the bootstrap heap.</li> <li>The immortal heap is a heap that bump-allocates out of the compact reservation. It's intended for small   objects that are immortal.</li> <li>The compact bootstrap heap is like the bootstrap heap, except that it allocates its memory from the compact   reservation, and allocates its free list from the bootstrap heap rather than itself.</li> <li>The compact large utility free heap is a fast large free heap that supports decommitting free memory (see   the scavenger section) and allocates its memory from the compact bootstrap heap.</li> <li>The utility heap is a segregated heap configured to be as simple as possible (no thread-local caches for   example) and can only be used while holding the heap lock. It only supports objects up to some size   (<code>PAS_UTILITY_LOOKUP_SIZE_UPPER_BOUND</code>), supports decommitting free memory, and gets its memory from the   compact bootstrap heap. One example of how the utility heap gets used is the nodes in the cartesian trees   used to implement fast large free heaps. So, for example, the compact large utility free heap relies on the   utility heap.</li> <li>The large utility free heap is a fast large free heap that supports decommitting free memory and allocates   its memory from the bootstrap heap.</li> </ul> <p>Note how the heaps pull memory from one another. Generally, a heap will not return memory to the heap it got memory from except to \"undo\" part of an allocation it had just done. So, this arrangement of who-pulls-memory-from-who is designed for type safety, memory efficiency, and elegantly supporting weird alignments:</p> <ul> <li>Libpas uses decommit rather than unmapping free memory because this ensures that we don't ever change the type   of memory after that memory gets its type for the first time.</li> <li>The lower-level heaps (like bootstrap and compact bootstrap) do not support decommit. So, if a higher-level   heap that does support decommit ever returned memory to the lower-level heap, then the memory would never get   decommitted.</li> <li>Page allocation APIs don't let us easily allocate with alignment greater than page size. Libpas does this by   over-allocating (allocating size + alignment and then searching for the first aligned start within that larger   reservation). This is all hidden inside the bootstrap heap; all other heaps that want memory on some weird   alignment just ask some other heap for memory (often the bootstrap heap) and that heap, or ultimately the   bootstrap heap, figure out what that means in terms of system calls.</li> </ul> <p>One missing piece to the metacircularity is having a fast utility heap that uses thread-local caches. There is currently maybe one utility heap callsite that only grabs the heap lock just because it wants to allocate in the utility heap. There's a possibility of a small speed-up if any callsite like that used a fast utility heap instead, and then no locking would be required. It's not clear how easy that would be; it's possible that some bad hacks would be required to allow code that uses TLCs to call into a heap that then also uses TLCs.</p>"},{"location":"Deep%20Dive/Libpas/Internals.html#segregated-heaps-and-tlcs-thread-local-caches","title":"Segregated Heaps and TLCs (Thread Local Caches)","text":"<p>Libpas's great performance is mostly due to the segregated heaps and how they leverage thread-local caches. TLCs provide a cache of global memory which. In the best case, this cache prevents threads from doing any synchronization during allocation and deallocation. Even when they do have to do some synchronization, TLCs make it unlikely that one thread will ever want to acquire a lock held by another thread. The strategy is three-fold:</p> <ul> <li>The TLC has a per-size-class allocator that caches some amount of that size class's memory. This means that   the allocation fast path doesn't have to do any locking or atomic instructions except when its cache runs out.   Then, it will have to do some synchronization -- in libpas's case, fine-grained locking and some lock-free   algorithms -- to get more memory. The amount of memory each allocator can cache is bounded (usually 16KB) and   allocators can only hold onto memory for about a second without using it before it gets returned (see the   scavenger section).</li> <li>The TLC has a deallocation log. The fast path of deallocating a segregated heap object is just pushing it onto   the deallocation log without any locking. The slow path is to walk the log and free all of the objects. The   libpas deallocation log flush algorithm cleverly avoids doing per-object locking; in the best case it will   acquire a couple of locks before flushing and release them after flushing the whole log.</li> <li>When the deallocation log flush frees memory, it tries to first make that memory available exclusively to the   thread that freed it by putting the free memory into the local view cache for that size class in that   thread. Memory moves from the local view caches into the global heap only if the view cache is full (has about   1.6MB of memory in it) or if it hasn't been used in about a second (see the scavenger section).</li> </ul> <p>This section lays out the details of how this works. Segregated heaps are organized into segregated directories. Each segregated directory is an array of page views. Each page view may or may not have a page associated with it. A view can be exclusive (the view has the page to itself), partial (it's a view into a page shared by others), or shared (it represents a page shared by many partial views). Pages have a page boundary (the address of the beginning of the page) and a page header (the object describing the page, which may or may not actually be inside the page). Pages maintain alloc bits to tell which objects are live. Allocation uses the heap's lookup tables to find the right allocator index in the TLC, which yields a local allocator; that allocator usually has a cache of memory to allocate from. When it doesn't, it first tries to pop a view from the local view cache, and if that fails, it uses the find first eligible algorithm on the corresponding directory to find an eligible view. One the allocator has a view, it ensures that the view has a page, and then scans the alloc bits to create a cache of free memory in that page. Deallocation fast paths just push the object onto the deallocation log. When the log is full, the TLC flushes its log while trying to amortize lock acquisition. Freeing an object in a page means clearing the corresponding alloc bit. Once enough alloc bits are clear, either the page's view ends up on the view cache, or the directory is notified to mark the page either eligible or empty. The sections that follow go into each of these concepts in detail.</p>"},{"location":"Deep%20Dive/Libpas/Internals.html#the-thread-local-cache","title":"The Thread Local Cache","text":"<p>Each thread has zero or one <code>pas_thread_local_cache</code>s. Libpas provides slow paths for allocating and deallocating without a TLC in cases where TLC creation is forbidden (like when a thread is shutting down). But usually, allocation and deallocation create a TLC if the thread doesn't already have one. TLCs are structured as follows:</p> <ul> <li>TLCs contain a fixed-size deallocation log along with an index that tells how much of the log is full.   Deallocation pushes onto that log.</li> <li>TLCs contain a variable-length array of allocators, which are really either local allocators or local   view caches. Allocators are variable length. Clients access allocators using an allocator index, which they   usually get from the directory that the allocator corresponds to.</li> <li>TLCs can get reallocated and deallocated, but they always point to a <code>pas_thread_local_cache_node</code>, which is   an immortal and compact descriptor of a TLC. TLC nodes are part of a global linked list. Each TLC node may or   may not have a live TLC associated with it. TLCs cannot be created or destroyed unless the heap lock is held,   so if you hold the heap lock, you can iterate the TLC node linked list to find all TLCs.</li> <li>The layout of allocator indices in a TLC is controlled by both the directories and the TLC layout data   structure (<code>pas_thread_local_cache_layout</code>). This is a global data structure that can tell us about all of the   allocators in a TLC. When holding the heap lock, it's possible to loop over the TLC layout linked list to   find what all of the valid allocator indices are and to introspect what is at those indices.</li> </ul> <p>Thread local caches tend to get large because both the local allocators and local view caches have inline arrays. The local allocator has an array of bits that tell the allocator where the free objects are. The local view cache has an array of view pointers (up to 1.6MB / 16KB = 100 entries, each using a 24-bit pointer). When used in single-heap applications, these overheads don't matter -- they end up being accounting for less than 10<sup>-5</sup> of the overall process footprint (not just in WebKit but when I experimented with libpas in daemons). But when used for many heaps, these overheads are substantial. Given thousands or tens of thousands of heaps, TLCs account for as much as 1% of memory. So, TLCs support partial decommit. Those pages that only have allocators that are inactive get decommitted. Note that TLC decommit has landed in the libpas.git repo as of 247029@main, but hasn't yet been merged into WebKit.</p> <p>The TLC deallocation log flush algorithm is designed to achieve two performance optimizations:</p> <ul> <li>It achieves temporal locality of accesses to page headers. If freeing each object meant flipping a bit in the   page header, then many of those operations would miss cache, since the page header is not accessed by normal   program operation -- it's only accessed during some allocation slow paths and when deallocating. But because   deallocation accesses page headers only during log flush and log flush touches about 1000 objects, it's likely   that the flush will touch the same page header cache lines multiple times.</li> <li>It reduces the average number of lock acquisitions needed to free an object. Each page uses its own lock to   protect its page header, and the page header's <code>alloc_bits</code>. But deallocation log flush will do no locking or   unlocking if the object at index i and the object at index i+1 use the same lock. Pages can dynamically   select which lock they use (thanks to <code>page-&gt;lock_ptr</code>), and they select it so that pages allocated out of by   a thread tend to share the same lock, so deallocation log flush usually only just acquires a lock at the start   of the 1000 objects and releases it when it finishes the 1000 objects.</li> </ul>"},{"location":"Deep%20Dive/Libpas/Internals.html#the-segregated-heap","title":"The Segregated Heap","text":"<p>The <code>pas_segregated_heap</code> object is the part of <code>pas_heap</code> that facilitates segregated heap and bitfit heap allocation. The details of the bitfit heap are discussed in a later section. Segregated heaps can be created separately from a <code>pas_heap</code>, but segregated heaps are almost always part of a <code>pas_heap</code> (and it would be easy to refactor libpas to make it so that segregated heaps are always part of heaps).</p> <p>Segregated heaps use size directories to track actual memory. Most of the exciting action of allocation and deallocation happens in directories. Each directory corresponds to some size class. Segregated heaps make it easy to find the directory for a particular size. They also make it easy to iterate over all the directories. Also, segregated heaps make it easy to find the allocator index for a particular size (using a lookup table that is essentially a cache of what you would get if you asked for the directory using the size-to-directory lookup tables and then asked the directory for the allocator index). The most exciting part of the segregated heap algorithm is <code>pas_segregated_heap_ensure_size_directory_for_size</code>, which decides what to do about allocating a size it hadn't encountered before. This algorithm will either return an existing directory, create a new one, or even retire an existing one. It handles all of the issues related to type size, type alignment, and the alignment argument to the current malloc call.</p> <p>The lookup tables maintained by segregated heaps have some interesting properties:</p> <ul> <li>They can be decommitted and rematerialized. This is a useful space saving when having lots of isoheaps. The   rematerialization happens because a heap also maintains a linked list of directories, and that linked list   never goes away. Each directory in the linked list knows what its representation would have been in the lookup   tables.</li> <li>They are optional. Some heaps can be configured to have a preferred size, called the basic size class. This   is very common for isoheaps, which may only ever allocate a single size. For isoheaps based on type, the   basic size class is just that type's size. Other isoheaps dynamically infer a preferred size based on the   first allocation. When a heap only has the basic size class, it will have no lookup tables.</li> <li>There are separate lookup tables for smaller sizes (not related to the small_segregated_config -- the   threshold is set separately) are just arrays whose index is the size divided by the heap's minalign, rounded   up. These may be populated under heap lock while they are accessed without any locks. So, accesses to them   have some guards against races.</li> <li>There are separate lookup tables for medium sizes (anything above the threshold for small lookup tables).   The medium table is a sorted array that the allocator binary-searches. It may be mutated, decommitted,   rematerialized, or reallocated under heap lock. The algorithm defends itself against this with a bunch of   compiler fences, a mutation count check. Mutating the table means incrementing-to-mutate before making changes   and incrementing-to-finish after making changes. So, the algorithm for lock-free lookup checks mutation count   before and after, and makes sure they are the same and neither indicates that we are mutating. This involves   clever use of dependency threading (like the ARM64 eor-self trick) to make sure that the mutation count reads   really happen before and after the binary search.</li> </ul>"},{"location":"Deep%20Dive/Libpas/Internals.html#segregated-directories","title":"Segregated Directories","text":"<p>Much of the action of managing memory in a segregated heap happens in the segregated directories. There are two kinds:</p> <ul> <li>Segregated size directories, which track the views belonging to some size class in some heap. These may be   exclusive views, which own a page, or partial views, which own part of a shared page. Partial views   range in size between just below 512 bytes to possibly a whole page (in rare cases).</li> <li>Segregated shared page directories, which track shared views. Each shared view tracks shared page and which   partial views belong to it. However, when a shared page is decommitted, to save space, the shared view will   forget which partial views belong to it; they will re-register themselves the first time someone allocates in   them.</li> </ul> <p>Both of them rely on the same basic state, though they use it a bit differently:</p> <ul> <li>A lock-free-access vector of compact view pointers. These are 4-byte pointers. This is possible because views   are always allocated out of the compact reservation (they are usually allocated in the immortal   heap). This vector may be appended to, but existing entries are immutable. So, resizing just avoids deleting   the smaller-sized vectors so that they may still be accessed in case of a race.</li> <li>A lock-free-access segmented vector of bitvectors. There are two bitvectors, and we interleave their 32-bit   words of bits. The eligible bitvector tells us which views may be allocated out of. This means different   things for size directories than shared page directories. For size directories, these are the views that have   some free memory and nobody is currently doing anything with them. For shared page directories, these are the   shared pages that haven't yet been fully claimed by partial views. The empty bitvector tells us which pages   are fully empty and can be decommitted. It's never set for partial views. It means the same thing for both   exclusive views in size directories and shared views in shared page directories.</li> </ul> <p>Both bitvectors are searched in order:</p> <ul> <li>Eligible bitvectors are searched first-fit.</li> <li>Empty bitvectors are searched last-fit.</li> </ul> <p>Searches are made fast because the directory uses the lock-free tricks of <code>pas_versioned_field</code> to maintain two indices:</p> <ul> <li>A first-eligible index. This always points to the first eligible bit, except in cases where some thread has   set the bit but hasn't gotten around to setting the first-eligible index. In other words, this may have some   lag, but the lag is bounded.</li> <li>A last-empty-plus-one index. This always points to the index right after the last empty bit. If it's zero, it   means there are no set empty bits. If it's the number of views, then it means the last view is empty for sure   and there may be any number of other empty views.</li> </ul> <p>These versioned indices can be read without any atomic instructions in many cases, though most mutations to them require a pair of 128-bit compare-and-swaps.</p> <p>The eligible bitvector together with the first-eligible index allow for very fast searches to find the first eligible view. Bitvector searches are fast to begin with, even over a segmented vector, since the segmented vector has large-enough chunks. Even searching the whole bitvector is quite efficient because of the properties of bitvector simd (i.e. using a 32-bit or 64-bit or whatever-bit word to hold that many bits). But the first-eligible index means that most searches never go past where that index points, so we get a mostly-O(1) behavior when we do have to find the first eligible view.</p> <p>The empty bitvector gives a similar property for the scavenger, which searches backwards to find empty views. The efficiency here arises from the fact that empty pages know the timestamp of when they became empty, and the scavenger will terminate its backwards search when it finds a too-recently emptied page.</p> <p>Directories have to make some choices about how to add views. View addition happens under heap lock and must be in this order:</p> <ul> <li>First we make sure that the bitvectors have enough room for a bit at the new index. The algorithm relies on   the size of the view vector telling us how many views there are, so it's fine for the bitvectors are too big   for a moment. The segmented vector algorithm used for the bitvectors requires appending to happen under heap   lock but it can run concurrently to accesses to the vector. It accomplishes this by never deallocating the   too-small vector spines.</li> <li>Then we append the the view to the view vector, possibly reallocating the view vector. Reallocation keeps the   old two-small copy of the vector around, allowing concurrent reads to the vector. The vector append stores the   value, executes an acqrel fence (probably overkill -- probably could just be a store fence), and then   increments the size. This ensures nobody sees the view until we are ready.</li> </ul> <p>Directories just choose what kind of view they will create and then create an empty form of that view. So, right at the point where the vector append happens, the view will report itself as not yet being initialized. However, any thread can initialize an empty view. The normal flow of allocation means asking a view to \"start allocating\". This actually happens in two steps (<code>will_start_allocating</code> and <code>did_start_allocating</code>). The will-start step checks if the view needs to commit its memory, which will cause empty exclusive views to allocate a page. Empty partial views get put into the partial primordial state where they grab their first chunk of memory from some shared view and prepare to possibly grab more chunks of that shared view, depending on demand. But all of this happens after the directory has created the view and appended it. This means that there is even the possibility that one thread creates a view, but then some other thread takes it right after it was appended. In that case, the first thread will loop around and try again, maybe finding some other view that had been made eligible in the meantime, or against appending another new view.</p> <p>Size directories maintain additional state to make page management easy and to accelerate allocation.</p> <p>Size directories that have enabled exclusive views have a <code>full_alloc_bits</code> vector that has bits set for those indices in their pages where an object might start. Pages use bitvectors indexed by minalign, and only set those bits that correspond to valid object offsets. The <code>full_alloc_bits</code> vector is the main way that directories tell where objects could possibly be in the page. The other way they tell is with <code>offset_from_page_boundary_to_first_object</code> and <code>offset_from_page_boundary_to_end_of_last_object</code>, but the algorithm relies on those a bit less.</p> <p>Size directories can tell if they have been assigned an allocator index or a view cache index, and control the policies of when they get them. A directory without an allocator index will allocate out of baseline allocators, which are shared by all threads. Having an allocator index implies that the allocator index has also been stored in the right places in the heap's lookup tables. Having a view cache index means that deallocation will put eligible pages on the view cache before marking them eligible in the directory.</p>"},{"location":"Deep%20Dive/Libpas/Internals.html#page-boundaries-headers-and-alloc-bits","title":"Page Boundaries, Headers and Alloc Bits","text":"<p>\"Pages\" in the segregated heap are a configurable concept. Things like their size and where their header lives can be configured by the <code>pas_segregated_page_config</code> of their directory. The config can tell which parts of the page are usable as object payload, and they can provide callbacks for finding the page header.</p> <p>The page header contains:</p> <ul> <li>The page's kind. The segregated page header is a subtype of the <code>pas_page_base</code>, and we support safely   downcasting from <code>pas_page_base*</code> to <code>pas_segregated_page*</code>. Having the kind helps with this.</li> <li>Whether the page is in use for allocation right now. This field is only for exclusive views. Allocation in   exclusive pages can only happen when some local allocator claims a page. For shared pages, this bit is in   each of the the partial views.</li> <li>Whether we have freed an object in the page while also allocating in it. This field is only for exclusive   views. When we finish allocating, and the bit is set, we do the eligibility stuff we would have done if we had   freed objects without the page being used for allocation. For shared pages, this bit is in each of the partial   views.</li> <li>The sizes of objects that the page manages. Again, this is only used for exclusive views. For shared pages,   each partial view may have a different size directory, and the size directory tells the object size. It's also   possible to get the object size by asking the exclusive view for its size directory, and you will get the same   answer as if you had asked the page in that case.</li> <li>A pointer to the lock that the page uses. Pages are locked using a kind of locking dance: you load the   <code>page-&gt;lock_ptr</code>, lock that lock, and then check if <code>page-&gt;lock_ptr</code> still points at the lock you tried.   Anyone who holds both the current lock and some other lock can change <code>page-&gt;lock_ptr</code> to the other lock.   For shared pages, the <code>lock_ptr</code> always points at the shared view's ownership lock. For exclusive views, the   libpas allocator will change the lock of a page to be the lock associated with their TLC. If contention   on <code>page-&gt;lock_ptr</code> happens, then we change the lock back to the view's ownership lock. This means that in   the common case, flushing the deallocation log will encounter page after page that wants to hold the same   lock -- usually the TLC lock. This allows the deallocation log flush to only do a handful of lock acquisitions   for deallocating thousands of objects.</li> <li>The timestamp of when the page became empty, using a unit of time libpas calls epoch.</li> <li>The view that owns the page. This is either an exclusive view or a shared handle, which is the part of the   shared view that gets deallocated for decommitted pages. Note: an obvious improvement is if shared handles   were actually part of the page header; they aren't only because until recently, the page header size had to be   the same for exclusive and shared pages.</li> <li>View cache index, if the directory enabled view caching. This allows deallocation to quickly find out which   view cache to use.</li> <li>The alloc bits.</li> <li>The number of 32-bit alloc bit words that are not empty.</li> <li>Optionally, the granule use counts. It's possible for the page config to say that the page size is larger than   the system page size, but that the page is divided up into granules which are system page size. In that   case, the page header will have an array of 1-byte use counts per granule, which count the number of objects   in that granule. They also track a special state when the granule is decommitted. The medium_segregated_config   uses this to offer fine-grained decommit of 128KB \"pages\".</li> </ul> <p>Currently we have two ways of placing the page header: either at the beginning of the page, or what we call the page boundary, or in an object allocated in the utility heap. In the latter case, we use the mostly-lock-free page header table to map between the page boundary and page header, or vice-versa. The page config has callbacks that allow either approach. I've also used page config hacking to attempt other kinds of strategies, like saying that every aligned 16MB chunk of pages has an array of page headers at the start of it; but those weren't any better than either of the two current approaches.</p> <p>The most important part of the page header is the alloc bits array and the <code>num_non_empty_words</code> counter. This is where most of the action of allocating and deallocating happens. The magic of the algorithm arises from the simple bitvector operations we can perform on <code>page-&gt;alloc_bits</code>, <code>full_alloc_bits</code> (from the size directory in case of exclusive pages or from the partial view in case of shared pages), and the <code>allocator-&gt;bits</code>. These operations allow us to achieve most of the algorithm:</p> <ul> <li>Deallocation clears a bit in <code>page-&gt;alloc_bits</code> and if this results in the word becoming zero, it decrements   the <code>num_non_empty_words</code>. The bit index is just the object's offset shifted by the page config's   <code>min_aligh_shift</code>, which is a compile-time constant in most of the algorithm. If the algorithm makes any bit   (for partials) or any word (for exclusives) empty, it makes the page eligible (either by putting it on a view   cache or marking the view eligible in its owning directory). If <code>num_non_empty_words</code> hits zero, the   deallocator also makes the view empty.</li> <li>Allocation does a find-first-set-bit on the <code>allocator-&gt;bits</code>, but in a very efficient way, because the   current 64-bit word of bits that the allocator is on is cached in <code>allocator-&gt;current_word</code> -- so allocation   rarely searches an array. So, the allocator just loads the current word, does a <code>ctz</code> or <code>clz</code> kind of   operation (which is super cheap on modern CPUs), left-shifts the result by the page config's minalign, and   adds the <code>allocator-&gt;page_ish</code> (the address in memory corresponding to the first bit in <code>current_word</code>).   That's the allocator fast path.</li> <li>We prepare <code>allocator-&gt;bits</code> by basically saying <code>allocator-&gt;bits = full_alloc_bits &amp; ~page-&gt;alloc_bits</code>. This   is a loop since each of the <code>bits</code> is an array of words of bits and each array is the same size. For a 16384   size page (the default for <code>small_segregated_config</code>) and a minalign shift of 4 (so minalign = 16, the default   for <code>small_segregated_config</code>), this means 1024 bits, or 32 32-bit words, or 128 bytes. The loop over the 32   32-bit words is usually fully unrolled by the compiler. There are no loop-carried dependencies. This loop   shows up in profiles, and though I've tried to make it faster, I've never succeeded.</li> </ul>"},{"location":"Deep%20Dive/Libpas/Internals.html#local-allocators","title":"Local Allocators","text":"<p>Each size directory can choose to use either baseline allocators or TLC local allocators for allocation. Each size directory can choose to have a local view cache or not. Baseline allocators are just local allocators that are global and not part of any TLC and allocation needs to grab a lock to use them. TLC local allocators don't require any locking to get accessed.</p> <p>Local allocators can be in any of these modes:</p> <ul> <li>They are totally uninitialized. All fast paths fail and slow paths will initialize the local allocator by   asking the TLC layout. This state happens if TLC decommit causes a local allocator to become all zero.</li> <li>They are in bump allocation mode. Bump allocation happens either when a local allocator decides to allocate in   a totally empty exclusive page, or for primordial partial allocation. In the former case, it's worth about 1%   performance to sometimes bump-allocate. In the latter case, using bump allocation is just convenient -- the   slow path will decide that the partial view should get a certain range of memory within a shared page and it   knows that this memory has never been used before, so it's natural to just set up a bump range over that   memory.</li> <li>They are in free bits mode. This is slightly more common than the bump mode. In this mode, the   <code>allocator-&gt;bits</code> is computed using <code>full_alloc_bits &amp; ~page-&gt;alloc_bits</code> and contains a bit for the start of   every free object.</li> <li>They are in bitfit mode. In this mode, the allocator just forwards allocations to the <code>pas_bitfit_allocator</code>.</li> </ul> <p>Local allocators can be stopped at any time; this causes them to just return all of their free memory back to the heap.</p> <p>Local allocators in a TLC can be used without any conventional locking. However, there is still synchronization taking place because the scavenger is allowed to stop allocators. To support this, local allocators set an <code>in_use</code> bit (not atomically, but protected by a <code>pas_compiler_fence</code>) before they do any work and clear it when done. The scavenger thread will suspend threads that have TLCs and then while the TLC is suspended, they can stop any allocators that are not <code>in_use</code>.</p>"},{"location":"Deep%20Dive/Libpas/Internals.html#the-bitfit-heaps","title":"The Bitfit Heaps","text":"<p>Libpas is usually used with a combination of segregated and large heaps. However, sometimes we want to have a heap that is more space-efficient than segregated but not quite as slow as large. The bitfit heap follows a similar style to segregated, but:</p> <ul> <li>While bitfit has a bit for each minalign index like segregated, bitfit actually uses all of the bits. To   allocate an object in bitfit, all of the bits corresponding to all of the minalign granules that the object   would use have to be free before the allocation and have to be marked as not free after the allocation.   Freeing has to clear all of the bits.</li> <li>The same page can have objects of any size allocated in it. For example, if a 100 byte object gets freed, then   it's legal to allocate two 50 byte objects out of the freed space (assuming 50 is a multiple of minalign).</li> <li>A bitfit directory does not represent a size class. A bitfit heap has one directory per bitfit page config   and each page config supports a large range of sizes (the largest object is ~250 times larger than the   smallest).</li> </ul> <p>Bitfit pages have <code>free_bits</code> as well as <code>object_end_bits</code>. The <code>free_bits</code> indicates every minalign granule that is free. For non-free granules, the <code>object_end_bits</code> has an entry for every granule that is the last granule in some live object. These bits get used as follows:</p> <ul> <li>To allocate, we find the first set free bit and then find the first clear free bit after that. If this range   is big enough for the allocation, we clear all of the free bits and set the object end bit. If it's not big   enough, we keep searching. We do special things (see below) when we cannot allocate in a page.</li> <li>To free, we find the first set object end bit, which then gives us the object size. Then we clear the object   end bit and set the free bits.</li> </ul> <p>This basic style of allocation is usually called bitmap allocation. Bitfit is a special kind of bitmap allocation that makes it cheap to find the first page that has enough space for an allocation of a given size. Bitfit makes allocation fast even when it is managing lots of pages by using two tricks:</p> <ul> <li>Bitfit directories have an array of bitfit views and a corresponding array of <code>max_free</code> bytes. Bitfit views   are monomorphic, unlike the polymorphic views of segregated heaps. Each bitfit view is either uninitialized or   has a bitfit page. A <code>max_free</code> byte for a page tells the maximum free object size in that page. So, in the   worst case, we search the <code>max_free</code> vector to find the find byte that is large enough for our allocation.</li> <li>Bitfit uses size classes to short-circuit the search. Bitfit leverages segregated heaps to create size   classes. Segregated size directories choose at creation time if they want to support segregated allocation or   bitfit allocation. If the latter, the directory is just used as a way of locating the bitfit size class. Like   with segregated, each local allocator is associated with a segregated size directory, even if it's a local   allocator configured for bitfit. Each size class maintains the index of the first view/page in the directory   that has a free object big enough for that size class.</li> </ul> <p>The updates to <code>max_free</code> and the short-circuiting indices in size classes happen when an allocation fails in a page. This is an ideal time to set those indices since failure to allocate happens to also tell you the size of the largest free object in the page.</p> <p>When any object is freed in a page, we mark the page as having <code>PAS_BITFIT_MAX_FREE_UNPROCESSED</code> and rather than setting any short-circuiting indices in size classes, we just set the <code>first_unprocessed_free</code> index in the <code>pas_bitfit_directory</code>. Allocation will start its search from the minimum of <code>directory-&gt;first_unprocessed_free</code> and <code>size_class-&gt;first_free</code>. All of these short-circuiting indices use <code>pas_versioned_field</code> just like how short-circuiting works in segregated directories.</p> <p>Bitfit heaps use fine-grained locking in the sense that each view has its own locks. But, there's no attempt made to have different threads avoid allocating in the same pages. Adding something like view caches to the bitfit heap is likely to make it much faster. You could even imagine that rather than having the <code>directory-&gt;first_unprocessed_free</code>, we instead have freeing in a page put the page's view onto a local view cache for that bitfit directory, and then we allocate out of the view cache until it's empty. Failure to allocate in a page in the view cache will then tell us the <code>max_free</code>, which will allow us to mark the view eligible in the directory.</p>"},{"location":"Deep%20Dive/Libpas/Internals.html#the-scavenger","title":"The Scavenger","text":"<p>Libpas returns memory to the OS by madvising it. This makes sense for a malloc that is trying to give strong type guarantees. If we unmapped memory, then the memory could be used for some totally unrelated type in the future. But by just decommitting the memory, we get the memory savings from free pages of memory and we also get to preserve type safety.</p> <p>The madvise system call -- and likely any mechanism for saying \"this page is empty\" -- is expensive enough that it doesn't make sense to do it anytime a page becomes empty. Pages often become empty only to refill again. In fact, last time I measured it, just about half of allocations went down the bump allocation path and (except for at start-up) that path is for completely empty pages. So, libpas has mechanisms for stashing the information that a page has become empty and then having a scavenger thread return that memory to the OS with an madvise call (or whatever mechanism). The scavenger thread is by default configured to run every 100ms, but will shut itself down if we have a period of non-use. At each tick, it returns all empty pages that have been empty for some length of time (currently 300ms). Those two thresholds -- the period and the target age for decommit -- are independently configurable at it might make sense (for different reasons) to have either number be bigger than the other number.</p> <p>This section describes the scavenging algorithm is detail. This is a large fraction of what makes libpas fast and space-efficient. The algorithm has some crazy things in it that probably didn't work out as well as I wanted but nonetheless those things seem to avoid showing up in profiles. That's sort of the outcome of this algorithm being tuned and twisted so many times during the development of this allocator. First I'll describe the deferred decommit log, which is how we coalesce madvise calls. Then I'll describe the page sharing pool, which is a mechanism for multiple participants to report that they have some empty pages. Then I'll describe how the large heap implements this with the large sharing pool, which is one of the singleton participants. Then I'll describe the segregated directory participants -- which are a bit different for shared page directories versus size directories. I'll also describe the bitfit directory participants, which are quite close to their segregated directory cousins. Then I'll describe some of the things that the scavenger does that aren't to do with the page sharing pool, like stopping baseline allocators, stopping utility heap allocators, stopping TLC allocators, flushing TLC deallocation logs, decommitting unused parts of TLCs, and decommitting expendable memory.</p>"},{"location":"Deep%20Dive/Libpas/Internals.html#deferred-decommit-log","title":"Deferred Decommit Log","text":"<p>Libpas's decommit algorithm coalesces madvise calls -- so if two adjacent pages, even from totally unrelated heaps, become empty, then their decommit will be part of one syscall. This is achieved by having places in the code that want to decommit memory instead add that memory to a deferred decommit log. This log internally uses a minheap based on address. The log also stores what lock was needed to decommit the range, since the decommit algorithm relies on fine-grained locking of memory rather than having a global commit lock. So, when the scavenger asks the page sharing pool to go find some memory to decommit, it gives it a deferred decommit log. The page sharing pool will usually return all of the empty pages in one go, so the deferred decommit logs can get somewhat big. They are allocated out of the bootstrap free heap (which isn't the best idea if they ever get very big, since bootstrap free heap memory is not decommitted -- but right now this is convenient because for a heap to support decommit, it needs to talk to deferred decommit logs, so we want to avoid infinite recursion). After the log is filled up, we can decommit everything in the log at once. This involves heapifying the array and then scanning it backwards while detecting adjacent ranges. This is the loop that actually calls decommit. A second loop unlocks the locks.</p> <p>Two fun complications arise:</p> <ul> <li>As the page sharing pool scans memory for empty pages, it may arrive at pages in a random order, which may   be different from any valid order in which to acquire commit locks. So, after acquiring the first commit lock,   all subsequent lock acquisitions are <code>try_lock</code>'s. If any <code>try_lock</code> fails, the algorithm returns early, and   the deferred decommit log helps facilitate detecting when a lock failed to be acquired.</li> <li>Libpas supports calling into the algorithm even if some commit locks, or the heap lock, are held. It's not   legal to try to acquire any commit locks other than by <code>try_lock</code> in that case. The deferred decommit log will   also make sure that it will not relock any commit locks that are already held.</li> </ul> <p>Libpas can be configured to return memory either by madvising it or by mmap-zero-filling it, which has a similar semantic effect but is slower. Libpas supports both symmetric and asymmetric forms of madvise, though the asymmetric form is faster and has a slight (though mostly theoretical) memory usage edge. By asymmetric I mean that you call some form of madvise to decommit memory and then do nothing to commit it. This works on Darwin and it's quite efficient -- the kernel will clear the decommit request and give the page real memory if you access the page. The memory usage edge of not explicitly committing memory in the memory allocator is that programs may allocate large arrays and never use the whole array. It's OK to configure libpas to use symmetric decommit, but the asymmetric variant might be faster or more efficient, if the target OS allows it.</p>"},{"location":"Deep%20Dive/Libpas/Internals.html#page-sharing-pool","title":"Page Sharing Pool","text":"<p>Different kinds of heaps have different ways of discovering that they have empty pages. Libpas supports three different kinds of heaps (large, segregated, and bitfit) and one of those heaps has two different ways of discovering free mempry (segregated shared page directories and segregated size directories). The page sharing pool is a data structure that can handle an arbitrary number of page sharing participants, each of which is able to say whether they have empty pages and whether those empty pages are old enough to be worth decommitting.</p> <p>Page sharing participants need to be able to answer the following queries:</p> <ul> <li>Getting the epoch of the oldest free page. This can be approximate; for example, it's OK for the participant   to occasionally give an epoch that is newer than the true oldest so long as this doesn't happen all the time.   We call this the <code>use_epoch</code>.</li> <li>Telling if the participant thinks it is eligible, i.e. has any free pages right now. It's OK for this to   return true even if there aren't free pages. It's not OK for this to return false if there are free pages. If   this returns true and there are no free pages, then it must return false after a bounded number of calls to   <code>take_least_recently_used</code>. Usually if a participant incorrectly says that it is eligible, then this state   will clear with exactly one call to <code>take_least_recently_used</code>.</li> <li>Taking the least recently used empty page (<code>pas_page_sharing_participant_take_least_recently_used</code>). This is   allowed to return that there aren't actually any empty pages. If there are free pages, this is allowed to   return any number of them (doesn't have to be just one). Pages are \"returned\" via a deferred decommit log.</li> </ul> <p>Participants also notify the page sharing pool when they see a delta. Seeing a delta means one of:</p> <ul> <li>The participant found a new free page and it previously thought that it didn't have any.</li> <li>The participant has discovered that the oldest free page is older than previously thought.</li> </ul> <p>It's OK to only report a delta if the participant knows that it was previously not advertising itself as being eligible, and it's also OK to report a delta every time that a free page is found. Most participants try to avoid reporting deltas unless they know that they were not previously eligible. However, some participants (the segregated and bitfit directories) are sloppy about reporting the epoch of the oldest free page. Those participants will conservatively report a delta anytime they think that their estimate of the oldest page's age has changed.</p> <p>The page sharing pool itself comprises:</p> <ul> <li>A segmented vector of page sharing participant pointers. Each pointer is tagged with the participant's type,   which helps the pool decide how to get the <code>use_epoch</code>, decide whether the participant is eligible, and take   free pages from it.</li> <li>A bitvector of participants that have reported deltas.</li> <li>A minheap of participants sorted by epoch of the oldest free memory.</li> <li>The <code>current_participant</code>, which the page sharing pool will ask for pages before doing anything else, so long   as there are no deltas and the current participant continues to be eligible and continues to report the same   use epoch.</li> </ul> <p>If the current participant is not set or does not meet the criteria, the heap lock is taken and all of the participants that have the delta bit set get reinserted into the minheap based on updated use epochs. It's possible that the delta bit causes the removal of entries in the minheap (if something stopped being eligible). It's possible that the delta bit causes the insertion of entries that weren't previously there (if something has just become eligible). And it's possible for an entry to be removed and then readded (if the use epoch changed). Then, the minimum of the minheap becomes the current participant, and we can ask it for pages.</p> <p>The pool itself is a class but it happens to be a singleton right now. It's probably a good idea to keep it as a class, because as I've experimented with various approaches to organizing memory, I have had versions where there are many pools. There is always the physical page sharing pool (the singleton), but I once had sharing pools for moving pages between threads and sharing pools for trading virtual memory.</p> <p>The page sharing pool exposes APIs for taking memory from the pool. There are two commonly used variants:</p> <ul> <li>Take a set number of bytes. The page sharing pool will try to take that much memory unless a try_lock fails,   in which case it records that it should take this additional amount of bytes on the next call.</li> <li>Take all free pages that are same age or older than some <code>max_epoch</code>. This API is called   <code>pas_physical_page_sharing_pool_scavenge</code>. This algorithm will only return when it is done. It will reloop in   case of <code>try_lock</code> failure, and it does this in a way that avoids spinning.</li> </ul> <p>The expected behavior is that when the page sharing pool has to get a bunch of pages it will usually get a run of them from some participant -- hence the emphasis on the <code>current_participant</code>. However, it's possible that various tweaks that I've made to the algorithm have made this no longer be the case. In that case, it might be worthwhile to try to come up with a way of recomputing he <code>current_participant</code> that doesn't require holding the <code>heap_lock</code>. Maybe even just a lock for the page sharing pool, rather than using the <code>heap_lock</code>, would be enough to get a speed-up, and in the best case that speed-up would make it easier to increase scavenger frequency.</p> <p>The page sharing pool's scavenge API is the main part of what the scavenger does. The next sections describe the inner workings of the page sharing participants.</p>"},{"location":"Deep%20Dive/Libpas/Internals.html#large-sharing-pool","title":"Large Sharing Pool","text":"<p>The large sharing pool is a singleton participant in the physical page sharing pool. It tracks which ranges of pages are empty across all of the large heaps. The idea is to allow large heaps to sometimes split a page among each other, so then no single large heap knows whether the page can be decommitted. So, all large heaps as well as the large utility free heap and compact large utility free heap report when they allocate and deallocate memory to the large sharing pool.</p> <p>Internally, the large sharing pool maintains a red-black tree and minheap. The tree tracks coalesced ranges of pages and their states. The data structure thinks it knows about all of memory, and it \"boots up\" with a single node representing the whole address space and that node claims to be allocated and committed. When heaps that talk to the large sharing pool acquire memory, they tell the sharing pool that the memory is now free. Any free-and-committed ranges also reside in the minheap, which is ordered by use epoch (the time when the memory became free).</p> <p>The large sharing pool can only be used while holding the <code>heap_lock</code>, but it uses a separate lock, the <code>pas_virtual_range_common_lock</code>, for commit and decommit. So, while libpas is blocked in the madvise syscall, it doesn't have to hold the <code>heap_lock</code> and the other lock only gets acquired when committing or decommitting large memory.</p> <p>It's natural for the large sharing pool to handle the participant API:</p> <ul> <li>The large sharing pool participant says it's eligible when the minheap is non-empty.</li> <li>The large sharing pool participant reports the minheap's minimum use epoch as its use epoch (though it can be   configured to do something else; that something else may not be interesting anymore).</li> <li>The large sharing pool participant takes least recently used by removing the minimum from the minheap and   adding that node's memory range to the deferred decommit log.</li> </ul> <p>The large sharing pool registers itself with the physical page sharing pool when some large heap reports the first bit of memory to it.</p>"},{"location":"Deep%20Dive/Libpas/Internals.html#segregated-directory-participant","title":"Segregated Directory Participant","text":"<p>Each segregated directory is a page sharing participant. They register themselves once they have pages that could become empty. Segregated directories use the empty bits and the last-empty-plus-one index to satisfy the page sharing pool participant API:</p> <ul> <li>A directory participant says it's eligible when last-empty-plus-one is nonzero.</li> <li>A directory participant reports the use epoch of the last empty page before where last-empty-plus-one points.   Note that in extreme cases this means having to search the bitvector for a set empty bit, since the   last-empty-plus-one could lag in being set to a lower value in case of races.</li> <li>A directory participant takes least recently used by searching backwards from last-empty-plus-one and taking   the last empty page. This action also updates last-empty-plus-one using the <code>pas_versioned_field</code> lock-free   tricks.</li> </ul> <p>Once an empty page is found the basic idea is:</p> <ol> <li>Clear the empty bit.</li> <li>Try to take eligibility; i.e. make the page not eligible. We use the eligible bit as a kind of lock    throughout the segregated heap; for example, pages will not be eligible if they are currently used by some    local allocator. If this fails, we just return. Note that it's up to anyone who makes a page ineligible to    then check if it should set the empty bit after they make it eligible again. So, it's fine for the scavenger    to not set the empty bit again after clearing it and failing to take the eligible bit. This also prevents a    spin where the scavenger keeps trying to look at this allegedly empty page even though it's not eligible. By    clearing the empty bit and not setting it again in this case, the scavenger will avoid this page until it    becomes eligible.</li> <li>Grab the ownership lock to say that the page is now decommitted.</li> <li>Grab the commit lock and then put the page on the deferred decommit log.</li> <li>Make the page eligible again.</li> </ol> <p>Sadly, it's a bit more complicated than that:</p> <ul> <li>Directories don't track pages; they track views. Shared page directories have views of pages whose actual   eligibility is covered by the eligible bits in the segregated size directories that hold the shared page's   partial views. So, insofar as taking eligibility is part of the algorithm, shared page directories have to   take eligibility for each partial view associated with the shared view.</li> <li>Shared and exclusive views could both be in a state where they don't even have a page. So, before looking at   anything about the view, it's necessary to take the ownership lock. In fact, to have a guarantee that nobody   is messing with the page, we need to grab the ownership lock and take eligibility. The actual algorithm does   these two things together.</li> <li>It's possible for the page to not actually be empty even though the empty bit is set. We don't require that   the empty bit is cleared when a page becomes nonempty.</li> <li>Some of the logic of decommitting requires holding the <code>page-&gt;lock_ptr</code>, which may be a different lock than   the ownership lock. So the algorithm actually takes the ownership lock, then the page lock, then the ownership   lock again, and then the commit lock.</li> <li>If a <code>try_lock</code> fails, then we set both the eligible and empty bits, since in that case, we really do want   the page sharing pool to come back to us.</li> </ul> <p>Some page configs support segregated pages that have multiple system pages inside them. In that case, the empty bit gets set when any system page becomes empty (using granule use counts), and the taking algorithm just decommits the granules rather than decommitting the whole page.</p>"},{"location":"Deep%20Dive/Libpas/Internals.html#bitfit-directory-participant","title":"Bitfit Directory Participant","text":"<p>Bitfit directories also use an empty bitvector and also support granules. Although bitfit directories are an independent piece of code, their approach to participating in page sharing pools exactly mirrors what segregated directories do.</p> <p>This concludes the discussion about the page sharing pool and its participants. Next, I will cover some of the other things that the scavenger does.</p>"},{"location":"Deep%20Dive/Libpas/Internals.html#stopping-baseline-allocators","title":"Stopping Baseline Allocators","text":"<p>The scavenger also routinely stops baseline allocators. Baseline allocators are easy to stop by the scavenger because they can be used by anyone who holds their lock. So the scavenger can stop any baseline allocator it wants. It will only stop those allocators that haven't been used in a while (by checking and resetting a dirty bit).</p>"},{"location":"Deep%20Dive/Libpas/Internals.html#stopping-utility-heap-allocators","title":"Stopping Utility Heap Allocators","text":"<p>The scavenger also does the same thing for utility allocators. This just requires holding the <code>heap_lock</code>.</p>"},{"location":"Deep%20Dive/Libpas/Internals.html#stopping-tlc-allocators","title":"Stopping TLC Allocators","text":"<p>Allocators in TLCs also get stopped, but this requires more effort. When the scavenger thread is running, it's possible for any thread to be using any of its allocators and no locks are held when this happens. So, the scavenger uses the following algorithm:</p> <ul> <li>First it tries to ask the thread to stop certain allocators. In each allocation slow path, the allocator   checks if any of the other allocators in the TLC have been requested to stop by the scavenger, and if so, it   stops those allocators. That doesn't require special synchronization because the thread that owns the   allocator is the one stopping it.</li> <li>If that doesn't work out, the scavenger suspends the thread and stops all allocators that don't have the   <code>in_use</code> bit set. The <code>in_use</code> bit is set whenever a thread does anything to a local allocator, and cleared   after.</li> </ul> <p>One wrinkle about stopping allocators is that stopped allocators might get decommitted. The way that this is coordinated is that a stopped allocator is in a special state that means that any allocation attempt will take a slow path that acquires the TLC's scavenging lock and possibly recommits some pages and then puts the allocator back into a normal state.</p>"},{"location":"Deep%20Dive/Libpas/Internals.html#flushing-tlc-deallocation-logs","title":"Flushing TLC Deallocation Logs","text":"<p>The TLC deallocation logs can be flushed by any thread that holds the scavenging lock. So, the scavenger thread flushes all deallocation logs that haven't been flushed recently.</p> <p>When a thread flushes its own log, it holds the scavenger lock. However, appending doesn't grab the lock. To make this work, when the scavenger flushes the log, it:</p> <ul> <li>Replaces any entry in the log that it deallocated with zero. Actually, the deallocation log flush always does   this.</li> <li>Does not reset the <code>thread_local_cache-&gt;deallocation_log_index</code>. In fact, it doesn't do anything except read   the field.</li> </ul> <p>Because of the structure of the deallocation log flush, it's cheap for it to null-check everything it loads from the deallocation log. So when, a thread goes to flush its log after the scavenger has done it, it will see a bunch of null entries, and it will skip them. If a thread tries to append to the deallocation log while the scavenger is flushing it, then this just works, because it ends up storing the new value above what the scavenger sees. The object is still in the log, and will get deallocated on the next flush.</p>"},{"location":"Deep%20Dive/Libpas/Internals.html#decommitting-unused-parts-of-tlcs","title":"Decommitting Unused Parts of TLCs","text":"<p>For any page in the TLC consisting entirely of stopped allocators, the scavenger can decommit those pages. The zero value triggers a slow path in the local allocator and local view cache that then commits the page and rematerializes the allocator. This is painstakingly ensured; to keep this property you generally have to audit the fast paths to see which parts of allocators they access, and make sure that the allocator goes to a slow path if they are all zero. That slow path then has to check if the allocator is stopped or decommitted, and if it is either, it grabs the TLC's scavenger lock and recommits and rematerializes.</p> <p>This feature is particularly valuable because of how big local allocators and local view caches are. When there are a lot of heaps, this accounts for tens of MBs in some cases. So, being able to decommit unused parts is a big deal.</p>"},{"location":"Deep%20Dive/Libpas/Internals.html#decommitting-expendable-memory","title":"Decommitting Expendable Memory","text":"<p>Segregated heaps maintain lookup tables that map \"index\", i.e. the object size divided by the heap's minalign, to allocator indices and directories. There are three tables:</p> <ul> <li>The index-to-allocator-index table. This only works for small-enough indices.</li> <li>The index-to-directory table. This also only works for small-enough indices.</li> <li>The medium index-to-directory-tuple binary search array. This works for the not-small-enough indices.</li> </ul> <p>It makes sense to let those tables get large. It might make sense for each isoheap to have a large lookup table. Currently, they use some smaller threshold. But to make that not use a lot of memory, we need to be able to decommit memory that is only used by lookup tables of heaps that nobody is using.</p> <p>So, libpas has this weird thing called <code>pas_expendable_memory</code>, which allows us to allocate objects of immortal memory that automatically get decommitted if we don't \"touch\" them frequently enough. The scavenger checks the state of all expendable memory, and decommits those pages that haven't been used in a while. The expendable memory algorithm is not wired up as a page sharing participant because the scavenger really needs to poke the whole table maintained by the algorithm every time it does a tick; otherwise the algorithm would not work. Fortunately, there's never a lot of this kind of memory. So, this isn't a performance problem as far as I know. Also, it doesn't actually save that much memory right now -- but also, right now isoheaps use smaller-size tables.</p> <p>This concludes the discussion of the scavenger. To summarize, the scavenger periodically:</p> <ul> <li>Stops baseline allocators.</li> <li>Stops utility heap allocators.</li> <li>Stops TLC allocators.</li> <li>Flushes TLC deallocation logs.</li> <li>Decommits unused parts of TLCs.</li> <li>Decommits expendable memory.</li> <li>Asks the physical page sharing pool to scavenge.</li> </ul> <p>While it does these things, it notes whether anything is left in a state where it would be worthwhile to run again. A steady state might look like that all empty pages have been decommitted, rather than that only old enough ones were decommitted. If a steady state looks like it's being reached, the scavenger first sleeps for a while, and then shuts down entirely.</p>"},{"location":"Deep%20Dive/Libpas/Internals.html#megapages-and-page-header-tables","title":"Megapages and Page Header Tables","text":"<p>Libpas provides non-large heaps two fast and scalable ways of figuring out about an object based on its address, like during deallocation, or during user queries like <code>malloc_size</code>.</p> <ol> <li>Megapages and the megapage table. Libpas describes every 16MB of memory using a two-bit enum called    <code>pas_fast_megapage_kind</code>. The zero state indicates that this address does not have a fast megapage. The    other two describe two different kinds of fast megapages, one where you know exactly the type of page it    is (small exclusive segregated) and one where you have to find out by asking the page header, but at least    you know that it's small (so usually 16KB).</li> <li>Page header table. This is a lock-free-to-read hashtable that tells you where to find the page header for    some page in memory. For pages that use page headers, we also use the page header table to find out if the    memory address is in memory owned by some kind of \"page\" (either a segregated page or a bitfit page).</li> </ol> <p>Usually, the small segregated and small bitfit configs use megapages. The medium segregated, medium bitfit, and marge bitfit configs use page header tables. Large heaps don't use either, since they have the large map.</p>"},{"location":"Deep%20Dive/Libpas/Internals.html#the-enumerator","title":"The Enumerator","text":"<p>Libpas supports the libmalloc enumerator API. It even includes tests for it. The way that this is implemented revolves around the <code>pas_enumerator</code> class. Many of the data structures that are needed by the enumerator have APIs to support enumeration that take a <code>pas_enumerator*</code>. The idea behind how it works is conceptually easy: just walk the heap -- which is possible to do accurately in libpas -- and report the metadata areas, the areas that could contain objects, and the live objects. But, we have to do this while walking a heap in a foreign process, and we get to see that heap through little copies of it that we request a callback to make for us. The code for enumeration isn't all that pretty but at least it's easy to find most of it by looking for functions and files that refer to <code>pas_enumerator</code>.</p> <p>But a challenge of enumeration is that it happens when the remote process is stopped at some arbitrary point. That point has to be an instruction boundary -- so CPU memory model issues aren't a problem. But this means that the whole algorithm has to ensure that at any instruction boundary, the enumerator will see the right thing. Logic to help the enumerator still understand the heap at any instruction is spread throughout the allocation algorithms. Even the trees and hashtables used by the large heap have special hacks to enable them to be enumerable at any instruction boundary.</p> <p>The enumerator is maintained so that it's 100% accurate. Any discrepancy -- either in what objects are reported live or what their sizes are -- gets flagged as an error by <code>test_pas</code>. The goal should be to maintain perfect enumerator accuracy. This makes sense for two reasons:</p> <ol> <li>I've yet to see a case where doing so is a performance or memory regression.</li> <li>It makes the enumerator easy to test. I don't know how to test an enumerator that is not accurate by design.    If it's accurate by design, then any discrepancy between the test's understanding of what is live and what    the enumerator reports can be flagged as a test failure. If it wasn't accurate by design, then I don't know    what it would mean for a test to fail or what the tests could even assert.</li> </ol>"},{"location":"Deep%20Dive/Libpas/Internals.html#the-basic-configuration-template","title":"The Basic Configuration Template","text":"<p>Libpas's heap configs and page configs allow for a tremendous amount of flexibility. Things like the utility heap and the <code>jit_heap</code> leverage this flexibility to do strange things. However, if you're using libpas to create a normal malloc, then a lot of the configurability in the heap/page configs is too much. A \"normal\" malloc is one that is exposed as a normal API rather than being internal to libpas, and that manages memory that doesn't have special properties like that it's marked executable and not writeable.</p> <p>The basic template is provided by <code>pas_heap_config_utils.h</code>. To define a new config based on this template, you need to:</p> <ul> <li>Add the appropriate heap config and page config kinds to <code>pas_heap_config_kind.def</code>,   <code>pas_segregated_page_config_kind.def</code>, and <code>pas_bitfit_page_config_kind.def</code>. You also have to do this if you   add any kind of config, even one that doesn't use the template.</li> <li>Create the files <code>foo_heap_config.h</code> and <code>foo_heap_config.c</code>. These are mostly boilerplate.</li> </ul> <p>The header file usually looks like this:</p> <pre><code>#define ISO_MINALIGN_SHIFT ((size_t)4)\n#define ISO_MINALIGN_SIZE ((size_t)1 &lt;&lt; ISO_MINALIGN_SHIFT)\n\n#define ISO_HEAP_CONFIG PAS_BASIC_HEAP_CONFIG( \\\n    iso, \\\n    .activate = pas_heap_config_utils_null_activate, \\\n    .get_type_size = pas_simple_type_as_heap_type_get_type_size, \\\n    .get_type_alignment = pas_simple_type_as_heap_type_get_type_alignment, \\\n    .dump_type = pas_simple_type_as_heap_type_dump, \\\n    .check_deallocation = true, \\\n    .small_segregated_min_align_shift = ISO_MINALIGN_SHIFT, \\\n    .small_segregated_sharing_shift = PAS_SMALL_SHARING_SHIFT, \\\n    .small_segregated_page_size = PAS_SMALL_PAGE_DEFAULT_SIZE, \\\n    .small_segregated_wasteage_handicap = PAS_SMALL_PAGE_HANDICAP, \\\n    .small_exclusive_segregated_logging_mode = pas_segregated_deallocation_size_oblivious_logging_mode, \\\n    .small_shared_segregated_logging_mode = pas_segregated_deallocation_no_logging_mode, \\\n    .small_exclusive_segregated_enable_empty_word_eligibility_optimization = false, \\\n    .small_shared_segregated_enable_empty_word_eligibility_optimization = false, \\\n    .small_segregated_use_reversed_current_word = PAS_ARM64, \\\n    .enable_view_cache = false, \\\n    .use_small_bitfit = true, \\\n    .small_bitfit_min_align_shift = ISO_MINALIGN_SHIFT, \\\n    .small_bitfit_page_size = PAS_SMALL_BITFIT_PAGE_DEFAULT_SIZE, \\\n    .medium_page_size = PAS_MEDIUM_PAGE_DEFAULT_SIZE, \\\n    .granule_size = PAS_GRANULE_DEFAULT_SIZE, \\\n    .use_medium_segregated = true, \\\n    .medium_segregated_min_align_shift = PAS_MIN_MEDIUM_ALIGN_SHIFT, \\\n    .medium_segregated_sharing_shift = PAS_MEDIUM_SHARING_SHIFT, \\\n    .medium_segregated_wasteage_handicap = PAS_MEDIUM_PAGE_HANDICAP, \\\n    .medium_exclusive_segregated_logging_mode = pas_segregated_deallocation_size_aware_logging_mode, \\\n    .medium_shared_segregated_logging_mode = pas_segregated_deallocation_no_logging_mode, \\\n    .use_medium_bitfit = true, \\\n    .medium_bitfit_min_align_shift = PAS_MIN_MEDIUM_ALIGN_SHIFT, \\\n    .use_marge_bitfit = true, \\\n    .marge_bitfit_min_align_shift = PAS_MIN_MARGE_ALIGN_SHIFT, \\\n    .marge_bitfit_page_size = PAS_MARGE_PAGE_DEFAULT_SIZE, \\\n    .pgm_enabled = false)\n\nPAS_API extern pas_heap_config iso_heap_config;\n\nPAS_BASIC_HEAP_CONFIG_DECLARATIONS(iso, ISO);\n</code></pre> <p>Note the use of <code>PAS_BASIC_HEAP_CONFIG</code>, which creates a config literal that automatically fills in a bunch of heap config, segregated page config, and bitfit page config fields based on the arguments you pass to <code>PAS_BASIC_HEAP_CONFIG</code>. The corresponding <code>.c</code> file looks like this:</p> <pre><code>pas_heap_config iso_heap_config = ISO_HEAP_CONFIG;\n\nPAS_BASIC_HEAP_CONFIG_DEFINITIONS(\n    iso, ISO,\n    .allocate_page_should_zero = false,\n    .intrinsic_view_cache_capacity = pas_heap_runtime_config_zero_view_cache_capacity);\n</code></pre> <p>Note that this just configures whether new pages are zeroed and what the view cache capacity for the intrinsic heap are. The intrinsic heap is one of the four categories of heaps that the basic heap configuration template supports:</p> <ul> <li>Intrinsic heaps are global singleton heaps, like the common heap for primitives. WebKit's fastMalloc bottoms   out in an intrinsic heap.</li> <li>Primitive heaps are heaps for primitive untyped values, but that aren't singletons. You can have many   primitive heaps.</li> <li>Typed heaps have a type, and the type has a fixed size and alignment. Typed heaps allow allocating single   instances of objects of that type or arrays of that type.</li> <li>Flex heaps are for objects with flexible array members. They pretend as if their type has size and alignment   equal to 1, but in practice they are used for objects that have some base size plus a variable-length array.   Note that libpas doesn't correctly manage flex memory in the large heap; we need a variant of the large heap   that knows that you cannot reuse flex memory between different sizes.</li> </ul> <p>The basic heap config template sets up some basic defaults for how heaps work:</p> <ul> <li>It makes small segregated and small bitfit page configs put the page header at the beginning of the page and   it arranges to have those pages allocated out of megapages.</li> <li>It makes medium segregated, medium bitfit, and marge bitfit use page header tables.</li> <li>It sets up a way to find things like the page header tables from the enumerator.</li> <li>It sets up segregated shared page directories for each of the segregated page configs.</li> </ul> <p>The <code>bmalloc_heap_config</code> is an example of a configuration that uses the basic template. If we ever wanted to put libpas into some other malloc library, we'd probably create a heap config for that library, and we would probably base it on the basic heap config template (though we don't absolutely have to).</p>"},{"location":"Deep%20Dive/Libpas/Internals.html#jit-heap-config","title":"JIT Heap Config","text":"<p>The JIT heap config is for replacing the MetaAllocator as a way of doing executable memory allocation in WebKit. It needs to satisfy two requirements of executable memory allocation:</p> <ul> <li>The allocator cannot read or write the memory it manages, since that memory may have weird permissions at any   time.</li> <li>Clients of the executable allocator must be able to in-place shrink allocations.</li> </ul> <p>The large heap trivially supports both requirements. The bitfit heap trivially supports the second requirement, and can be made to support the first requirement if we use page header tables for all kinds of memory, not just medium or marge. So, the JIT heap config focuses on just using bitfit and large and it forces bitfit to use page header tables even for the small bitfit page config.</p>"},{"location":"Deep%20Dive/Libpas/Internals.html#security-considerations","title":"Security Considerations","text":""},{"location":"Deep%20Dive/Libpas/Internals.html#probabilistic-guard-malloc","title":"Probabilistic Guard Malloc","text":"<p>Probabilistic Guard Malloc (PGM) is a new allocator designed to catch use after free attempts and out of bounds accesses. It behaves similarly to AddressSanitizer (ASAN), but aims to have minimal runtime overhead.</p>"},{"location":"Deep%20Dive/Libpas/Internals.html#allocation","title":"Allocation","text":"<p>Each time an allocation is performed an additional guard page is added above and below the newly allocated page(s). An allocation may span multiple pages. Allocations are either left or right aligned at random, which ensures the catching of both overflow and underflow errors.</p>"},{"location":"Deep%20Dive/Libpas/Internals.html#deallocation","title":"Deallocation","text":"<p>When a deallocation is performed, the page(s) allocated will be protected using <code>mprotect</code> to ensure that any use after frees will trigger a crash. Virtual memory addresses are never reused, so we will never run into a case where object 1 is freed, object 2 is allocated over the same address space, and object 1 then accesses the memory address space of now object 2.</p>"},{"location":"Deep%20Dive/Libpas/Internals.html#memory-usage","title":"Memory Usage","text":"<p>PGM does add notable memory overhead. Each allocation, no matter the size, adds an additional 2 guard pages (8KB for X86_64 and 32KB for ARM64). In addition, there may be free memory left over in the page(s) allocated for the user. This memory may not be used by any other allocation.</p> <p>We added limits on virtual memory and wasted memory to help limit the memory impact on the overall system. Virtual memory for this allocator is limited to 1GB. Wasted memory, which is the unused memory in the page(s) allocated by the user, is limited to 1MB. These overall limits should ensure that the memory impact on the system is minimal, while helping to tackle the problems of catching use after frees and out of bounds accesses.</p>"},{"location":"Deep%20Dive/Libpas/Internals.html#the-fast-paths","title":"The Fast Paths","text":"<p>All of the discussion in the previous sections is about the innards of libpas. But ultimately, clients want to just call malloc-like and free-like functions to manage memory. Libpas provides fast path templates that actual heap implementations reuse to provide malloc/free functions. The fast paths are:</p> <ul> <li><code>pas_try_allocate.h</code>, which is the single object allocation fast path for isoheaps. This function just takes a   heap and no size; it allocates one object of the size and alignment that the heap's type wants.</li> <li><code>pas_try_allocate_array.h</code>, which is the array and aligned allocation fast path for isoheaps. You want to use   it with heaps that have a type, and that type has a size and alignment, and you want to allocate arrays of   that type or instances of that type with special alignment.</li> <li><code>pas_try_allocate_primitive.h</code>, which is the primitive object allocation fast path for heaps that don't have   a type (i.e. they have the primitive type as their type -- the type says it has size and alignment equal to   1).</li> <li><code>pas_try_allocate_intrinsic.h</code>, which is the intrinsic heap allocation fast path.</li> <li><code>pas_try_reallocate.h</code>, which provides variants of all of the allocators that reallocate memory.</li> <li><code>pas_deallocate.h</code>, which provides the fast path for <code>free</code>.</li> <li><code>pas_get_allocation_size.h</code>, which is the fast path for <code>malloc_size</code>.</li> </ul> <p>One thing to remember when dealing with the fast paths is that they are engineered so that malloc/free functions do not have a stack frame, no callee saves, and don't need to save the LR/FP to the stack. To facilitate this, we have the fast path call an inline-only fast path, and if that fails, we call a \"casual case\". The inline-only fast path makes no out-of-line function calls, since if it did, we'd need a stack frame. The only slow call (to the casual case) is a tail call. For example:</p> <pre><code>static PAS_ALWAYS_INLINE void* bmalloc_try_allocate_inline(size_t size)\n{\n    pas_allocation_result result;\n    result = bmalloc_try_allocate_impl_inline_only(size, 1);\n    if (PAS_LIKELY(result.did_succeed))\n        return (void*)result.begin;\n    return bmalloc_try_allocate_casual(size);\n}\n</code></pre> <p>The way that the <code>bmalloc_try_allocate_impl_inline_only</code> and <code>bmalloc_try_allocate_casual</code> functions are created is with:</p> <pre><code>PAS_CREATE_TRY_ALLOCATE_INTRINSIC(\n    bmalloc_try_allocate_impl,\n    BMALLOC_HEAP_CONFIG,\n    &amp;bmalloc_intrinsic_runtime_config.base,\n    &amp;bmalloc_allocator_counts,\n    pas_allocation_result_identity,\n    &amp;bmalloc_common_primitive_heap,\n    &amp;bmalloc_common_primitive_heap_support,\n    pas_intrinsic_heap_is_designated);\n</code></pre> <p>All allocation fast paths require this kind of macro that creates a bunch of functions -- both the inline paths and the out-of-line paths. The deallocation, reallocation, and other fast paths are simpler. For example, deallocation is just:</p> <pre><code>static PAS_ALWAYS_INLINE void bmalloc_deallocate_inline(void* ptr)\n{\n    pas_deallocate(ptr, BMALLOC_HEAP_CONFIG);\n}\n</code></pre> <p>If you look at <code>pas_deallocate</code>, you'll see cleverness that ensures that the slow path call is a tail call, similarly to how allocators work. However, for deallocation, I haven't had the need to make the slow call explicit in the client side (the way that <code>bmalloc_try_allocate_inline</code> has to explicitly call the slow path).</p> <p>This concludes the discussion of libpas design.</p>"},{"location":"Deep%20Dive/Libpas/Internals.html#testing-libpas","title":"Testing Libpas","text":"<p>I've tried to write test cases for every behavior in libpas, to the point that you should feel comfortable dropping a new libpas in WebKit (or wherever) if <code>test_pas</code> passes.</p> <p><code>test_pas</code> is a white-box component, regression, and unit test suite. It's allowed to call any libpas function, even internal functions, and sometimes functions that libpas exposes only for the test suite.</p> <p>Libpas testing errs on the side of being comprehensive even if this creates annoying situations. Many tests assert detailed things about how many objects fit in a page, what an object's offset is in a page, and things like that. This means that some behavior changes in libpas that aren't in any way wrong will set off errors in the test suite. So, it's common to have to rebase tests when making libpas changes.</p> <p>The libpas test suite is written in C++ and uses its own test harness that forks for each test, so each test runs in a totally pristine state. Also, the test suite can use <code>malloc</code> and <code>new</code> as much as it likes, since in the test suite, libpas does not replace <code>malloc</code> and <code>free</code>.</p> <p>The most important libpas tests are the so-called chaos tests, which randomly create and destroy objects and assert that the heap's state is still sane (like that no live objects overlap, that all live objects are enumerable, etc).</p>"},{"location":"Deep%20Dive/Libpas/Internals.html#conclusion","title":"Conclusion","text":"<p>Libpas is a beast of a malloc, designed for speed, memory efficiency, and type safety. May whoever maintains it find some joy in this insane codebase!</p>"},{"location":"Deep%20Dive/Libpas/Libpas.html","title":"Libpas","text":"<p>Libpas is a configurable memory allocator toolkit designed to enable adoption of isoheaps. Currently, libpas' main client is WebKit's bmalloc project, where it's used as a replacement for all of bmalloc's functionality (bmalloc::api, IsoHeap&lt;&gt;, and Gigacage). Libpas' jit_heap API is also used by WebKit's ExecutableAllocator.</p>"},{"location":"Deep%20Dive/Libpas/Libpas.html#overview","title":"Overview","text":"<p>Libpas is a toolkit for creating memory allocators. When built as a dylib, it exposes a bunch of different memory allocators, with different configurations ranging from sensible to deranged, without overriding malloc and friends. For production use, libpas is meant to be built as part of some larger malloc project; for example, when libpas sees the PAS_BMALLOC macro, it will provide everything that WebKit's bmalloc library needs to create an allocator.</p> <p>Libpas' toolkit of allocators has three building blocks:</p> <ul> <li> <p>The segregated heap. This implements something like simple segregated   storage. Roughly speaking, size classes hold onto collections of pages, and   each page contains just objects of that size. The segregated heap also has   some support for pages having multiple size classes, but this is intended as   a \"cold start\" mode to reduce internal fragmentation. The segregated heap   works great as an isoheap implementation since libpas makes it easy and   efficient to create lots of heaps, and heaps have special optimizations for   having only one size class. Segregated heaps are also super fast. They beat   the original bmalloc at object churn performance. A typical segregated   allocation operation does not need to use any atomic operations or fences   and only relies on a handful of loads, some addition and possibly bit math,   and a couple stores. Ditto for a typical segregated deallocation operation.</p> </li> <li> <p>The bitfit heap. This implements first-fit allocation using bit-per-minalign-   granule. Bitfit heaps are super space-efficient but not nearly as fast as   segregated heaps. Bitfit heaps have an upper bound on the sizes they can   handle, but can be configured to allocate objects up to hundreds of KB.   Bitfit heaps are not appropriate for isoheaps. Bitfit is used mostly for   \"marge\" allocations (larger than segregated's medium but smaller than large)   and for the jit_heap.</p> </li> <li> <p>The large heap. This implements a cartesian-tree-based first-fit allocator   for arbitrary sized objects. Large heaps are appropriate for isoheaps; for   example they remember the type of every free object in memory and they   remember where the original object boundaries are.</p> </li> </ul> <p>Each of the building blocks can be configured in a bunch of ways. Libpas uses a C template programming style so that the segregated and bitfit heaps can be configured for different page sizes, minaligns, security-performance trade-offs, and so on.</p> <p>All of the heaps are able to participate in physical page sharing. This means that anytime any system page of memory managed by the heap becomes totally empty, it becomes eligible for being returned to the OS via decommit. Libpas' decommit strategy is particularly well tuned so as to compensate for the inherent memory overheads of isoheaping. Libpas achieves much better memory usage than bmalloc because it returns pages sooner than bmalloc would have, and it does so with neglibile performance overheads. For example, libpas can be configured to return a page to the OS anytime it has been free for 300ms.</p> <p>Libpas is a heavy user of fine-grained locking and intricate lock dancing. Some data structures will be protected by any of a collection of different locks, and lock acquisition involves getting the lock, checking if you got the right lock, and possibly relooping. Libpas' algorithms are designed around:</p> <ul> <li> <p>Reducing the likelihood that any long-running operation would want to hold a   lock that any frequently-running operation would ever need. For example,   decommitting a page requires holding a lock and making a syscall, but the   lock that gets held is one that has a low probability of ever being needed   during any other operation. That \"low probability\" is not guaranteed but it   is made so thanks to lots of different tricks.</p> </li> <li> <p>Reducing the likelihood that two operations that run in a row that both grab   locks would need to grab different locks. Libpas has a bunch of tricks to   make it so that data structures that get accessed together dynamically adapt   to using the same locks to reduce the total number of lock acquisitions.</p> </li> </ul> <p>Finally, libpas is designed for having lots of tests, including unit and mock tests. While libpas itself is written in C (to reduce friction of adoptiong it in either C or C++ codebases), the test suite is written in C++. Ideally everything that libpas does has a test.</p>"},{"location":"Deep%20Dive/Libpas/LibpasBuildAndTest.html","title":"How To Build And Test","text":"<p>This section describes how to build libpas standalone. You'll be doing this a lot when making changes to libpas. It's wise to run libpas' tests before trying out your change in any larger system (like WebKit) since libpas tests are great at catching bugs. If libpas passes its own tests then basic browsing will seem to work. In production, libpas gets built as part of some other project (like bmalloc), which just pulls all of libpas' files into that project's build system.</p> <p>Build and Test</p> <pre><code>./build_and_test.sh\n</code></pre> <p>Build</p> <pre><code>./build.sh\n</code></pre> <p>Test</p> <pre><code>./test.sh\n</code></pre> <p>Clean</p> <pre><code>./clean.sh\n</code></pre> <p>On my M1 machine, I usually do this</p> <pre><code>./build_and_test.sh -a arm64e\n</code></pre> <p>This avoids building fat arm64+arm64e binaries.</p> <p>The libpas build will, by default, build (and test, if you're use <code>build_and_test.sh</code> or <code>test.sh</code>) both a testing variant and a default variant. The testing variant has testing-only assertions. Say you're doing some speed tests and you just want to build the default variant:</p> <pre><code>./build.sh -v default\n</code></pre> <p>By default, libpas builds Release, but you can change that:</p> <pre><code>./build.sh -c Debug\n</code></pre> <p>All of the tools (<code>build.sh</code>, <code>test.sh</code>, <code>build_and_test.sh</code>, and <code>clean.sh</code>) take the same options (<code>-h</code>, <code>-c &lt;config&gt;</code>, <code>-s &lt;sdk&gt;</code>, <code>-a &lt;arch&gt;</code>, <code>-v &lt;variant&gt;</code>, <code>-t &lt;target&gt;</code>, and <code>-p &lt;port&gt;</code>).</p> <p>Libpas creates multiple binaries (<code>test_pas</code> and <code>chaos</code>) during compilation, which are used by <code>test.sh</code>. Calling these binaries directly may be preferred if you would like to test or debug just one or a handful of test cases.</p> <p><code>test_pas</code> allows you to filter which test cases will be run. These are a few examples</p> <pre><code>  ./test_pas 'JITHeapTests' # Run all JIT heap tests\n  ./test_pas 'testPGMSingleAlloc()' # Run specific test\n  ./test_pas '(1):' # Run test case 1\n</code></pre>"},{"location":"Deep%20Dive/Modules/CompressionStreams.html","title":"Compression Streams","text":"<p>Compression Streams Module</p>"},{"location":"Deep%20Dive/Modules/CompressionStreams.html#overview","title":"Overview","text":"<p>Compression Streams offer a built in way to compress and decompress data based on several common file formats.</p>"},{"location":"Deep%20Dive/Modules/CompressionStreams.html#supported-formats","title":"Supported Formats","text":"Format Specification gzip RFC1952 deflate (also known as ZLIB) RFC1950 deflate-raw RFC1951"},{"location":"Deep%20Dive/Modules/CompressionStreams.html#specification","title":"Specification","text":"<ul> <li>Online Specification</li> <li>Compression Streams GitHub</li> </ul>"},{"location":"Deep%20Dive/Modules/CompressionStreams.html#design","title":"Design","text":"<p>Compression Streams work by passing a chunk of data (uint8 array) in and then returning the compression/decompression output of that chunk. There can be many chunks of data passed in during a compression/decompression stream.</p> <p>Speed is of paramount importance when trying to compress/decompress data.</p> <p>In Compression Streams, we try to allocate enough memory up front, so we only have to perform the compression once. For Decompression Streams, we employee a 2x increase in memory allocated each pass we attempt to decompress a chunk (with a cap of 1GB). This design was done to ensure minimal amounts of allocations being required, and to limit the number of times we needed to call inflate/deflate.</p> <p>Of course with the increase in memory usage, on smaller memory devices we may hit a cap. To handle this, we employee a back off algorithm that will quickly scale back memory allocation size if they start to fail.</p> <p>We currently buffer all the compression/decompression output per chunk, and return them to the user all at once. We could also consider the possibility of returning the chunk output to the user after every call to inflate/deflate.</p>"},{"location":"Deep%20Dive/Modules/CompressionStreams.html#libraries","title":"Libraries","text":"<p>On Windows and Linux we exclusively use zlib to perform the heavy lifting for all compression and decompression algorithms. </p> <p>On Apple platforms we leverage the Compression Framework for the deflate-raw decompression to achieve extra performance. The rest of the operations use zlib. Compression Framework only supports deflate-raw for our use case, and there are currently no performance benefits for using this framework to handle compression over zlib.</p>"},{"location":"Deep%20Dive/Modules/CompressionStreams.html#notes","title":"Notes","text":"<p>Compression Streams does contain one minor flaw for the gzip and deflate algorithms. There is a checksum located at the end of both of these file formats. This means that you can decompress a large file, but if any issue occurs during verifying the checksum step you may not be able to trust any of the output. This is ultimately due to the file formats design, but it does not allow for any real salvaging of any of the decompressed data if something goes wrong.</p>"},{"location":"Deep%20Dive/Modules/MediaSourceExtensions.html","title":"Media Source Extensions","text":""},{"location":"Deep%20Dive/Modules/MediaSourceExtensions.html#basic-concepts","title":"Basic Concepts","text":"<p>The Media Source Extensions specification defines a set of classes which allows clients to implement their own loading, buffering, and variant switching behavior, as opposed to requiring the UA to handle same.</p> <p>Clients <code>fetch()</code> media initialization segments and media segments, typically subsets of a single fragmented MP4 file or WebM file, and append those segments into a SourceBuffer object, which is associated with a HTMLMediaElement through a MediaSource object.</p>"},{"location":"Deep%20Dive/Modules/MediaSourceExtensions.html#relevant-classes","title":"Relevant Classes","text":""},{"location":"Deep%20Dive/Modules/MediaSourceExtensions.html#mediasource","title":"MediaSource","text":"<p>(.idl, .h, .cpp)</p> <p>MediaSource serves two purposes:</p> <ul> <li>Creating SourceBuffer objects.</li> <li>Associating those SourceBuffer objects with a HTMLMediaElement.</li> </ul> <p>Once created, clients can create query for container and codec support via <code>isTypeSupported(type)</code>, SourceBuffer objects via <code>addSourceBuffer(type)</code>, explicitly set the MediaSource's <code>duration</code>, and signal an end of the stream via <code>endOfStream(error)</code>.</p> <p>Before creating any SourceBuffer objects, the MediaSource must be associated with a HTMLMediaElement. The MediaSource can be set directly as the HTMLMediaElement's <code>srcObject</code>. Alternatively, an extension to DOMURL allows an ObjectURL to be created from a MediaSource object, and that ObjectURL can be set as the HTMLMediaElement's <code>src</code>.</p> <p>A MediaSource object will fire a <code>\"sourceopen\"</code> event when successfully associated with a HTMLMediaElement, and a <code>\"sourceclose\"</code> event when disassociated.  The state of the MediaSource object can be queried via its <code>readyState</code> property.</p>"},{"location":"Deep%20Dive/Modules/MediaSourceExtensions.html#sourcebuffer","title":"SourceBuffer","text":"<p>(.idl, .h, .cpp)</p> <p>SourceBuffer accepts buffers of initialization segments and media segments, which are then parsed into media tracks and media samples. Those samples are cached within the SourceBuffer (inside its SourceBufferPrivate object)  and enqueued into platform-specific decoders on demand. The primary storage mechanism for these samples is a SampleMap, which orders those samples both in terms of each sample's DecodeTime and PresentationTime. These two times can differ for codecs that support frame reordering, typically MPEG video codecs such as h.264 and HEVC.</p> <p>Clients append these segments via <code>appendBuffer()</code>, which sets an internal <code>updating</code> flag, fires the <code>\"updatestart\"</code> event, and subsequently fires the <code>\"updateend\"</code> event and clears the <code>updating</code> flag once parsing is complete. The results of the append are visible by querying the <code>buffered</code> property, or by querying the <code>audioTracks</code>, <code>videoTracks</code>, and <code>textTracks</code> TrackList objects.</p>"},{"location":"Deep%20Dive/Modules/MediaSourceExtensions.html#mediasourceprivate","title":"MediaSourcePrivate","text":"<p>(.h)</p> <p>MediaSourcePrivate is an abstract base class which allows MediaSource to communicate through the platform boundary to a platform-specific implementation of MediaSource.</p> <p>When the GPU Process is enabled, the MediaSourcePrivate in the WebContent process is typically a MediaSourcePrivateRemote, which will pass commands and properties across the WebContent/GPU process boundary.</p> <p>For Apple ports, the MediaSourcePrivate is typically a MediaSourcePrivateAVFObjC.</p> <p>For GStreamer-based ports, the MediaSourcePrivate is typically a MediaSourcePrivateGStreamer.</p> <p>When running in DumpRenderTree/WebKitTestRunner, a \"mock\" MediaSourcePrivate can be enabled, and a MockMediaSourcePrivate can be created. This is useful for writing platform-independent tests which exercise the platform-independent MediaSource and SourceBuffer objects directly.</p>"},{"location":"Deep%20Dive/Modules/MediaSourceExtensions.html#sourcebufferprivate","title":"SourceBufferPrivate","text":"<p>(.h, .cpp)</p> <p>SourceBufferPrivate is a semi-abstract base class which accepts initialization segment and media segment buffers, parse those buffers with platform-specific parsers, and enqueue the resulting samples into platform-specific decoders.  SourceBufferPrivate is also responsible for caching parsed samples in a SampleMap.</p>"},{"location":"Deep%20Dive/Modules/MediaSourceExtensions.html#mediatime","title":"MediaTime","text":"<p>(.h, .cpp)</p> <p>MediaTime is a rational-number time class for manipulating time values commonly found in media files. The unit of MediaTime is seconds.</p> <p>Media containers such as mp4 and WebM represent time values as a ratio between a \"time base\" and a \"time value\".  These values cannot necessarily be accurately represented as floating-point values without incurring cumulative rounding errors.  For example, a common frame rate in video formats is 29.97fps, however that value is an approximation of 30000/1001.  So a media file containing a video track with a 29.97fps content will declare a \"time base\" scalar of 30000, and each frame will have a \"time value\" duration of 1001.</p> <p>Media Source Extension algorithms are very sensitive to small gaps between samples, and due to its rational-number behavior, MediaTime guarantees samples are contiguous by avoiding floating-point rounding errors.</p> <p>MediaTime offers convenience methods to convert from (<code>createTimeWithDouble()</code>) and to (<code>toDouble()</code>) floating-point values.</p>"},{"location":"Deep%20Dive/Modules/MediaSourceExtensions.html#mediasample","title":"MediaSample","text":"<p>(.h)</p> <p>MediaSample is an abstract base class representing a sample parsed from a media segment. MediaSamples have <code>presentationTime()</code>, <code>decodeTime()</code>, and <code>duration()</code>, each of which are MediaTime values,  which are used to order these samples relative to one another in a SampleMap.  For codecs which support frame reordering, <code>presentationTime()</code> and <code>decodeTime()</code> for each sample may differ.</p>"},{"location":"Deep%20Dive/Modules/MediaSourceExtensions.html#samplemap","title":"SampleMap","text":"<p>(.h, .cpp)</p> <p>SampleMap is a high-performance, binary-tree, storage structure for holding MediaSamples.</p> <p>Because decoders typically require frames to be enqueued in decode-time order, but many of the Media Source Extension algorithms work in presentation-time order, SampleMap contains two binary-tree structures: a <code>decodeOrder()</code> map, and a <code>presentationOrder()</code> map.</p>"},{"location":"Deep%20Dive/Modules/Modules.html","title":"Modules","text":"<p>WebKit Modules</p>"},{"location":"Deep%20Dive/Modules/Modules.html#overview","title":"Overview","text":"<p>WebKit modules enhance the HTML, CSS, JS experience by providing new APIs that allows developers to interact with interesting features, often provided by the operating system, in a secure way.</p> <p>The source code for the Modules can be found here.</p>"},{"location":"Deep%20Dive/Modules/Modules.html#modules_1","title":"Modules","text":"Module Description airplay applepay applepay-ams-ui applicationmanifest async-clipboard beacon cache compression Compression Streams provides an async JS api to compress and decompress common data formats (gzip, deflate, and deflate-raw). contact-picker cookie-consent credentialmanagement encryptedmedia entriesapi fetch filesystemaccess gamepad Provides a low level interface to represent gamepad devices. geolocation highlight indexeddb A NoSQL database that allows long term storage of large amounts of data. mediacapabilities mediacontrols mediarecorder mediasession mediasource mediastream model-element modern-media-controls notifications paymentrequest pdfjs-extras PDFJS provides rendering PDF documents using JavaScript replacing the native PDF renderer. permissions pictureinpicture plugins push-api remoteplayback reporting speech storage streams system-preview web-locks webaudio webauthn webcodecs webdatabase webdriver websockets webxr WebGPU"},{"location":"Getting%20Started/BugTracking.html","title":"Bug Tracking","text":""},{"location":"Getting%20Started/BugTracking.html#overview","title":"Overview","text":"<p>WebKit uses Bugzilla as our primary bug tracking tool, which is hosted at bugs.webkit.org. We use Bugzilla to file bugs, and then we perform code review using GitHub.</p>"},{"location":"Getting%20Started/BugTracking.html#filing-a-bug","title":"Filing a Bug","text":"<p>To file a new WebKit bug please review the steps below.</p>"},{"location":"Getting%20Started/BugTracking.html#create-a-bugzilla-account","title":"Create a Bugzilla Account","text":"<p>You\u2019ll need to create a Bugzilla account to be able to report bugs and comment on them.</p>"},{"location":"Getting%20Started/BugTracking.html#check-your-webkit-version","title":"Check Your WebKit Version","text":"<p>Please ensure you are using latest version of WebKit before filing to verify your issue has not already been resolved. You can download the latest WebKit build from our build archives here.</p>"},{"location":"Getting%20Started/BugTracking.html#search-bugzilla","title":"Search Bugzilla","text":"<p>Please search through Bugzilla first to check if your issue has already been filed. This step is very important! If you find that someone has filed your bug already, please add your comments on the existing bug report.</p>"},{"location":"Getting%20Started/BugTracking.html#file-the-bug","title":"File the Bug!","text":"<p>If a bug does not already exist you can file a bug here. The Writing a Good Bug Report document gives some tips about the most useful information to include in bug reports. The better your bug report, the higher the chance that your bug will be addressed (and possibly fixed) quickly!</p>"},{"location":"Getting%20Started/BugTracking.html#next-steps","title":"Next Steps","text":"<p>Once your bug is filed, you\u2019ll receive email when it\u2019s updated at each stage in the bug life cycle. After the bug is considered fixed, you may be asked to download the latest WebKit Build Archive and confirm that the fix works for you.</p> <p>Note: Safari specific bugs should be reported to Apple here.</p>"},{"location":"Getting%20Started/BugTracking.html#editing-bugs","title":"Editing Bugs","text":"<p>To edit an existing bug on Bugzilla you may need editbug-bits.</p>"},{"location":"Getting%20Started/BugTracking.html#reporting-security-bugs","title":"Reporting Security Bugs","text":"<p>Security bugs have their own components in bugs.webkit.org. We\u2019re also working on a new policy to delay publishing tests for security fixes until after the fixes have been widely deployed.</p> <p>Please keep all discussions of security bugs and patches in the Security component of Bugzilla.</p>"},{"location":"Getting%20Started/ContributingCode.html","title":"Getting Started Contributing","text":"<p>WebKit has a rigorous code contribution process and policy in place to maintain the quality of code.</p>"},{"location":"Getting%20Started/ContributingCode.html#getting-setup-to-contribute","title":"Getting Setup to Contribute","text":"<p>Please run this command below to setup your environment to make pull requests.</p> <pre><code>git webkit setup\n</code></pre> <p>The <code>setup</code> sub-command of git-webkit configures your local WebKit checkout for contributing code to the WebKit project. This script will occasionally prompt the user for input. The script does the following:</p> <ul> <li>Set your name and email address for the WebKit repository</li> <li>Make Objective-C diffs easier to digest</li> <li>Setup a commit message generator</li> <li>Set an editor for commit messages</li> <li>Store a GitHub API token in your system credential store</li> <li>Configure <code>git</code> to use the GitHub API token when prompted for credentials, if using the HTTPS remote</li> <li>Create a user owned fork of the WebKit repository</li> </ul>"},{"location":"Getting%20Started/ContributingCode.html#submitting-a-pull-request","title":"Submitting a pull request","text":"<p>Firstly, please make sure you file a bug for the thing you are adding or fixing! Or, find a bug that you think is relevant to the fix you are making.</p> <p>Assuming you are working off \"main\" branch, once your patch is working and tests are passing, simply run:</p> <pre><code>git webkit pr --issue &lt;your bug number here&gt;\n</code></pre> <p>That will pull down the details from bugs.webkit.org, create a new git branch, and generate a commit message for you. If necessary, please add additional details describing what you've added, modified, or fixed.</p> <p>Once your pull request is on GitHub, the Early Warning System (a.k.a. EWS) will automatically build and run tests against your code change. This allows contributors to find build or test failures before committing code changes to the WebKit\u2019s repository.</p> <p>Note, if you'd like to submit a draft pull request, you can do so by running:</p> <pre><code>git webkit pr --draft\n</code></pre>"},{"location":"Getting%20Started/ContributingCode.html#addressing-review-feedback","title":"Addressing review feedback","text":"<p>After you receive review feedback on GitHub, you should collaborate with the reviewer to address the feedback.</p> <p>Once done, you can update your pull request to include the changes by again simply running:</p> <pre><code>git webkit pr\n</code></pre>"},{"location":"Getting%20Started/ContributingCode.html#landing-changes","title":"Landing Changes","text":""},{"location":"Getting%20Started/ContributingCode.html#merge-queues","title":"Merge Queues","text":"<p>To land a pull request, add the <code>safe-merge-queue</code>, <code>merge-queue</code>, or <code>unsafe-merge-queue</code> label to your pull request. These labels will put your pull request into Safe-Merge-Queue, Merge-Queue, and Unsafe-Merge-Queue, respectively, which will commit your pull request to the WebKit repository.</p> <p>Each queue runs a style-check and inserts reviewer information into the commit message and modified change logs. They check that a pull request has been reviewed by checking the commit message before landing the change.</p> <p>Safe-Merge-Queue checks the status of pull requests with the label every 15 minutes. Once all EWS tests pass, the pull request is automatically landed. If a test fails, the pull request is labelled with merging-blocked.</p> <p>Merge-Queue validates that a pull request builds on macOS and runs WK2 layout tests before landing the change.</p> <p>Unsafe-Merge-Queue does not perform any additional validation and should only be used for changes that have minimal impact (e.g. setting test expectations, adding to contributors.json) or changes that need to be landed quickly (e.g. build fix or revert).</p>"},{"location":"Getting%20Started/ContributingCode.html#git-webkit-land","title":"git-webkit land","text":"<p>Landing should be achieved via merge-queue, this outlines the current behavior of <code>git-webkit land</code></p> <p>To land a change, run <code>git-webkit land</code> from the branch to be landed. Note that only a committer has the privileges to commit a change to the WebKit repository. <code>git-webkit land</code> does the following:</p> <ul> <li>Check to ensure a pull-request is approved and not blocked</li> <li>Insert reviewer names into the commit message</li> <li>Rebase the pull-request against its parent branch</li> <li>Canonicalize the commits to be landed</li> <li>Update the pull-request with the landed commit</li> </ul>"},{"location":"Getting%20Started/ContributingCode.html#coding-style","title":"Coding style","text":"<p>Code you write must follow WebKit\u2019s coding style guideline. You can run <code>Tools/Scripts/check-webkit-style</code> to check whether your code follows the coding guidelines or not (it can report false positives or false negatives). If you use <code>Tools/Scripts/webkit-patch upload</code> to upload your patch, it automatically runs the style checker against the code you changed so there is no need to run <code>check-webkit-style</code> separately.</p> <p>The style checker cannot automatically fix the code style issues it finds. Supposing your patch was already commited as HEAD of your PR branch, you can re-format the code and amend your patch, as shown below:</p> <pre><code>Tools/Scripts/webkit-patch format -g HEAD\ngit commit -a --amend --no-edit\n</code></pre> <p>Then you can try <code>Tools/Scripts/webkit-patch upload</code> again.</p> <p>Some older parts of the codebase do not follow these guidelines. If you are modifying such code, it is generally best to clean it up to comply with the current guidelines.</p>"},{"location":"Getting%20Started/ContributingCode.html#convenience-tools","title":"Convenience Tools","text":"<p><code>Tools/Scripts/webkit-patch</code> provides a lot of utility functions like applying the latest patch on bugs.webkit.org (<code>apply-from-bug</code>) and uploading a patch (<code>upload --git-commit=&lt;commit hash&gt;</code>) to a bugs.webkit.org bug. Use <code>--all-commands</code> to the list of all commands this tool supports.</p>"},{"location":"Getting%20Started/ContributingCode.html#regression-tests","title":"Regression Tests","text":"<p>Once you have made a code change, you need to run the aforementioned tests (layout tests, API tests, etc...) to make sure your code change doesn\u2019t break existing functionality. These days, uploading a patch on bugs.webkit.org triggers the Early Warning System (a.k.a. EWS).</p> <p>For any bug fix or a feature addition, there should be a new test demonstrating the behavior change caused by the code change. If no such test can be written in a reasonable manner (e.g. the fix for a hard-to-reproduce race condition), then the reason writing a tests is impractical should be explained in the accompanying commit message.</p> <p>Any patch which introduces new test failures or performance regressions may be reverted. It\u2019s in your interest to wait for the Early Warning System to fully build and test your patch on all relevant platforms.</p>"},{"location":"Getting%20Started/ContributingCode.html#commit-messages","title":"Commit messages","text":"<p>Commit messages serve as change logs, providing historical documentation for all changes to the WebKit project. Running <code>git-webkit setup</code> configures your git hooks to properly generate commit messages.</p> <p>The first line shall contain a short description of the commit message (this should be the same as the Summary field in Bugzilla). On the next line, enter the Bugzilla URL.  Below the \"Reviewed by\" line, enter a detailed description of your changes.  There will be a list of files and functions modified at the bottom of the commit message. You are encouraged to add comments here as well. (See the commit below for reference). Do not worry about the \u201cReviewed by NOBODY (OOPS!)\u201d line, GitHub will update this field upon merging.</p> <pre><code>Allow downsampling when invoking Remove Background or Copy Subject\nhttps://bugs.webkit.org/show_bug.cgi?id=242048\n\nReviewed by NOBODY (OOPS!).\n\nSoft-link `vk_cgImageRemoveBackgroundWithDownsizing` from VisionKitCore, and call into it to perform\nbackground removal when performing Remove Background or Copy Subject, if available. On recent builds\nof Ventura and iOS 16, VisionKit will automatically reject hi-res (&gt; 12MP) images from running\nthrough subject analysis; for clients such as WebKit, this new SPI allows us to opt into\ndownsampling these large images, instead of failing outright.\n\n* Source/WebCore/PAL/pal/cocoa/VisionKitCoreSoftLink.h:\n* Source/WebCore/PAL/pal/cocoa/VisionKitCoreSoftLink.mm:\n* Source/WebCore/PAL/pal/spi/cocoa/VisionKitCoreSPI.h:\n* Source/WebKit/Platform/cocoa/ImageAnalysisUtilities.h:\n* Source/WebKit/Platform/cocoa/ImageAnalysisUtilities.mm:\n(WebKit::requestBackgroundRemoval):\n\nRefactor the code so that we call `vk_cgImageRemoveBackgroundWithDownsizing` if it's available, and\notherwise fall back to `vk_cgImageRemoveBackground`.\n\n* Source/WebKit/UIProcess/ios/WKContentViewInteraction.mm:\n(-[WKContentView doAfterComputingImageAnalysisResultsForBackgroundRemoval:]):\n(-[WKContentView _completeImageAnalysisRequestForContextMenu:requestIdentifier:hasTextResults:]):\n(-[WKContentView imageAnalysisGestureDidTimeOut:]):\n* Source/WebKit/UIProcess/mac/WebContextMenuProxyMac.mm:\n(WebKit::WebContextMenuProxyMac::appendMarkupItemToControlledImageMenuIfNeeded):\n(WebKit::WebContextMenuProxyMac::getContextMenuFromItems):\n\nAdditionally, remove the `cropRect` completion handler argument, since the new SPI function no\nlonger provides this information. The `cropRect` argument was also unused after removing support for\nrevealing the subject, in `249582@main`.\n</code></pre> <p>The \u201cNo new tests. (OOPS!)\u201d line will appear if <code>git webkit commit</code> did not detect the addition of new tests. If your patch does not require test cases (or test cases are not possible), remove this line and explain why you didn\u2019t write tests. Otherwise all changes require test cases which should be mentioned in the commit message.</p>"},{"location":"Getting%20Started/Introduction.html","title":"Introduction to WebKit","text":"<p>WebKit is an open-source Web browser engine.  It\u2019s a framework in macOS and iOS, and used by many first party and third party applications including Safari, Mail, Notes, Books, News, and App Store.</p>"},{"location":"Getting%20Started/Introduction.html#what-is-webkit","title":"What is WebKit?","text":"<p>The WebKit codebase is mostly written in C++ with bits of C and assembly, primarily in JavaScriptCore, and some Objective-C to integrate with Cocoa platforms.</p> <p>It primarily consists of the following components, each inside its own directory in Source:</p> <ul> <li>bmalloc - WebKit\u2019s malloc implementation as a bump pointer allocator. It provides an important security feature, called IsoHeap,     which segregates each type of object into its own page to prevent type confusion attacks upon use-after-free.</li> <li>WTF - Stands for Web Template Framework. WebKit\u2019s template library.     The rest of the WebKit codebase is built using this template library in addition to, and often in place of, similar class templates in the C++ standard library.     It contains common container classes such as Vector, HashMap (unordered), HashSet, and smart pointer types such as Ref, RefPtr, and WeakPtr used throughout the rest of WebKit.</li> <li>JavaScriptCore - WebKit\u2019s JavaScript engine; often abbreviated as JSC.     JSC parses JavaScript and generates byte code, which is then executed by one of the following four tiers.     Many tiers are needed to balance between compilation time and execution time.     Also see Phil's blog post about Speculation in JavaScriptCore.<ul> <li>Interpreter - This tier reads and executes instructions in byte code in C++.</li> <li>Baseline JIT - The first Just In Time compiler tier serves as the profiler as well as a significant speed up from the interpreter.</li> <li>DFG JIT - Data Flow Graph Just In Time compiler uses the data flow analysis to generate optimized machine code.</li> <li>FTL JIT - Faster than Light Just In Time compiler which uses B3 backend.     It\u2019s the fastest tier of JSC. JavaScriptCode also implements JavaScriptCore API for macOS and iOS applications.</li> </ul> </li> <li>WebCore - The largest component of WebKit, this layer implements most of the Web APIs and their behaviors.     Most importantly, this component implements HTML, XML, and CSS parsers and implements HTML, SVG, and MathML elements as well as CSS.     It also implements CSS JIT, the only Just In Time compiler for CSS in existence.     It works with a few tree data structures:<ul> <li>Document Object Model - This is the tree data structure we create from parsing HTML.</li> <li>Render Tree - This tree represents the visual representation of each element in DOM tree computed from CSS and also stores the geometric layout information of each element.</li> </ul> </li> <li>WebCore/PAL and WebCore/platform - Whilst technically a part of WebCore, this is a platform abstraction layer for WebCore     so that the rest of WebCore code can remain platform independent / agnostic across all the platforms WebKit can run on: macOS, iOS, Windows, Linux, etc...     Historically, most of this code resided in WebCore/platform.     There is an ongoing multi-year project to slowly migrate code to PAL as we remove the reverse dependencies to WebCore.</li> <li>WebKitLegacy (a.k.a. WebKit1) - This layer interfaces WebCore with the rest of operating systems in single process and implements WebView on macOS and UIWebView on iOS.</li> <li>WebKit (a.k.a. WebKit2) - This layer implements the multi-process architecture of WebKit, and implements WKWebView on macOS and iOS.     WebKit\u2019s multi-process architecture consists of the following processes:<ul> <li>UI process - This is the application process. e.g. Safari and Mail</li> <li>WebContent process - This process loads &amp; runs code loaded from websites.     Each tab in Safari typically has its own WebContent process.     This is important to keep each tab responsive and protect websites from one another.</li> <li>Networking process - This process is responsible for handling network requests as well as storage management.     All WebContent processes in a single session (default vs. private browsing) share a single networking session in the networking process.</li> </ul> </li> <li>WebInspector / WebDriver - WebKit\u2019s developer tool &amp; automation tool for Web developers.</li> </ul>"},{"location":"Infrastructure/MemoryInspection.html","title":"Memory Inspection","text":"<p>Tracking WebKit's memory usage is important to ensure we do not use excessive resources. The operating system (in combination with WebKit tools) provides numerous ways to inspect WebKit to discover where our memory is being allocated.</p>"},{"location":"Infrastructure/MemoryInspection.html#build-settings","title":"Build Settings","text":""},{"location":"Infrastructure/MemoryInspection.html#malloc-heap-breakdown","title":"Malloc Heap Breakdown","text":"<p>Malloc Heap Breakdown allows for fine-grained analysis of memory allocated per class. Classes marked with <code>WTF_MAKE_FAST_ALLOCATED_WITH_HEAP_IDENTIFIER(ClassName);</code> will be individually marked when using tools like <code>footprint</code>.</p> <p>To enable this build setting you need to flip two flags. One in <code>PlatformEnable.h</code> and the second in <code>BPlatform.h</code>.</p> <pre><code>/* Source/WTF/wtf/PlatformEnable.h */\n\n/*\n * Enable this to put each IsoHeap and other allocation categories into their own malloc heaps, so that tools like vmmap can show how big each heap is.\n * Turn BENABLE_MALLOC_HEAP_BREAKDOWN on in bmalloc together when using this.\n */\n#if !defined(ENABLE_MALLOC_HEAP_BREAKDOWN)\n#define ENABLE_MALLOC_HEAP_BREAKDOWN 0\n#endif\n</code></pre> <pre><code>/* Source/bmalloc/bmalloc/BPlatform.h */\n\n/* Enable this to put each IsoHeap and other allocation categories into their own malloc heaps, so that tools like vmmap can show how big each heap is. */\n#define BENABLE_MALLOC_HEAP_BREAKDOWN 0\n</code></pre>"},{"location":"Infrastructure/MemoryInspection.html#commands","title":"Commands","text":""},{"location":"Infrastructure/MemoryInspection.html#footprint","title":"Footprint","text":"<p>Footprint is a macOS specific tool that allows the developer to check memory usage across regions.</p> <pre><code>&gt; footprint WebKit\nFound process com.apple.WebKit.WebContent [27416] from partial name WebKit\n======================================================================\ncom.apple.WebKit.WebContent [27416]: 64-bit    Footprint: 142 MB (16384 bytes per page)\n======================================================================\n\n  Dirty      Clean  Reclaimable    Regions    Category\n    ---        ---          ---        ---    ---\n 108 MB        0 B        23 MB         11    WebKit malloc\n9664 KB        0 B          0 B         24    MALLOC_TINY\n6384 KB        0 B        16 KB          8    MALLOC_SMALL\n3904 KB        0 B          0 B        768    JS VM Gigacage\n...\n    ---        ---          ---        ---    ---\n 142 MB      21 MB        23 MB       7001    TOTAL\n\nAuxiliary data:\n    dirty: N\n    phys_footprint_peak: 424 MB\n    phys_footprint: 142 MB\n</code></pre>"},{"location":"Infrastructure/MemoryInspection.html#results","title":"Results","text":"<p>Refer to <code>man footprint</code> for a full guide on this tool.</p>"},{"location":"Infrastructure/MemoryInspection.html#dirty","title":"Dirty","text":"<p>Memory that is written to by the process. Includes Swapped, non-volatile, and implicitly written memory.</p>"},{"location":"Infrastructure/MemoryInspection.html#clean","title":"Clean","text":"<p>Memory which is neither dirty nor reclaimable.</p>"},{"location":"Infrastructure/MemoryInspection.html#reclaimable","title":"Reclaimable","text":"<p>Memory marked as available for reuse.</p>"},{"location":"Infrastructure/MemoryInspection.html#regions","title":"Regions","text":"<p>Number of VM Regions that contribute to this row.</p>"},{"location":"Infrastructure/MemoryInspection.html#category","title":"Category","text":"<p>Descriptive name for this entry.</p>"},{"location":"Infrastructure/UpdatingAngle.html","title":"Update ANGLE","text":""},{"location":"Infrastructure/UpdatingAngle.html#fixing-angle-bugs","title":"Fixing ANGLE Bugs","text":"<p>Before you commit a WebKit patch that modifies ANGLE, please run <code>Tools/Scripts/update-angle --regenerate-changes</code>. This will update <code>Source/ThirdParty/ANGLE/changes.diff</code> so that people can see the diff from upstream at a glance.</p> <p>When fixing bugs in ANGLE, please create a new bug on ANGLE's bug tracker and attach the patch applied to WebKit so that changes can eventually be merged upstream instead of maintained locally.</p>"},{"location":"Infrastructure/UpdatingAngle.html#merging-angle-from-upstream","title":"Merging ANGLE from Upstream","text":"<p>To pull in a new revision of ANGLE, run the script <code>Tools/Scripts/update-angle</code> and follow its instructions.  This script will attempt to update to a new version of ANGLE without losing WebKit's local changes by performing a git rebase.  It also helps to update the CMake build files and <code>ANGLE.plist</code>.</p>"},{"location":"Infrastructure/WPTTests.html","title":"Web Platform Tests Integration","text":"<p>WebKit maintains a separate fork of the Web Platform Tests living in <code>\u200cLayoutTests/imported/w3c/web-platform-tests</code>. When changes are made upstream we need to import them to stay up to date.</p>"},{"location":"Infrastructure/WPTTests.html#importing-tests-from-wpt","title":"Importing Tests from WPT","text":"<pre><code>Tools/Scripts/import-w3c-tests -t web-platform-tests/folder_to_import_here\n</code></pre> <p>When running the script above the latest WPT will be downloaded into WebKitBuild directory. The requested files will be copied over into the WebKit WPT directory.</p> <p>After importing the tests ensure to run <code>run-webkit-tests</code> to generate new expectations. You may need to update the <code>LayoutTests/TestExpectations</code> which need to be marked <code>SKIP</code> based on <code>import-w3c-tests</code> output.</p>"},{"location":"Infrastructure/WPTTests.html#import-wpt-tests-from-a-local-checkout-of-wpt","title":"Import WPT Tests from a local checkout of WPT","text":"<p>If you have the upstream WPT repository locally you can skip redownloading it by running the following below.</p> <pre><code>Tools/Scripts/import-w3c-tests web-platorm-tests/folder_to_import_here -l -s path_to_web_platform_tests\n</code></pre>"},{"location":"Other/Contributors.html","title":"Contributors","text":""},{"location":"Other/Contributors.html#overview","title":"Overview","text":"<p>There are three different kinds of contributors in the WebKit project.</p> <ul> <li>Contributor - This category encompasses everyone. Anyone who files a bug or contributes a code change or reviews a code change is considered as a contributor</li> <li>Committer - A committer is someone who has write access to WebKit's repository.</li> <li>Reviewer - A reviewer is someone who has the right to review and approve code changes other contributors proposed.</li> </ul> <p>See Commit and Review Policy for more details on how to become a committer or a reviewer.</p>"},{"location":"Other/Licensing.html","title":"Licensing","text":""},{"location":"Other/Licensing.html#overview","title":"Overview","text":"<p>Much of the code we inherited from KHTML is licensed under LGPL. New code contributed to WebKit will use the two clause BSD license. When contributing new code, update the copyright date. When moving the existing code, you need to include the original copyright notice for the moved code and you should also not change the license, which may be BSD or LGPL depending on a file, without the permission of the copyright holders.</p>"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2010.html","title":"WebKit Contributor Meeting 2010","text":"<p>WebKit Contributors Meeting April 12-13, 2010</p>"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2010.html#schedule","title":"Schedule","text":""},{"location":"Other/Contributor%20Meetings/ContributorMeeting2010.html#monday","title":"Monday","text":"Time Talk 9-9:30 breakfast 9:30-10:00 intro session 10:15-11:15 sessions 1 (3 or 4 tracks) 11:30-12:30 sessions 2 (3 or 4 tracks) 12:30-2:00 lunch 2:00-3:00 sessions 3 (3 or 4 tracks) 3:15-5:00 hackfest 1 (3 or 4 tracks)"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2010.html#tuesday","title":"Tuesday","text":"Time Talk 9:00-10:00 breakfast 10:00-11:00 session 1 (3 or 4 tracks) 11:15-12:15 session 2 (3 or 4 tracks) 12:15-1:45 lunch 1:45-2:45 session 3 (3 or 4 tracks) 3:00-5:00 hackfest 2 (3 or 4 tracks)"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2010.html#proposed-talks-discussions","title":"Proposed Talks / Discussions","text":"Talks Host Would Attend Mobile Features Talk Using webkit-patch for fun and profit eseidel, abarth I didn't know the Web Inspector could do that! JoePeck How the commit-queue saved my life, and other stories. eseidel maruel WebKit2, Electric Boogaloo! weinig, andersca othermaciej, abarth, dimich, xan, fishd The messiah cometh: how new-run-webkit-tests will save us all! ojan, eseidel, abarth othermaciej, xan Do we need a better code review tool? maruel abarth, eseidel, kbr, jamesr,dave_levin JavaScript engine performance optimization. ggaren? othermaciej, xan Loader code: can we make it hurt our eyes less? othermaciej, abarth? dave_levin, xan, fishd gyp - maybe we can get more ports to use it eseidel, othermaciej, abarth, maruel,dave_levin, xan, tronical HTML5 new elements and input types othermaciej, tkent, morrita, fishd, bergkvist CSS Animations, Transitions &amp; Transforms: how they work smfr othermaciej, jamesr, icefox, bergkvist What's new with WebGL? kbr?, cmarrin? Hardware accelerated drawing othermaciej, jamesr, xan, tronical, fishd, smfr The new V8 autogen testing and how we can use it for other ports! yaar?, natechapin? eseidel, abarth Features for mobile devices - should we get them into standards? - viewport meta, touch events, etc manyoso othermaciej, bergkvist Device element / Stream API / device access bergkvist The future of porting macros othermaciej eseidel, dimich, icefox, xan, fishd Improving the editing code enrica? Licensing, license compliance, dealing with license violations george s Git icefox simon, bergkvist Project-wide planning for new and experimental features othermaciej dave_levin, fishd, bergkvist, tronical,dimich How you can help to get the review queue under control. abarth? dave_levin WebKit releasing: branching/maintaining webkit releases tronical Documentation: coordinating documentation for web developers, including webkit extensions tronical Bug tracking: best practices ? tronical Remote Web Inspector pmueller Can we have fewer platform-specific tests, maybe with reftests? manyoso Automating performance tests enrica Memory use jasonr Media elements ecarlson"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2010.html#proposed-hackathons","title":"Proposed Hackathons","text":"Talks Host Would Attend Fix the issues that stop you from using webkit-patch eseidel, abarth, cjerdonek Unforking our URL code othermaciej abarth, fishd, dave_levin Adding new-run-webkit-tests support for your platform. eseidel abarth Get the Windows Bots green! aroben?, bweinstein? eseidel, abarth Get the Qt Bot green! Ossy? WebKit2 for your platform weinig, andersca icefox, tronical Cutting down Skipped lists darin Reduce number of unanswered bugs Rik Fix leaks on leaks bot eseidel?"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2010.html#attendees","title":"Attendees","text":"<p>(alphabetical by last name)</p> <ul> <li>Adam Barth (abarth)</li> <li>Daniel Bates (dydx)</li> <li>Adam Bergkvist</li> <li>Alejandro Garcia Castro</li> <li>Kenneth Rohde Christiansen</li> <li>Darin Fisher (fishd)</li> <li>Yuzo Fujishima (yuzo)</li> <li>Laszlo Gombos</li> <li>Kim Gr\u00f6nholm</li> <li>Simon Hausmann (tronical)</li> <li>Ariya Hidayat (ariya)</li> <li>Chris Jerdonek (cjerdonek)</li> <li>David Kilzer (ddkilzer)</li> <li>Yuta Kitamura (yutak)</li> <li>Geoff Levand</li> <li>David Levin (dave_levin)</li> <li>Xan Lopez</li> <li>Evan Martin (evmar)</li> <li>Benjamin C. Meyer (icefox)</li> <li>Hajime Morita (morrita)</li> <li>Philippe Normand</li> <li>Julie Parent (jparent)</li> <li>Jakob Petsovits</li> <li>Anthony Ricaud</li> <li>James Robinson (jamesr)</li> <li>Marc-Antoine Ruel (maruel)</li> <li>Jason Rukman</li> <li>Kenneth Russell (kbr)</li> <li>Eric Seidel (eseidel)</li> <li>Chang Shu</li> <li>Kent Tamura (tkent)</li> <li>Dmitry Titov (dimich)</li> <li>Adam Treat</li> <li>Eric Uhrhane</li> <li>Fumitoshi Ukai</li> <li>Tor Arne Vestb\u00f8</li> <li>Ojan Vafai (ojan)</li> <li>Sam Weinig (weinig)</li> </ul>"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2011.html","title":"WebKit Contributor Meeting 2011","text":"<p>WebKit Contributors Meeting April 25-26, 2011</p>"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2011.html#overview","title":"Overview","text":"<p>Group photo from meeting.</p>"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2011.html#proposed-talks-discussions","title":"Proposed Talks / Discussions","text":"Talk Host Would Attend Importance Getting compile time under control weinig levin, mjs, dpranke, ojan, sjl, kling, torarne, estes, alexg, dimich, eseidel, aroben, lgombos, more... Super-High Unifying the build system abarth ddkilzer, dpranke, ojan, torarne, estes, eseidel, demarchi, jturcotte, aroben, lgombos High WebKit2 - One year later weinig kling, mms, enrica, yael, noamr, torarne, estes, alexg, demarchi, jturcotte, philn, aroben, lgombos, kenneth, joone Medium-High Common thread patterns in WebKit levin mjs, dimich, aroben, lgombos Medium-High Removing or rejecting features - is there a way for the WebKit project to say no to things? mjs weinig, abarth, levin, morrita, dpranke, ojan, sjl, noamr, kling, torarne, dimich, lgombos Medium-High Advanced tool usage (webkit-patch, commit-queue, ews-bots, sheriff-bot, re-baseline tool, new-run-webkit-test, reviewtool, etc.) eseidel mjs, dpranke, ojan, sjl, yael, estes, alexg, jparent, philn, aroben, inferno-sec, lgombos, hayato Medium Redesign of Position-related classes rniwa leviw, enrica, eae Medium Add a way to RenderObject destruction aka add RenderObject guard rniwa Shadow DOM and the component model dglazkov and friends rniwa, leviw, mjs, enrica, dbates, dpranke, sjl, yael, jparent, eseidel, inferno-sec, kenneth Medium Hardware acceleration roundup, what do we share? what can we share? noamr sjl, torarne, alexg, jturcotte, aroben, lgombos, smfr, joone Medium Getting layout test times under control, getting new-run-webkit-tests working for everyone? geoffrey garen/dpranke Medium \u200bStrategies for decreasing the number/frequency/duration of test failures aroben lgombos, eseidel, levin Medium HTML5 parser eseidel, abarth mjs, estes, demarchi, inferno-sec Medium Sharing LayoutTestController Code? morrita Medium-Low Understanding line-layout bidi, line-box tree, etc. eseidel Media elements using the WebKit loader scherkus Medium-Low Reducing checkout/update times, reducing layout test result churn Medium-Low Switching layout offsets to floats from ints leviw dglazkov, ojan, eseidel, inferno-sec, eae Low Improving the verbosity of the editing markup enrica rniwa Low MathML update alex milowski Low Media element pseudo classes eric Low New CSS positioning modes, eg. flex, grid Low Coming together on threading patterns for speed optimizations ap? Low Updates on the grand loader fix, including plugins Low Sharing more code between WebKit1 and WebKit2 Low Overview of adding a new Element subclass Two LayoutTestAnalyzer, what is it? Low Gardening and keeping the bots green Low Adding multi-threaded code to WebKit, strategies jchaffraix Low Advanced text layout (vertical text, ruby, etc) mjs, eseidel XML Processor Profiles (W3C LC Draft and WebKit ) alex milowski Single Getting more ports to enable pixel tests on bots, and making them less brittle Silence"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2011.html#proposed-hackathons","title":"Proposed Hackathons","text":"Talk Host Would Attend Importance EventHandler cleanup rniwa dglazkov, weinig, ojan, sjl, yael, demarchi, kenneth Medium-Low Fuzz-a-thon - run fuzzers, find bugs, fix them mjs, inferno-sec Medium-Low Converting more rendertree/manual tests to dumpAsText/dumpAsMarkup Medium-Low Review-a-thon! Get as many patches out of webkit.org/pending-review as possible eseidel mjs, abarth, levin, ojan, weinig, kling, aroben Medium-Low Flip on strict mode for smart pointers Medium-Low Component model API brainstorming dglazkov weinig, lgombos, kenneth Low Hacking webkitpy/bugzilla for fun and profit. Tour all the tool code? Write our own sheriff-bot command? eseidel levin, ojan, jparent, abarth, philn, aroben Low KURL unforking revisited mjs abarth, weinig Low Finish bust'n up the Frame class cluster, and other big classes Low Splitting JSC into its own library for GTK Low Moving another port to GYP Low Splitting WTF out of JavaScriptCore Low Hacking check-webkit-style so you never have to flag the issue in a review again. levin Double TextInputTestSuite\u2014improving the coverage of editing in input type=text, search, etc. and textarea rniwa, enrica, xji, yael, morrita, dglazkov"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2011.html#attendees","title":"Attendees","text":"<ul> <li>adambe</li> <li>adamk</li> <li>adele</li> <li>alexg</li> <li>alexmilowski</li> <li>amruthraj</li> <li>antonm</li> <li>aroben</li> <li>benjaminp</li> <li>bdath</li> <li>bweinstein</li> <li>caseq</li> <li>cmumford</li> <li>cshu</li> <li>darin</li> <li>darktears</li> <li>dave_levin</li> <li>ddkilzer</li> <li>demarchi</li> <li>dethbakin</li> <li>dglazkov</li> <li>dgrogan</li> <li>dimich</li> <li>dominicc</li> <li>dpranke</li> <li>enne</li> <li>enrica</li> <li>eric_carlson</li> <li>ericu</li> <li>eseidel</li> <li>estes</li> <li>fishd</li> <li>geoff-</li> <li>ggaren</li> <li>gyuyoung</li> <li>hayato</li> <li>honten</li> <li>inferno-sec</li> <li>jamesr</li> <li>japhet</li> <li>jeffm7</li> <li>jennb</li> <li>jhoneycutt</li> <li>jianli</li> <li>jonlee</li> <li>joone</li> <li>jparent</li> <li>jschuh</li> <li>jturcott</li> <li>kbr_google</li> <li>keishi</li> <li>kenne, kenneth</li> <li>kinuko</li> <li>kling</li> <li>krit</li> <li>lca</li> <li>leviw</li> <li>lgombos</li> <li>loislo</li> <li>makulkar, maheshk</li> <li>mihaip</li> <li>mitzpettel</li> <li>morrita</li> <li>mrobinson</li> <li>msaboff</li> <li>msanchez</li> <li>noamr</li> <li>othermaciej</li> <li>ojan</li> <li>pererik</li> <li>pewtermoose (mlilek)</li> <li>pnormand</li> <li>prasadt</li> <li>psolanki</li> <li>rafaelw</li> <li>rniwa</li> <li>rolandsteiner</li> <li>smfr</li> <li>svillar</li> <li>thakis</li> <li>tkent</li> <li>tonikitoo,agomes,antonio</li> <li>tony^work</li> <li>torarne</li> <li>toyoshim</li> <li>tronical</li> <li>weinig</li> <li>xan</li> <li>yael</li> <li>yutak</li> <li>yuzo</li> </ul>"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2012.html","title":"WebKit Contributor Meeting 2012","text":"<p>WebKit Contributors Meeting April 19-20, 2012</p>"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2012.html#discussion-notes","title":"Discussion Notes","text":"<ul> <li>Accelerated rendering and compositing</li> <li>Instrumentation and Metrics</li> <li>WebComponentsStatusMeetingNotes</li> <li>April 2012 MeetingRethinkingRendering</li> <li>April 2012 Canvas Canvass</li> <li>Importing Thirdparty tests</li> <li>Deprecating features and vendor prefixes</li> <li>April 2012 HTML5 Media Element &amp; WebAudio</li> <li>April 2012 Reducing build systems</li> <li>Scrolling Session Meeting 2012</li> <li>April 2012 JavaScriptCore Roundup</li> <li>\u200b2-minute overview of everything</li> <li>April 2012 Write Your Own Render Object</li> <li>April 2012 Keeping the bots green</li> <li>\u200bWhat's new in Graphics</li> <li>\u200bPerf-o-Matic and Performance Tests</li> <li>\u200bMulticore Discussion</li> </ul>"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2012.html#proposed-talks-discussions","title":"Proposed Talks / Discussions","text":"Talk Host Would Attend Writing your own RenderObject eric dbarton, rniwa, leviw, eae, jchaffraix, torarne, dbates 2-Minute Overview of Each Component Perf-o-matic and performance tests rniwa tomz, dtharp, jacobg, enne, kov, kseo What type of benchmarks do we need in WebKit? rniwa morrita, kseo Instrumentation and metrics collection morrita, jchaffraix, zherczeg, tomz, eae, kov, kseo Reducing our number of \"supported\" build systems from 7 to fewer-than-7 eric torarne Introduction to new CSS layout modes maybe ojan/tony^work/TabAtkins/mihnea ? rniwa, hober, astearns, nov, dbates Better documenting status of new features rniwa, hober Rethinking the rendering architecture: RenderLayer, RenderObject hierarchy, ... jchaffraix eae, enne, kov, dbarton, huangdongsung Revisiting the convention and the process to import third-party (e.g. W3C) tests jacobg/astearns rniwa, tomz, dtharp, dbarton, hober Indexed Database discussion - interest from other ports?, overview of implementation, roadmap jsbell Canvas accessibility - how to implement latest whatwg additions like focusable fallback content, hit testing, ... dmazzoni hober Ninja build system: \u200bninja is a new fast build system that builds the chromium/mac port 6x as fast as xcodebuild and the chromium/linux port 2x as fast as make. There are ninja backends for cmake and gyp. If there's interest, I can give a short overview/demo of ninja thakis Web Components: progress update dglazkov dominicc Explain accelerated compositing smfr"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2012.html#proposed-hackathons","title":"Proposed Hackathons","text":"Talk Host Would Attend Add performance tests rniwa kseo Stabilize performance tests rniwa kseo Import/sync with w3c test suites jacobg rniwa, astearns Reviewathon 2012! eric? rniwa, jchaffraix, kov Move methods from layoutTestController and friends to internals morrita/dglazkov? rniwa, enne Add features to / fix bugs with garden-o-matic, flakiness dashboard, run-webkit-tests, etc. ojan Work through shadow DOM design issues dglazkov dominicc \u200bConvert Pixel tests to ref or text-only tests"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2013.html","title":"WebKit Contributor Meeting 2013","text":"<p>WebKit Contributors Meeting May 2-3, 2013</p>"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2013.html#talks-discussions","title":"Talks / Discussions","text":"Talk Host Would Attend Unprefixing frenzy - state of the art, sharing the experience on some of the work done, list and create bugs for features we should unprefix Simon Fraser Importing W3C Tests rhauck stearns State and future of the Web Inspector &amp; demo of deterministic record/replay xenon/brrian JoePeck, vivekg, brrian Restructuring of CSS, cleanup of StyleResolver/StyleBuilder krit, rniwa Pagination with Hyatt! weinig Status of JSC / Optimizations Pizlo Tools, Infrastructure, and Process simon fraser/dirk OpenType martin Build systems - why do we need so freaking many mark salsbury Improving the DOM now that we only have 1 JS engine / the removal of ScriptState! rniwa Enabling Experimental Features beartravis Managing the differences between ports noam JSC Profiler pizlo Merging iOS WebKit back to WebKit trunk ggaren, ddklizer WebKit2 Governance / Come for the Yelling weinig How to security now!? weinig Regions and Shapes mihnea Touch interaction and fat fingers gmak Remove all the things / Process for removing weinig How to debug the JIT in JSC What is our strategy for compatibility with more others / when to adopt / when to say no! Improving performance of Text Layout What are the layers of WebKit and why"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2013.html#hackathons","title":"Hackathons","text":"Talk Host Would Attend Removing/Adding Layering Violations Simplifying &amp; refactoring the binding generators jmason rniwa"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2013.html#transcripts","title":"Transcripts","text":"<ul> <li>Managing the differences between the ports</li> <li>How to debug the JIT in JSC</li> <li>Status of JSC / Optimizations</li> <li>W3C/WebKit Test Integration</li> <li>Restructuring of CSS</li> <li>Tools and Infrastructure Process</li> <li>Web Inspector &amp; Timelapse</li> <li>Merging iOS WebKit</li> <li>Unifying Build Systems</li> <li>CSS Regions, Exclusions &amp; Shapes</li> <li>Experimental Features, Prefixes, and Deleting All of the Things</li> <li>Binding generators</li> <li>Profiling in JavaScriptCore</li> </ul>"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2014.html","title":"WebKit Contributor Meeting 2014","text":"<p>WebKit Contributors Meeting April 15-16, 2014</p>"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2014.html#talksdiscussions","title":"Talks/Discussions","text":"Talk Host Would Attend What can be removed? Are there crufty ENABLE or USE flags that no remaining port uses? Can we turn some on by default? CSS Regions - Past, Present and Future abucur Zoltan What are all the smart pointers for? Ref, RenderPtr etc? kling? bemjb, spenap What are all the iterators for? descendantsOfType&lt;&gt; etc? anttik? kling, spenap CSS Custom Properties astearns rniwa, bemjb CSS Grid Layout - Current status and roadmap. lajava bemjb, betravis Subpixel layout. How it works, how to write subpixel-friendly code. zalan? rniwa, bemjb Security - Address Sanitizer, Fuzzing, etc. ddkilzer? rniwa, bemjb, Zoltan, spenap CSS Shapes - status and future bemjb, Zoltan) Talk about floats and continuations. bemjb Zoltan Web Inspector - status and future direction. xenon? bemjb, betrays, Zoltan Performance Tests and Dashboard - rniwa Zoltan, spenap What is UI-side compositing? smfr/thorton Web Components rniwa spenap Performance Optimizations anttik? Improving Selection Painting Code enrica?/hyatt? rniwa iOS Simulator WebKit Nightly - is it possible to create? ddkilzer Subtree style / layout invalidation hyatt? weinig What in the world is bmalloc? ggaren? weinig Dramatic reading of webkit-help emails andersca, dino"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2014.html#hackathons","title":"Hackathons","text":"Talk Host Would Attend Finally get rid of DeprecatedStyleBuilder krit? review? Import more W3C tests rniwa bemjb Add funny webkitbot/WKR commands. Get rid of various WebCore export generators. Fix issues found with Undefined Behavior Sanitizer (UBSan). ddkilzer Properly track and cleanup overhanging and intruding floats. bemjb Fix All The Security Bugs\u2122. ddkilzer bemjb"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2014.html#transcripts","title":"Transcripts","text":"<ul> <li>Making the WebKit Project more awesome</li> <li>Making the build faster</li> <li>CSS Custom Properties / Variables</li> <li>WebGL</li> <li>WebComponents</li> <li>Security - ASan, Fuzzing, etc.</li> <li>What are all the smart pointers and iterators for?</li> <li>Subpixel layout</li> </ul>"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2015.html","title":"WebKit Contributor Meeting 2015","text":"<p>WebKit Contributors Meeting November 11-12, 2015</p>"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2015.html#transcripts","title":"Transcripts","text":"<ul> <li>Encouraging more open source contributions</li> <li>Display lists</li> <li>Next Steps for Color</li> <li>webkit.org discussion</li> <li>Color discussion</li> <li>CMake discussion</li> <li>Network Cleanup discussion</li> <li>b3 discussion</li> <li>Web Inspector discussion</li> <li>Encouraging open source contribution \u200bNote</li> <li>Late 2015 5-year plan</li> <li>WebKitGTK+ Security Issues</li> <li>WebComponents discussion</li> </ul>"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2016.html","title":"WebKit Contributor Meeting 2016","text":"<p>WebKit Contributors Meeting October 26, 2016</p>"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2016.html#schedule","title":"Schedule","text":"Time Talk Presenter 8 AM Sign-in and Breakfast 9 AM Welcome 9:10 AM The State of JSC JSC Team 10 AM State of Web Inspector Developer Experience Team 11 AM Web Components Ryosuke Niwa 12 PM Lunch 1 PM Sony WebKit Work The Sony Team 2 PM WebGL New Extension Proposal Byungseon Shin (LG) 3 PM Break 3:30 PM Lightning Talks 4:30 PM WebKit Security Guidelines Brent Fulgham 6-9 PM Party"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2016.html#lightning-talks","title":"Lightning Talks","text":"<ul> <li>Input events (whsieh)</li> <li>A tour of webkit.org website stats (JonDavis)</li> <li>Direct2D WebKit (bfulgham)</li> <li>URL Parser (alexchristensen)</li> <li>Font Variations (litherium)</li> </ul>"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2016.html#transcripts","title":"Transcripts","text":"<ul> <li>The State of JSC</li> <li>The State of Web Inspector</li> <li>Web Components (\u200bslides)</li> <li>Sony WebKit Work</li> <li>WebGL New Extension Proposal</li> <li>DOM Input Events</li> <li>Direct2D WebKit</li> <li>URLs, CMake, and Networking</li> <li>Variable Fonts</li> <li>Security Issues in WebKit</li> </ul>"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2017.html","title":"WebKit Contributor Meeting 2017","text":"<p>WebKit Contributors Meeting October 13, 2017</p>"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2017.html#schedule","title":"Schedule","text":"Time Talk Presenter 8 AM Sign-in and Breakfast 9 AM Welcome 9:10 AM Optimizing zlib for ARM \u200bslides Adenilson Cavalcanti 10 AM JavaScriptCore Overview Saam Barati 11 AM WebKit Goals for 2018 Maciej Stachowiak 12 PM Lunch 1 PM Next-Gen Layout &amp; Display List Simon Fraser &amp; Zalan Bujtas 2 PM Secure WebCore Code Jiewen Tan &amp; Ryosuke Niwa 3 PM Break 3:30 PM Modern Clipboard API &amp; Pasteboard Refactoring Ryosuke Niwa &amp; Wenson Hsieh 4 PM-5:30 PM Lightning Talks 6-9 PM Party"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2017.html#lightning-talks","title":"Lightning Talks","text":"<ul> <li>Clang Tooling &amp; Playstation Port Update, Don Olmstead</li> <li>NetFront Browser NX WebKit port experience, Koji Egashira</li> <li>Why / where is Google contributing to WebKit, Ali Juma</li> <li>PAL, Don Olmstead &amp; Myles Maxfield</li> <li>Async Image Decoding Said</li> <li>Web standards at Akamai, Yoav Weiss</li> </ul>"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2017.html#transcripts","title":"Transcripts","text":"<ul> <li>Optimizing zlib for ARM</li> <li>WebKit Goals for 2018</li> <li>Next Generation Layout and Rendering</li> </ul>"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2018.html","title":"WebKit Contributor Meeting 2018","text":"<p>WebKit Contributors Meeting October 12, 2018</p>"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2018.html#schedule","title":"Schedule","text":"Time Talk Presenter 8 AM Sign-in and Breakfast 9 AM Welcome 9:10 AM WebKit Goals for 2019 ggaren 10 AM How to Investigate Leaks and Memory Bloat smfr 11 AM Static Accessibility Trees cfleizach 12 PM Lunch 1 PM Web Inspector and WebDriver JoePeck &amp; bburg 1:45 PM How Text Works myles 2 PM RegExp in JSC msaboff 2:30 PM State of JSC sbarati 3 PM Break 3:30 PM Igalia's Contributions to WebKit fwang 4 PM-5:30 PM Lightning Talks 6-9 PM Party"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2018.html#lightning-talks","title":"Lightning Talks","text":"<ul> <li>EWS updates (dean)</li> <li>WPT and webkit tests (youenn)</li> <li>Last 2 Years of Platform/Network/cURL (basuke)</li> <li>Fixing WebKit Development Annoyances (smfr)</li> <li>Redesigned Form Controls and Dark Mode CSS (xenon)</li> <li>IntersectionObserver (alijuma)</li> </ul>"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2018.html#transcripts","title":"Transcripts","text":"<ul> <li>WebKit Feature Focus 2018-2019</li> <li>Investigating Leaks and Bloat</li> <li>WebKit Accessibility Performance</li> <li>WebDriver Notes 2018</li> <li>Web Inspector Notes 2018</li> <li>JavaScript RegExp Processing and JavaScriptCore Goals</li> <li>Igalia Contributions to WebKit</li> <li>WebKitGTK+ and WPE WebKit in Five Minutes</li> <li>EWS Updates</li> <li>WPT and WebKit Tests</li> <li>Last 2 Years of Platform/Network/cURL</li> <li>Fixing WebKit Development Annoyances</li> <li>Redesigned Form Controls and Dark Mode CSS</li> <li>IntersectionObserver Notes</li> </ul>"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2019.html","title":"WebKit Contributor Meeting 2019","text":"<p>WebKit Contributors Meeting November 1, 2019</p>"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2019.html#schedule","title":"Schedule","text":"Time Talk 8 AM Sign-in and Breakfast 9 AM Welcome 9:10 AM Igalia Focus and Goals 2020 10 AM WebKit Goals for 2020 11 AM PlayStation WebKit Port Update 12 PM Lunch 1 PM DOM Bindings, Event Loop &amp; More 2 PM Improving Interop with WPT 3 PM Break 3:30 PM The State of JSC 4-5:30 PM Lightning Talks 6-9 PM Dinner &amp; Party"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2019.html#lightning-talks","title":"Lightning Talks","text":"<ul> <li>WKWebView: An Embedder's Perspective</li> <li>Handling Test Expectations</li> <li>Python 3</li> <li>zlib optimizations for ARM</li> <li>Undo API</li> <li>WebKit Windows Port</li> <li>Privacy Features in WebKit</li> <li>Simon's 5-Year Plan Retrospective?</li> </ul>"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2019.html#transcripts","title":"Transcripts","text":"<ul> <li>Igalia Focus and Goals 2020</li> <li>WebKit Goals for 2020</li> <li>PlayStation WebKit Port Update</li> <li>DOM Bindings, Event Loop</li> <li>Back-Forward Cache</li> <li>Improving Interop with WPT</li> <li>The State of JSC</li> <li>WKWebView: An Embedder's Perspective</li> <li>Handling Test Expectations</li> <li>Python 3</li> <li>zlib optimizations for ARM</li> <li>Undo API</li> <li>WebKit Windows Port</li> <li>Privacy Features in WebKit</li> <li>Simon's 5-Year Plan Retrospective</li> </ul>"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2020.html","title":"WebKit Contributor Meeting 2020","text":"<p>WebKit Contributors Meeting November 16-17, 2020</p>"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2020.html#schedule","title":"Schedule","text":""},{"location":"Other/Contributor%20Meetings/ContributorMeeting2020.html#monday","title":"Monday","text":"Talk Welcome Igalia Priorities for 2021 Apple WebKit Goals for 2021 Angle for WebGL Fuzzing"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2020.html#tuesday","title":"Tuesday","text":"Talk WebKit Standards Positions Standards Implementation &amp; Conformance in JSC Web Inspector Overview Moving WebKit to Git"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2020.html#transcripts","title":"Transcripts","text":"<ul> <li>Igalia Priorities for 2021</li> <li>Apple WebKit Goals for 2021</li> <li>Angle for WebGL</li> <li>Fuzzing a WKWebView-based Browser Session</li> <li>WebKit Standards Positions</li> <li>Collaboration on Standards in JSC: A Cross-Organizational Update</li> <li>Moving WebKit to Git</li> <li>Sony WebKit 2020 Retrospective</li> <li>What's changing with scrolling on macOS</li> <li>GPUProcess</li> </ul>"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2021.html","title":"WebKit Contributor Meeting 2021","text":"<p>WebKit Contributors Meeting September 27-28, 2021</p>"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2021.html#schedule","title":"Schedule","text":""},{"location":"Other/Contributor%20Meetings/ContributorMeeting2021.html#monday","title":"Monday","text":"Talk Welcome WebKit 2021/2022 Igalia Apple Plans for WebKit: 2022 Edition Protected Collaboration Tree GitHub and New Processes OffscreenCanvas 2021 Update"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2021.html#tuesday","title":"Tuesday","text":"Talk WPT Update Introduction to LFC GPU Process TV on the Web Dialog/Inert WebGL 2.0, ANGLE and the direct-to-Metal Compiler in WebKit Rendering, UI and Privacy challenges in WebXR Browsers SVG in WebKit: Status of Compositing WPE Android Diving into bmalloc Contributing to Web Inspector Control Flow Integrity in WebKit"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2021.html#transcripts","title":"Transcripts","text":"<ul> <li>WebKit 2021/2022 Igalia</li> <li>Apple Plans for WebKit: 2022 Edition</li> <li>Protected Collaboration Tree</li> <li>GitHub and New Processes</li> <li>OffscreenCanvas 2021 Update</li> <li>WPT Update</li> <li>Introduction to LFC</li> <li>GPU Process</li> <li>TV on the Web</li> <li>Dialog/Inert</li> <li>WebGL 2.0, ANGLE and the direct-to-Metal Compiler in WebKit</li> <li>Rendering, UI and Privacy challenges in WebXR Browsers</li> <li>SVG in WebKit: Status of Compositing</li> <li>WPE Android</li> <li>Diving into bmalloc</li> <li>Contributing to Web Inspector</li> <li>Control Flow Integrity in WebKit</li> </ul>"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2022.html","title":"WebKit Contributor Meeting 2022","text":"<p>WebKit Contributors Meeting November 9-10, 2022</p>"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2022.html#schedule","title":"Schedule","text":""},{"location":"Other/Contributor%20Meetings/ContributorMeeting2022.html#wednesday-sessions","title":"Wednesday Sessions","text":"Talks WebKit 2022-2023 by Eric Meyer Sony WebKit 2023 Priorities by Don Olmstead Site Isolation by Alex Christensen What\u2019s new in LFC/IFC by Alan Baradlay Status of the new Layer-Based SVG Engine by Nikolas Zimmerman"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2022.html#thursday-sessions","title":"Thursday Sessions","text":"Talks Standards Positions by Anne van Kesteren Apple Goals for WebKit (cancelled) WebCodecs by Youenn Fablet State of WebKit SCM by Jonathan Bedard WPE WebKit for Android by Jani Hautakangas JavaScript Precompiled Bytecode Evaluation by Basuke Suzuki clang-tidy on JSC by Mikhail R. Gadelha Interop 2022/2023 by Jen Simmons WebDriver by Patrick Angle Bringing the GPU Process to macOS by Simon Fraser Improving Contributor Onramps by Jon Davis"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2022.html#talks","title":"Talks","text":""},{"location":"Other/Contributor%20Meetings/ContributorMeeting2022.html#wednesday","title":"Wednesday","text":""},{"location":"Other/Contributor%20Meetings/ContributorMeeting2022.html#webkit-2022-2023-igalia","title":"WebKit 2022-2023 (Igalia)","text":""},{"location":"Other/Contributor%20Meetings/ContributorMeeting2022.html#notes","title":"Notes","text":"<ul> <li>Igalia is a distributed company of 128 people, focused on open source</li> <li>8 main areas ranging from browsers to the cloud. Focused on all kinds of appliances and devices that use the web</li> <li>Investing this year in the Wolvic VR browser</li> <li>Mostly focused though on Linux and Android devices</li> <li>Moving to 2022 in review</li> <li>In 2021, about 16% of WebKit contributions were from Igalia, 77% from Apple, 4% from Sony</li> <li>In 2022, about 13% from Igalia, 79% from Apple, 3% from Sony</li> <li>But the other slice went from 2% to 4%, so more individual contributors</li> <li>A lot of Igalia's contributions came from WPE WebKit</li> <li>Got Angle enabled on dev branch</li> <li>Started working on GPU process and WebGPU, require some buffer sharing tech to work</li> <li>Refactoring scrolling code to make scrolling smoother, in WPE and GTK</li> <li>Analysis of animation frame vsync</li> <li>Upstreamed GStreamer backend for WebRTC</li> <li>Upstreamed work for MSE and EME</li> <li>Working on cloud gaming for WebRTC</li> <li>One of Igalia's biggest areas of work is WPE for Android</li> <li>Driven by having so many embedded devices running Android</li> <li>Completed Android WebView compat for a subset of APIs, hardware acceleration for media playback (decoding only, not encoding), PSON (important for security), fullscreen support, 64-bit ARM target support, lots of refactoring</li> <li>JavaScriptCore work: off-thread compilation, 32-bit platform-related work including WASM signaling memory</li> <li>Web Platform work: GamePad API (for a customer that wants to support gaming on embedded device), HTML interactive form validation, Layer-Based SVG engine, WebSpeech API (still in progress), :focus-visible, ARIA attribute reflection</li> <li>QA work: 2 new Ubuntu 22 bots, new bot to build with clang, Raspberry Pi bot</li> <li>Interop 2022 (cross-browser project with focus areas for improving interop): as of Mon Nov 7, Safari is on top (after starting the year at the bottom). All browsers have significantly improved over the year, from ~60% to ~80%. Many contributions, not just from Igalia</li> <li>Interop 2022 work: cascade layers, form elements, scrolling, subgrid tests</li> <li>Moving to Plans for 2023</li> <li>WPE for Android: keep updated with latest WPE WebKit (should be straightforward); implement missing APIs; support for WebDriver, WebInspector, WebXR; bug fixes and performance improvements (important for embedded devices, esp. low energy)</li> <li>Graphics: perf improvements for 2d rendering (requested by clients, who want smooth interfaces on touchscreens, show videos during setup, etc.); WebGL2 support; GPU process on top of ANGLE; fix issues that hurt correct results for WPE on benchmarks; speculative plans for Vulkan and WebGPU support</li> <li>Multimedia: WebRTC GStreamer backend; improve MSE eviction algorithm to be more aggressive; video PIP support (depending on GPU process progress); media playback using playbin3</li> <li>JavaScriptCore: support FTL tier for 32 bit; support LOL JIT tier on 32-bit; port JSC to RISC-V; investigate runtime sanitizer for PAC failures on ARM64e; temporal API support -- working on cross-browser spec</li> <li>12:30 PMWeb platform: upstreaming layer-based SVG engine (historically, SVG and MathML have been their own rendering engine, want to unify with HTML); WebSpeech AP and ImageBitmap for GTK + WPE (useful for embedded devices); DeviceOrientation for WPE port (good for handheld devices); WebXR implementation (depending on test infra)</li> <li>Fully support a11y on GTK4; HTTP/3; WebExtensions API</li> <li>QA: improve support for Flatpak on ARMv7 and ARM64; WebKitGTK/WPE in OSS-Fuzz; WebKitSearch build bot; WPE security bot</li> </ul>"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2022.html#qa","title":"Q&amp;A","text":"<ul> <li>Q from Geoff: about the layer-based SVG engine, what got Igalia interested in this work?</li> <li>A from Brian: We did a podcast on this. Original creators of KSVG are Igalians, they know all the warts. Will share a link to podcast in Slack. Important for embedded devices for efficiency and enables things that would not otherwise be possible</li> <li>https://www.igalia.com/chats/Igalia-Chats-Niko-SVG-WPE</li> <li>A from Niko: You'll hear more soon. Igalia is interested to reduce paint time, esp on low-end CPUs. We are doing painting on software, so if you can do more hardware-accelerated then you win, esp on embedded devices</li> <li>Q from Tim: Activity from Igalia on content-visibility and similar properties. Why is Igalia interested?</li> <li>A from Brian: Don't have an answer as to why it's interesting. It is :slightly_smiling_face:</li> <li>A from Cathie: content-visibility is good for performance and provide many ways developers can decide which parts has the priority to render or skip rendering</li> <li>Q from Jonathan: How are we going to make sure we don't break build for another platform?</li> <li>Q from Simon: What about WebXR + GPU process combined? We at Apple think some WebXR code has to run in GPU process (edited) </li> <li>A from Brian: I think that rings true to me</li> <li>A from Alex: Still in early stages so not sure, but yeah we want to try to do it in the GPU process</li> </ul>"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2022.html#sony-webkit-2023-priorities","title":"Sony WebKit 2023 Priorities","text":""},{"location":"Other/Contributor%20Meetings/ContributorMeeting2022.html#notes_1","title":"Notes","text":"<ul> <li>Two year anniversary of PS5</li> <li>Use WebKit for Media apps</li> <li>Game developers use as library</li> <li>Plans encompass 1-5 years</li> <li>WinCairo Port<ul> <li>Remove WebKitLegacy support<ul> <li>Held back by testing</li> <li>Need to update test configuration</li> </ul> </li> </ul> </li> <li>Network Backend<ul> <li>Add HTTP/3 support to cURL backend</li> <li>Movement within cURL to support this</li> <li>Not completely working<ul> <li>Some preliminary work done, eg displaying requests in Web Inspector</li> </ul> </li> </ul> </li> <li>Looking into cURL WebSockets API</li> <li>JavaScriptCore<ul> <li>Aim to reduce startup time</li> <li>Leverage byte code</li> </ul> </li> <li>To be expanded in future presentation</li> <li>Temporal API</li> <li>Record &amp; Tuple</li> <li>WPE PlayStation Backend<ul> <li>Joint efforts with Igalia</li> <li>Working on using WPE renderer</li> <li>Media support</li> <li>Gamepad support</li> </ul> </li> <li>Media<ul> <li>Historically PlayStation specific code not upstreamed, interest in improving this</li> <li>MSE</li> <li>Ideas about a \"Media tab\" in Web Inspector</li> </ul> </li> <li>GPU Process</li> <li>Called \"Web Compositor\"<ul> <li>Async composition</li> <li>Async scrolling</li> <li>Video rendering</li> </ul> </li> <li>Want to enable for PlayStation, currently enabled in Windows build</li> <li>WebGPU<ul> <li>Get Dawn + WebGPU working</li> <li>Port Dawn on PlayStation hardware</li> <li>Dawn is Google's equivalent of ANGLE</li> </ul> </li> <li>Modern 2D rendering<ul> <li>Fixing TextureMapper bugs</li> <li>Experimented with using WebGPU API for TextureMapper<ul> <li>As opposed to GL</li> <li>Speed improvement</li> </ul> </li> </ul> </li> </ul>"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2022.html#qa_1","title":"Q&amp;A","text":"<ul> <li>Q from Alex: Asked about WebGPU prototype wrt Cairo port?</li> <li>A: Being experimented with, not necessarily final plan.</li> </ul>"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2022.html#site-isolation","title":"Site Isolation","text":"<p>Site isolation is the next step to make browsers more secure, and its implementation is a major architecture shift.</p>"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2022.html#notes_2","title":"Notes","text":"<ul> <li>Old days: single process browser / WebKitLegacy<ul> <li>Stability: one tab crashes, everything crashes</li> <li>Security: no sandboxing</li> <li>Perf: one thread should be enough for 10 websites</li> <li>Responsiveness: that one thread will never hang right?<ul> <li>iOS 1.0 introduced WebThread</li> </ul> </li> </ul> </li> <li>Current multiprocess architecture / WebKit2<ul> <li>Multiple tightly sandboxed web content processes</li> <li>Network/storage process</li> <li>GPU process</li> <li>UI process</li> </ul> </li> <li>Partitioned Storage<ul> <li>IndexedDB / caches / other persistent storage is partitioned by origin</li> <li>If a.com and b.com both embed example.com, example.com's cached data is stored separately in a.com and b.com's partitions respectively</li> </ul> </li> <li>Why site isolation?<ul> <li>Meltdown/Spectre vulnerabilities: JS can now read arbitrary data</li> <li>Partial mitigations:</li> <li>Cross-Origin-Embedder-Policy, Cross-Origin-OpenerPolicy</li> <li>Disabling SharedArrayBuffer and performance.now's precision</li> <li>Exploit read/write/execute gadgets can send rogue messages; what can it reach?</li> </ul> </li> <li>Data isolation: already in open source<ul> <li>Check if firstPartyForCookies is reasonable before allowing access to cookies</li> <li>window.open can still put multiple first parties in the same process</li> </ul> </li> <li>Site isolation<ul> <li>Put cross-origin iframes in different process than its parent</li> <li>Each process can read only one partition of data; ideally the partition is never a message parameter</li> <li>Spectre attacks can't read anything interesting</li> </ul> </li> <li>New cross-origin iframe loading flow<ul> <li>decidePolicyForNavigationAction - tell old process navigation will likely continue in new process</li> <li>when navigation commits, tell old process that happened, switch from LocalFrame to RemoteFrame in frame tree</li> <li>When loading completes in new process, tell old process that happened, then on-load can fire</li> <li>Make all FrameTree traversal use IPC to pull or push info</li> </ul> </li> <li>Other things to consider<ul> <li>Drawing to the screen/compositing</li> <li>Accessibility tree needs to know about out-of-process iframes</li> <li>Perf - processes aren't free</li> <li>Web inspector - needs to be able to introspect out-of-process state</li> <li>Printing with cross-origin iframes</li> <li>Anything related to frames, must audit at least 200 uses of frames</li> </ul> </li> <li>Injected bundles: many APIs do not work with out-of-process iframes<ul> <li>InjectedBundlePagePolicyClient: allows you to navigate without even asking the UI client</li> <li>WKBundleFrameCopyChildFrames: API has to be removed or changed, can't return a copy of all frames in the same way in an isolated world</li> </ul> </li> </ul>"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2022.html#qa_2","title":"Q&amp;A","text":"<ul> <li>Q from Jean-Yves: What is the estimated timeline to finish this? Past experience shows that it will take a very long time to complete.</li> <li>A: We know it's a large effort and will take more than one year.</li> <li>Q from Cameron: Other browsers have decided that on mobile devices it's not practical to have a process per origin due to perf constraints. Would WebKit be subject to the same limitations?</li> <li>A: We are aware of this and we'll have to measure the overhead to help make that decision. We are still in the early stages of just getting something that works. (edited) </li> <li>Q from Brian Kardell: WebKit was among the first to do partitioned storage. Were there a lot of issues opened during the timeframe when that feature was introduced?</li> <li>A from Alex: There are cross-origin communication channels available in the web standard. We decided that we don't want to make it easy to do cross-origin tracking in WebKit for privacy purposes, even though there are some legit uses of that tracking.</li> <li>A from John: the privacy boundary for partitioned storage is site not origin. Originally when implementing storage partitions, we we also wanted to partition cookies but there was too much breakage. Took until 2017 to implement partitioned cookies and took until this year for a web standard to address this issue.</li> </ul>"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2022.html#whats-new-in-lfc","title":"What\u2019s new in LFC","text":"<p>Inline and flex layout features added throughout the year to LFC -aka how much WebKit\u2019s line layout has progressed.</p>"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2022.html#notes_3","title":"Notes","text":"<ul> <li>LFC, quick recap<ul> <li>Layout formatting context</li> <li>Initiative to reimplement and redesign WebKit's render engine (similar to Blink's layout ng work)</li> <li>Focusing on Inline Formatting Context and Flex Formatting Context</li> </ul> </li> <li>IFC<ul> <li>Webkit maintains 2 inline layout implementations that are independent of each other (legacy and modern)</li> <li>Legacy is always enabled and modern is behind a runtime flag</li> <li>Has been enabled in Safari for many years now</li> <li>When both are enabled, we have to make a certain decision during layout whether to use modern or legacy</li> <li>Decision is made at the block level to determine if IFC can handle the content</li> </ul> </li> <li>Example<ul> <li>The second block container has to use legacy since the block has a property that is not supported by IFC</li> </ul> </li> <li>How much content is going through IFC as of 10/8 trunk<ul> <li>Some text heavy pages use IFC 100% and does not use legacy at all</li> <li>Some other pages the coverage value can drop to under 50%</li> <li>line-clamp is one offender that causes the % to drop</li> </ul> </li> <li>IFC, progressions<ul> <li>WPT with IFC disabled there are many tests that fail</li> <li>Even some older bug reports get fixed with IFC enabled</li> </ul> </li> <li>IFC, expanding coverage, what's new<ul> <li>Keep adding new features to IFC to bring it up to same level as legacy so that we can remove it</li> <li>Many progressions with BiDi content using IFC</li> <li>When working on inline layout, you do not have to care about line orientations since everything works with logical dimensions</li> <li>Traditionally WebKit has a preferred width code path that basically runs layout twice</li> <li>These 2 codepaths can come back with different values which can result in unexpected overflow or line breaking</li> <li>New floating layout should hopefully be easily to understand and maintain</li> </ul> </li> <li>FFC, the new flex layout implementation<ul> <li>Rewriting flex layout based on LFC design principles</li> <li>Not currently enabled because we need to do integration work</li> <li>Need ways to integrate with existing render tree architecture</li> </ul> </li> </ul>"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2022.html#status-of-the-new-layer-based-svg-engine","title":"Status of the new Layer-Based SVG Engine","text":"<p>A closer look at the status of the Layer-Based SVG Engine in WebKit.</p>"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2022.html#notes_4","title":"Notes","text":"<ul> <li>based in Germany - started contributing to KHTML in 2001, co-founded ksvg with Rob Buis (also an Igalian) and moved on as it moved into webkit</li> <li>been working exclusively in webkit for some time</li> <li>What is LBSE? The layout based SVG engine we\u2019ve been developing at Igalia in WebKit since 2019</li> <li>aimed at resolving architectural issues present for 15+ years</li> <li>Allows us to use hardware acceleration and features of CSS that are useful</li> <li>POC patch in October 2021 passed all existing tests, pixel perfect</li> <li>performance was comparable to legacy engine in motion mark - some tests were way faster, some were slower\u2026 but in principle we proved it is possible and not a perf issue</li> <li>how is it achieved? coordinating system decisions in the redesign with CSS, reuse as much code as possible in RenderLayer without SVG specific changes</li> <li>evolution since 2021: the \u201cfinal\u201d version of the prototype was a drop in replacement for the old SVG engine, but it was not a way to upstream it - it would be a huge patch.  It is all or nothing, there is no way to merge parts - too big a change at once</li> <li>we developed an upstreaming plan to evolve this and land pieves in atomic pieces, all of the code is behind a compile time flag</li> <li>Unfortunately this means a lot of manual rework, basically equivalent to yet another rewrite\u2026 but..</li> <li>there is a master bug report 90738 \u201cHarmonize HTML and SVG Rendering\u201d</li> <li>89 patches landed, 75% upstream</li> <li>we started adding runtime flags in November last year</li> <li>and it wasn\u2019t until January this year we were beginning into real foundations</li> <li>along the way some interesting and important upstream bugs were fixed - some about a decade old, perspective should not be affected by transform-origin</li> <li>by the end of April we were on to SVG text</li> <li>then we added all of the basic shapes, then viewbox, then compositing - this also fixed a 10+ year old bug about upscaling an object via css transforms</li> <li>(that brings us through July)</li> <li>in august we activated support for \u2018defs\u2019 and foreign objects and image elements and activated remaining shapes (polyline, polygon, line)</li> <li>in September we fixed some bigs about compositing and device pixels alignment and pixel snapping when elements are composited</li> <li>we activated sub-pixel precision for render tree dumps</li> <li>very recently (in October) the size negotiation for"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2022.html#qa_3","title":"WebKit Contributor Meeting 2022","text":""},{"location":"Other/Contributor%20Meetings/ContributorMeeting2022.html#thursday","title":"WebKit Contributor Meeting 2022","text":""},{"location":"Other/Contributor%20Meetings/ContributorMeeting2022.html#standards-positions","title":"WebKit Contributor Meeting 2022","text":""},{"location":"Other/Contributor%20Meetings/ContributorMeeting2022.html#notes_5","title":"WebKit Contributor Meeting 2022","text":""},{"location":"Other/Contributor%20Meetings/ContributorMeeting2022.html#webcodecs","title":"WebKit Contributor Meeting 2022","text":""},{"location":"Other/Contributor%20Meetings/ContributorMeeting2022.html#notes_6","title":"WebKit Contributor Meeting 2022","text":""},{"location":"Other/Contributor%20Meetings/ContributorMeeting2022.html#qa_4","title":"WebKit Contributor Meeting 2022","text":""},{"location":"Other/Contributor%20Meetings/ContributorMeeting2022.html#state-of-webkit-scm","title":"WebKit Contributor Meeting 2022","text":""},{"location":"Other/Contributor%20Meetings/ContributorMeeting2022.html#notes_7","title":"WebKit Contributor Meeting 2022","text":""},{"location":"Other/Contributor%20Meetings/ContributorMeeting2022.html#qa_5","title":"WebKit Contributor Meeting 2022","text":""},{"location":"Other/Contributor%20Meetings/ContributorMeeting2022.html#wpe-webkit-for-android","title":"WebKit Contributor Meeting 2022","text":""},{"location":"Other/Contributor%20Meetings/ContributorMeeting2022.html#notes_8","title":"WebKit Contributor Meeting 2022","text":""},{"location":"Other/Contributor%20Meetings/ContributorMeeting2022.html#javascript-precompiled-bytecode-evaluation","title":"WebKit Contributor Meeting 2022","text":""},{"location":"Other/Contributor%20Meetings/ContributorMeeting2022.html#notes_9","title":"WebKit Contributor Meeting 2022","text":""},{"location":"Other/Contributor%20Meetings/ContributorMeeting2022.html#qa_6","title":"WebKit Contributor Meeting 2022","text":""},{"location":"Other/Contributor%20Meetings/ContributorMeeting2022.html#clang-tidy-on-jsc","title":"WebKit Contributor Meeting 2022","text":""},{"location":"Other/Contributor%20Meetings/ContributorMeeting2022.html#notes_10","title":"WebKit Contributor Meeting 2022","text":""},{"location":"Other/Contributor%20Meetings/ContributorMeeting2022.html#qa_7","title":"WebKit Contributor Meeting 2022","text":""},{"location":"Other/Contributor%20Meetings/ContributorMeeting2022.html#interop-2022-2023","title":"WebKit Contributor Meeting 2022","text":""},{"location":"Other/Contributor%20Meetings/ContributorMeeting2022.html#notes_11","title":"WebKit Contributor Meeting 2022","text":""},{"location":"Other/Contributor%20Meetings/ContributorMeeting2022.html#webdriver","title":"WebKit Contributor Meeting 2022","text":""},{"location":"Other/Contributor%20Meetings/ContributorMeeting2022.html#notes_12","title":"WebKit Contributor Meeting 2022","text":""},{"location":"Other/Contributor%20Meetings/ContributorMeeting2022.html#qa_8","title":"WebKit Contributor Meeting 2022","text":""},{"location":"Other/Contributor%20Meetings/ContributorMeeting2022.html#bringing-the-gpu-process-to-macos","title":"WebKit Contributor Meeting 2022","text":""},{"location":"Other/Contributor%20Meetings/ContributorMeeting2022.html#notes_13","title":"WebKit Contributor Meeting 2022","text":""},{"location":"Other/Contributor%20Meetings/ContributorMeeting2022.html#qa_9","title":"WebKit Contributor Meeting 2022","text":""},{"location":"Other/Contributor%20Meetings/ContributorMeeting2022.html#improving-contributor-onramps","title":"WebKit Contributor Meeting 2022","text":""},{"location":"Other/Contributor%20Meetings/ContributorMeeting2022.html#notes_14","title":"WebKit Contributor Meeting 2022","text":""},{"location":"Other/Contributor%20Meetings/ContributorMeeting2023.html","title":"WebKit Contributors Meeting 2023","text":"<p>WebKit Contributors Meeting October 24-25, 2023</p>"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2023.html#schedule","title":"Schedule","text":""},{"location":"Other/Contributor%20Meetings/ContributorMeeting2023.html#tuesday-sessions","title":"Tuesday Sessions","text":"Talks Speaker Sony WebKit 2024 Priorities Don Olmstead Igalia and WebKit: status update and plans Mario S\u00e1nchez Prada Apple Plans for WebKit: 2024 Edition Maciej Stachowiak Managing Security Changes in WebKit Jonathan Bedard Current Status of Cloud Gaming Byungseon(Sun) Shin Contributing to Web Inspector Devin Rousso"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2023.html#wednesday-sessions","title":"Wednesday Sessions","text":"Talks Speaker WPE platform API cgarcia Gstreamer WebRTC Status Philn WebKitGTK and WPE ports SDK Patrick Griffis Status of LBSE Nikolas Zimmermann &amp; Rob Buis Lightning Talks Speaker Vertical Form Controls Aditya Keerthi Masonry Layout Brandon Stewart Speedometer 3 Ryosuke Niwa All Fonts Feature (Proposal) Frances Cornwall Playwright and the state of modern e2e testing Pavel Feldman Standards Positions Page and Contributing Anne van Kesteren Implementing the WebAssembly GC proposal in JSC Asumu Takikawa Enabling WebAssembly and FTL JIT on Windows Port Ian Grunert Shipping Web APIs to Customers Tim Nguyen &amp; Elika J. Etemad Quirks: Kintsugi for the Web Karl Dubost Proposal: New WebKit Logo Jon Davis"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2023.html#talks","title":"Talks","text":""},{"location":"Other/Contributor%20Meetings/ContributorMeeting2023.html#tuesday","title":"Tuesday","text":""},{"location":"Other/Contributor%20Meetings/ContributorMeeting2023.html#sony-webkit-2024-priorities","title":"Sony WebKit 2024 Priorities","text":""},{"location":"Other/Contributor%20Meetings/ContributorMeeting2023.html#notes","title":"Notes","text":"<ul> <li>Wincairo: Build and build maintenance. MSVC issues with 2023.</li> <li>Windows Playwright is interesting - webDriver-sh  and looks very promising.</li> <li>Http3: Curl backend is experimental but working on ironing out bugs.</li> <li>WASM support: Our interest is in Jit-less . Experimental and research stage at this point.</li> <li>JS: Ross Krisling continues to works on TC39 standards</li> <li>LibWPE:<ul> <li>Collaboration with Igalia on Hardware V-sync. Video and gamepad(newer version for vibration and haptics in conjunction with VR).</li> <li>Hope to get this to a shipping state</li> </ul> </li> <li>Graphics:<ul> <li>WC compositor porting to playstation</li> <li>GPU process is if specific interest for security.</li> <li>WebGPU - Currently using Google's dawn backend</li> <li>Able to get the basic rudimentary rendering on PS port.</li> <li>Hoping to move this to the broader webkit project</li> </ul> </li> </ul>"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2023.html#questions","title":"Questions","text":"<ul> <li>Does PS4 and PS5 use the webkit differently?<ul> <li>Mostly NO</li> <li>Any web-transport examples? Some internal teams were interested due to video.</li> </ul> </li> <li>What is connection between windows and PlayStation port?<ul> <li>Webinspector &amp; debug tools</li> <li>backend portions - CURL, cairo, compositor</li> </ul> </li> <li>Wincairo -&gt; Webkit windows Browser how much is involved?<ul> <li>Would love but resources corporate goals</li> <li>Minibrowser is nice starting point but a lot to build upon.</li> </ul> </li> <li>GPUProcess &amp; WC compositor - how much was successful?<ul> <li>Windows is running. (Fuji)</li> </ul> </li> </ul>"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2023.html#igalia-and-webkit-status-update-and-plans","title":"Igalia and WebKit: status update and plans","text":""},{"location":"Other/Contributor%20Meetings/ContributorMeeting2023.html#slides","title":"Slides","text":""},{"location":"Other/Contributor%20Meetings/ContributorMeeting2023.html#notes_1","title":"Notes","text":"<ul> <li>Mario is the coordinator of the Igalia WebKit team</li> <li>Igalia is an open source consultancy, fully remote, flat structure, contributors to engines such as: WebKit, Chromium Gecko, Servo</li> <li>Igalia contributes to W3C and other spec bodies</li> <li>inside the WebKit Igalia mainly works as the maintainer of WebKitGTK and WPE ports, and the implementation of Web standards and Javascript features</li> <li>Igalia spends time in other parts of WebKit such as bugfixing, refactorings, performance, 32-bit, support, QA, etc.</li> <li>The list of embedded devices where WebKit runs: set-top boxes, smart homes, medical devices, navigational, digital sinage, QA, etc.</li> <li>Who are our users?<ul> <li>we have multiple kind of users from the perspective of what they do with the engine: <ul> <li>port users (app developers), platform providers (such as fedora, yocto)</li> <li>Web developers that test things in different engines</li> <li>end users that access the web</li> </ul> </li> </ul> </li> <li>Strategic goals<ul> <li>we aim for compatibility and interoperability, that it means the platform is good and homogeneus</li> <li>performance it is very important in embedded devices, and efficiency using resources in the hardware</li> <li>QA and security, are basic to make sure products continue running</li> <li>development tools and documentation is a key part for a good story</li> <li>it is a goal to improve collaboration with other ports inside the WebKit community</li> </ul> </li> <li>Next we are going to talk about the recent  work, later what we plan to do</li> <li>WebKit contributors 2023<ul> <li>graph with the amount of the commits inside of the WebKit project</li> <li>mostly is Apple and Igalia is second, with Sony and RedHat later</li> <li>Igalia makes 50% of the commits that are not Igalia</li> <li>in previous years we had a bigger percentage because now we get more commits for other contributors</li> <li>which is good considering the health of the project</li> </ul> </li> <li>Web platform Contributions<ul> <li>CSS properties: content-visibility</li> <li>HTML Fetch Priority</li> <li>Popover API</li> <li>Secure curves</li> <li>we implemented those standards this year</li> </ul> </li> <li>Graphics<ul> <li>we did a lot of work here this year</li> <li>in the port specific side and in the core</li> <li>Finished ANGLE integration and WebGL2 support</li> <li>Sharing buffers between process is an important part of our future development, we are working to move to that kind of architecture</li> <li>we cleaned up some other things such as: remove the wayland server, synchronization using displayLink architecture</li> <li>we have worked is the replacement of cairo as the 2D rendering engine<ul> <li>today we do not have anything to present but it is a big effort internally</li> </ul> </li> <li>we have a experimental GPUProcess support</li> <li>we implemented SVG layers as a first class citizen in the LBSE effort</li> </ul> </li> <li>Multimedia<ul> <li>biggest users for us if the STB which means multimedia is important</li> <li>we did DMAbuf sink integration</li> <li>WebCodecs, WebRTC using gstreamer, improved power consumption, etc.</li> </ul> </li> <li>JSC<ul> <li>the biggest effort we do is to support 32-bit</li> <li>we added multiple features to the ARMv7</li> <li>we work in WASM GC and it is the topic of asumu's talk tomorrow</li> </ul> </li> <li>new WPE API<ul> <li>we have several problems with the API, it was created that way for a reason in the past, but we wan to change that</li> <li>wayland centric, API documentation lacking, complexity</li> <li>we plan to create a new simpler API</li> <li>we started to work on that</li> <li>we are starting to push patches</li> <li>tomorrow carlos is going to talk about this</li> </ul> </li> <li>WebKit on Android<ul> <li>last year we presented it</li> <li>the idea is to use a different engine in Android using WebKit</li> <li>we did not have that much time this year to work on it</li> <li>we are consuming the WPE API</li> <li>we support multiple architectures, native integration, etc.</li> </ul> </li> <li>Quality Assurance<ul> <li>this is not our strongest point in our ports</li> <li>we have seen multiple problems this year</li> <li>we have added more people to the team and we spent a lot of time fixing API and Layout failures</li> <li>the numbers are interesting, we have increased the amount of tests running and reduced the amount of skipped tests</li> <li>we can see the graphs with the situation</li> <li>it is not great but we are improving, but we are still not happy with the situation</li> <li>we reduced the amount of flakes</li> <li>we are shifting the work for GTK</li> <li>for WPE has a better situation, as a result of the effort we are doing</li> </ul> </li> <li>for security we do multiple releases every year</li> <li>we publish them in the website</li> <li>tooling and documentation</li> <li>we want to move to the new github documentation infrastructure</li> <li>people in the QA is the one that works in this</li> <li>on the tooling we did a biggest an effort</li> <li>patrick will talk about this tomorrow</li> <li>WebKit is not an easy project and we would like to have a solution for developers and testers</li> <li>the solutions we had were good for one or the other but not for both</li> <li>so we have created a solution with a container based</li> <li>OCI compatible container</li> <li>you can download it and you can use it even to hack in the dependencies</li> <li>for instance it is important for the people working in gstreamer</li> <li>next steps section</li> <li>on Web Platform</li> <li>we will work in the Navigation API and the hasUAVisualTransition</li> <li>graphics<ul> <li>refactoring is important in the next year</li> <li>use dmabuf more extensively in the architecture</li> <li>replace the 2D engines sooner than later and the GPUProcess</li> <li>the new SVG engine should be upstreamed next year completely without any performance regression</li> </ul> </li> <li>multimedia<ul> <li>gstreamer webrtc backend and general maintenance is the main work we plan to do</li> </ul> </li> <li>JSC<ul> <li>mainly ARMv7 improvements</li> <li>enable OMG and FTL</li> <li>land WASM GC</li> </ul> </li> <li>new WPE API<ul> <li>finish some initial version, review the API docs and deprecate the old one when we think it is ready</li> <li>we want to start landing patches sooner than later</li> <li>there is no specific release date</li> </ul> </li> <li>WebKit on Android<ul> <li>we want a first usable version of WPE on Android</li> </ul> </li> <li>there is a list of next steps we plan in the future next year</li> <li>QA<ul> <li>we need to improve the charts of tests running and fixes</li> <li>we have the new SDK that we hope it helps to improve the story</li> <li>prepare GTK4 bots, because it is the next version</li> <li>tooling and documentation<ul> <li>release the first version of the SDK</li> <li>and improve the documentation</li> </ul> </li> </ul> </li> </ul>"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2023.html#questions_1","title":"Questions","text":"<ul> <li>WASM BBQJIT in ARMv7, work in the registers support</li> <li>they talk about the specifics of how to allocate registers in the ARMv7 architecture</li> <li>cairo replacement, is that something you try to use for Windows too</li> <li>right know it is mostly for Linux, but we do not discard that running in the Windows is important</li> <li>graph of regressions where they come from</li> <li>it comes from webkit test hunter for our ports</li> <li>webkit for android, do you hope that someone can create a browser out of it or you have niche cases in mind?<ul> <li>ideally it could be a complete replacement but we do not have all the features but for the moment we don't have all the features</li> </ul> </li> <li>we did a proposal for the Windows registers allocation, and he can discuss this later more specifically</li> <li>are you planning to use the android support as a general browser or as a webview replacement?</li> <li>mario explains that it is going to be a webview replacement</li> <li>curious about the CI side, windows using containers</li> <li>we already use containers LXC in the bots</li> <li>the idea is to use the SDK container in the bots</li> <li>either running them in the LXC or directly</li> </ul>"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2023.html#current-status-of-cloud-gaming","title":"Current Status of Cloud Gaming","text":""},{"location":"Other/Contributor%20Meetings/ContributorMeeting2023.html#slides_1","title":"Slides","text":""},{"location":"Other/Contributor%20Meetings/ContributorMeeting2023.html#notes_2","title":"Notes","text":"<ul> <li>Agenda:<ul> <li>What is cloud gaming</li> <li>what are the difference from normal video gaming</li> <li>current standardization activity at w3c working group</li> </ul> </li> <li>What is cloud gaming? Graphic rendering on the cloud side. Client side does need  high perf machine.</li> <li>Customer does not need to upgrade hw console every year</li> <li>Click to pixel latency is one of the major performance indices.</li> <li>It\u2019s recommended to be under 150ms in the cloud gaming</li> <li>slide: 120fps processing timeline.</li> <li>Demo (recorded video) (edited) </li> <li>current standard activities w3c webrtc wg:<ul> <li>quality improvements on lossy network condition</li> <li>faster video recovery</li> <li>consistent latency</li> </ul> </li> <li>nvidia trying to propose to the working groups</li> <li>macOS/iOS topics:<ul> <li>WebRTC Release Cycle, by annual</li> <li>Higher Resolution 4K Support, (HEVC behind a flag)</li> <li>User Input gamepad, keyboard, and mouse</li> </ul> </li> </ul>"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2023.html#questions_2","title":"Questions","text":"<ul> <li>slide: What are the difference from normal video streaming:<ul> <li>ultra low latency</li> <li>consistent latency</li> <li>video content</li> <li>QoS</li> <li>\u2026and more</li> </ul> </li> <li>How about WebTransport or WebCode instead of webrtc?<ul> <li>Yes. internally considering.</li> </ul> </li> </ul>"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2023.html#contributing-to-web-inspector","title":"Contributing to Web Inspector","text":""},{"location":"Other/Contributor%20Meetings/ContributorMeeting2023.html#slides_2","title":"Slides","text":""},{"location":"Other/Contributor%20Meetings/ContributorMeeting2023.html#notes_3","title":"Notes","text":"<ul> <li>Devin spent 8 years working on Web Inspector.</li> <li>Going to give run down.</li> <li>Debuggable is the main thing you are inspecting</li> <li>Target is the thing that is being debugged (e.g., Javascript, page, service worker)</li> <li>It may be weird that you have two separate list... some are one 1 to 1.</li> <li>but others allow a 1 to many relationship. Like web pages can debug many things</li> <li>Frontend is our UI.</li> <li>And backend is the page we are inspecting&gt;</li> <li>Protocol is used to communicate between \"front end\" and \"backend\"</li> <li>Going to focus this talk about the protocol... the protocol is auto generated.</li> <li>There is also the \"remote\", which is important for devices... like connecting to iOS and Simulator</li> <li>With remote, we get some assurances around IPC</li> <li>Front end is just vanilla javascript and HTML, CSS, etc.</li> <li>The InspectorFrontendHost is just a special API (non-standard)</li> <li>And InspectorFrontendAPI is how things talk to web inspector</li> <li>We use a few frameworks to help us (e.g., codeMirror).</li> <li>We use extensive use of event listeners</li> <li>We have a \"custom layout engine\"TM</li> <li>we use this as a \"view\", and we use requestAnimationFrame etc to keep everything in sync.</li> <li>And we follow a simple MVC</li> <li>Controllers are mainly \"managers\"... mostly a singletons on the frontend</li> <li>They handle the protocol communication</li> <li>They need to know which target they are communicating with and handling</li> <li>But other than that is just models and views</li> <li>The models are just data wrappers</li> <li>The UI is split into multiple areas that interact with each other</li> <li>As you interact, each area is given metadata to control various aspects of web inspector.</li> <li>Web inspector care about RTL, Dark mode, eetc. so you need to care about that when contributing to the project</li> <li>There are various APIs attatched to WI.*   ... WI.Table, WI.NavigationBar, etc. as components.</li> <li>Lots of examples of how these components are used. Makes it easier for new developers to contribute</li> <li>The protocol is almost entirely auto generated</li> <li>Basic IPC system based on JSON</li> <li>There are a ton of protocols</li> <li>And there are other domains.</li> <li>Like CSS domain corresponds to CSS things, runtime is \"javascript\"</li> <li>new domains are just specified with JSON</li> <li>beyond domains, there are corresponding types</li> <li>It provides type safety and assurances</li> <li>Plus it provides events and expected structures</li> <li>Devin shows an example of what a definition of a domain looks like in JSON</li> <li>Similarly with types... we define it as \"type\": \"integer \"</li> <li>Makes it really easy to understand... same with enums</li> <li>e.g., \"before\" and \"after\"</li> <li>and gets more complex from there</li> <li>but not too complex </li> <li>You can also specify \"commands\"... like., \"getDocument\"... and that would send from frontend to backend, and it sends a serialized version of the command.</li> <li>As a developer, you just implement the the frontend call, backend function, but you don't need to worry about the IPC. That's all handled for you. (edited) </li> <li>And for events, there is just parameter associated with it. It keeps things simple.</li> <li>Web inspector needs to consider:<ul> <li>Inspector Web Process</li> <li>Inspector UIProcess</li> <li>Inspected UIProcess</li> <li>Inspected WebProcess</li> </ul> </li> <li>and how each communicates / route through each one. We handle that for you. You never have to touch this code to make it easier.</li> <li>But what's important is that \"inspector\" and \"inspected\" are totally different devices potentially. But you don't need to worry about that as a developer</li> <li>It's possible to inspect from a new macOs to an older iOS, but older iOS can't inspect a newer macOS... i.e., you can inspect \"the past\", but not the future</li> <li>We have automated systems to check for support if the backend has support for a particular command</li> <li>Which is very helpful. So you can recover</li> <li>Again, this is all auto generated,</li> <li>....Devin shows how this is done with JS... communication between front and backend</li> <li>... lots of JS code...</li> <li>Similarly with events... it's not that hard</li> <li>Heavy use of WTFJSON</li> <li>Backend... each debugable has a top level controller object</li> <li>It's responsible to create an agent to help with a domain... it provides the functionality for that domain.</li> <li>E.g., all would have a runtime agent because they all talk \"javascfript\", but they are normally specialized</li> <li>There are specialized objects to help you work with different things you might want to debug.</li> <li>General tips... a lot of prior out throughout the code base. Only confusing bit is the protocol.</li> <li>But generally everything is pretty straight forward</li> <li>You can even inspect web inspector with web inspector</li> <li>We rely on ESLint</li> <li>to keep the code clean and consistent</li> <li>There are barely no UI tests... you don't have to write tests.... but the downside is that you need to manually test stuff</li> <li>No need to wait for EWS... it's easy to make changes to the UI</li> </ul>"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2023.html#questions_3","title":"Questions","text":"<ul> <li>Razvan, when you create agents on the backend, how do you maintain sprawl?<ul> <li>generally try to think to figure out what category it falls into? (e.g., look on MDN)... you could argue that you could make a new domain. Try to use an existing agent if you can. Or if it requires a special case that talks to multiple agents. You need to think hard about the domain and the agents to handle different cases. So yeah, think about the architecture and reuse as much as possible</li> </ul> </li> <li>I tried to make changes to the code UI... do you plan to add UI frameworks?<ul> <li>we are not huge fans of UI frameworks because they can get in the way. We like to name things well, and keep things super simple. The view system is our answer to that. We have a very basic system, it's only like 20 lines of code.</li> <li>We define the rules. And we enforce them on ourselves</li> <li>The less code we have to deal with the better. Nowadays, we don't need things like jQuery anymore because how much better browsers are now</li> </ul> </li> <li>Don from Sony. You are allowed to use whatever is available in the web platform. What web platform would you like to see more of?<ul> <li>we see a lot uses of var but generally we leave those alone if it's working. We use new things as a test bed, but we don't got out of our way. Like, going forward, we always replace functionality old functionality and also use new things when they become available?</li> </ul> </li> <li>do you use ESLint on EWS or plan to?<ul> <li>more often than not, we use it as guide but we sometimes don't agree with ESLint for various reasons. So, more of a guide an principle.</li> </ul> </li> <li>comment: Karl. I use web inspector a lot for web compat. It helps a lot when you fix web inspector it helps fix web platform health overall.</li> <li>Please fix more stuff</li> <li>Devin: sure. We want to help everyone - web developers and WebKit engineers</li> <li>We sometimes even do very specific things for web inspector developers. And we add things specific things just for WebKit engineers, but also benefit web developers</li> <li>Frances. What suggestions do you have to find Web Inspector bugs for webkit engineers?<ul> <li>here is no way to explicitly find them, but there is no specific list. I'm sure there are quite a few</li> </ul> </li> <li>Razvan: to rephrase is bugzilla, the right place to file Webkit bugs?<ul> <li>yes, put things under the Web Inspector components</li> </ul> </li> <li>Elika, great presentation! Where can we find this info later?<ul> <li>Great question. I want to write a blog post about this. I don't have speaker notes.</li> </ul> </li> </ul>"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2023.html#wednesday","title":"Wednesday","text":""},{"location":"Other/Contributor%20Meetings/ContributorMeeting2023.html#wpe-platform-api","title":"WPE platform API","text":""},{"location":"Other/Contributor%20Meetings/ContributorMeeting2023.html#slides_3","title":"Slides","text":""},{"location":"Other/Contributor%20Meetings/ContributorMeeting2023.html#notes_4","title":"Notes","text":"<ul> <li>we are working in the new WPE API</li> <li>current status</li> <li>the architecture nowadays has WPE as an external library for rendering and input handling</li> <li>backends implement does interfaces for rendering and input handling</li> <li>we have the fdo backend for wayland support</li> <li>there are other backend such as the RDK for set top boxes</li> <li>we have cog which is a browser you can use as reference implementation</li> <li>problems with this model</li> <li>we have multiple problems</li> <li>we designed with EGL and wayland for rendering</li> <li>but with time we realized we can render better without EGL and wayland</li> <li>wpe is an external library</li> <li>used by WPEWebKit, backends and applications</li> <li>this way libwpe is not obvious what parts are needed from the UIProcess and the WebProcess</li> <li>it has its own IPC mechanism</li> <li>it is not easy to maintain because we have to sync with releases with WPEWebKit and cog</li> <li>there is lack of documentation</li> <li>and just simple implementation requires a lot of understanding</li> <li>cog is not just a reference it includes a lot of glue code to complement</li> <li>that is why it is mainly used</li> <li>new architecture</li> <li>we want to get rid by the wayland model</li> <li>and use a buffer sharing mechanism that the target platform can provide</li> <li>the rendering of the WebProcess it is independent of the target platform not</li> <li>with gbm, o surfaceledd contexts we can avoid that dependency</li> <li>and WebKit IPC is what we want to use for the communication</li> <li>web process send message when allocating a buffer</li> <li>and it notifies when the buffer has a frame</li> <li>and the UIProcess notifies when the buffer is rendered</li> <li>this is already upstream in gtk port</li> <li>wpe is not using it now because it requires a new API for this</li> <li>we don neew the wayland compositor to share the buffers</li> <li>the new WPE API</li> <li>we need it because we need a new model because the requirements are different</li> <li>the plan is to make it part of WebKit</li> <li>instead of an external library</li> <li>it still has the same users: WPEWebKit, platform implementations and applications</li> <li>in this case it is optional applications know about this API</li> <li>there will be builtin platform implementations now</li> <li>this is a difference with the past library</li> <li>we will have support to load external ones</li> <li>but we will include the ones that cover most of the cases as part of the library</li> <li>it would be optional to compile them</li> <li>drm, wayland and headless would be the first additions</li> <li>DRM is the most efficient to render a fullscreen mode browser</li> <li>we can do direct scan-out buffers</li> <li>headless would be used mainly for testing</li> <li>the API is now UIProcess only (edited) </li> <li>making it easier to use</li> <li>there would be API documentation, we would fail if there are no symbols for an API</li> <li>and it will be easy for most use cases but flexible enough</li> <li>current status</li> <li>the API is already and working</li> <li>wayland platform implements most of the interfaces</li> <li>headless is complete too</li> <li>and the DRM is on going, it implements only some input events</li> <li>Plans</li> <li>we want to upstream the patches from now on</li> <li>it is a big patch</li> <li>but it is well self contained</li> <li>the architecture is already in the repository</li> <li>because it is enabled for the GTK port</li> <li>we have enough code to be confident this is the way to go</li> <li>we have to implement the missing features</li> <li>we need to review all the documentation</li> <li>because we are adding changes and we need to update</li> <li>we will deprecate the old API</li> <li>but we will not remove it for the moment</li> <li>because some platforms are not able to use sharing buffers</li> <li>it does require any API or ABI version bump</li> <li>and applications using the old API will just work</li> <li>but they could use the new API with the new constructor</li> <li>it would be released when it is ready</li> <li>we do not have a deadline for this</li> <li>we have a branch internal that we rebase</li> <li>it would be upstream with a build flag</li> <li>so that we don't break anything and distributors do not ship it until it is ready</li> </ul>"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2023.html#questions_4","title":"Questions","text":"<ul> <li>N/A</li> </ul>"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2023.html#gstreamer-webrtc-status","title":"Gstreamer WebRTC Status","text":""},{"location":"Other/Contributor%20Meetings/ContributorMeeting2023.html#slides_4","title":"Slides","text":""},{"location":"Other/Contributor%20Meetings/ContributorMeeting2023.html#notes_5","title":"Notes","text":"<ul> <li>Brief summary of the timeline (libwebrtc, \u2026)</li> <li>2015-17 \u2026</li> <li>2017-2019: LibWebRTC attempts</li> <li>Bundled in WebKit for Apple WebKit pots.. Integration with GStreamer Encoders, Hardware accelerated support requiring custom decoder/sinks</li> <li>2019 onwards: New GstWebRTC announced at GSTConf</li> <li>2017, and the initial experimentation which we upstreamed a backend for in 2022</li> <li>Screen capture when you want to capture mic or video. In WebKit that takes a permission from UIProcess to WebProcess</li> <li>We differ a bit form the apple ports in that I think apple does it in the GPU process</li> <li>we had to design from RPC from the webkit process to the ui process</li> <li>that means it\u2019s not easily testable right now</li> <li>we have some infra for this, it breaks now and then, it\u2019s not that hard to fix but you have to watch it</li> <li>one pipeline per capture device</li> <li>Stream capture (getDisplayMedia()) - we have two layers of permissions</li> <li>in linux we have the webkit permission request + desktop portal - we have to talk to it over IPC</li> <li>The desktop portal can remember previous permission requests so perhaps we can improve that</li> <li>then we have another pipeline which relies on pipewire, it\u2019s deplohyed on linux desktops nowadays and it can captire - it provides DMABufs</li> <li>for MediaStream handling - when you need to capture, in the gstreamer ports we have a plugin that is able to feed the popline with data coming from the media stream</li> <li>we use that quite a lot for handing incoming streams, canvas/video/capture</li> <li>{shows a diagram showing a simplified version}</li> <li>webrtcbin implements the JavaScript SEssion Establishment Protocol, the integration with webkit was quite easy - GstPromis is hand in hand with webkit\u2019s Promise <li>When you receive a media stream, in javascript you will usually connect that to a video element (as .src) - in webcore it will internally hook to webkitmediastreamsrc -</li> <li>this all means that we use the same thing for normal playback of video, etc \u2014 which I think is different form apple ports again</li> <li>additional APIs: Can\u2019t do everything that is required of the spec, some we did upstream adds \u2014 we had to do some plumbing to provide frames-encoded, bitrate, etc</li> <li>also with DataChannel - we had to do some patches in Gstreamer, but now it is integrated quite well</li> <li>we are passing ~60% of the tests, but that isn\u2019t WPT</li> <li>there is still work to do obviously, but we\u2019re getting there</li> <li>we\u2019re also doing dedicated bring ups of specific platforms - we focuesed on gaming with amazon luna - we followed kind of the chrome approach, I tried with a safari UA but it doesn\u2019t work.. So we have to send the chromium UA and rely on some legacy RTC\u2026</li> <li>we have a pull request, it\u2019s not been reviewed yet</li> <li>we had some issues with the service expecting some chrome specific specs - we provided some shim implementations of those to just allow us to move forward - it\u2019s not great</li> <li>another one we\u2019re bringing up is jitsi \u2014 it\u2019s unfortunately not working yet.</li> <li>it requires features we don\u2019t yet support: renegotiation, simulcast, etc</li> <li>Takeaway: Every platform requires a dedicated bring up. We can have spec compliance, but every one requires more\u2026 Being spec compliant is just the tip of the iceberg for webrtc</li> <li>re: encoding - we have to provide for simulcast - GSTWebRTC has some support but it\u2019s not used by WebKit yet\u2026</li> <li>there are 2 types of scalability</li> <li>re: Privacy and security\u2026 All of our pipelines run in the WebProcess, we can\u2019t send that into production, that\u2019s bad</li> <li>ideally the plan on the roadmap is to move that to a restricted NetworkPRocess</li> <li>etc</li> <li>another deals with capture devices - same issue in the WebProcess\u2026 tentative plan is to improve permissions/portal on GTK/Desktop. We\u2019ll have to figure it out on embedded where user input could be lacking. Then we\u2019d like to move all of the GStreamer pipelines to the GPU process</li> <li>Then we have a missing features backlog:</li> <li>ICE candidates filtering</li> <li>SFrame encryption</li> <li>Improve stats coverage</li> <li>Tranceiver direction changes</li> <li>DTMF?</li>"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2023.html#questions_5","title":"Questions","text":"<ul> <li>How do I accelerate video? are there licensing issues?</li> <li>on adb platforms the support is provided by third party provider GCMA plugin that you can use so it really depends on each case</li> <li>second question: You mentioned the keyboard for cloud gaming. Did you have any plan to upstream that implementation?</li> <li>I dont think there are positive signal from apple at this point. WE could provide an implementation but it would have to be disabled by default because there were some issues related with that spec that we\u2019ve not addressed yet</li> <li>Indeed</li> <li>The question was about keyboard-lock, for the record</li> </ul>"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2023.html#webkitgtk-and-wpe-ports-sdk","title":"WebKitGTK and WPE ports SDK","text":""},{"location":"Other/Contributor%20Meetings/ContributorMeeting2023.html#slides_5","title":"Slides","text":""},{"location":"Other/Contributor%20Meetings/ContributorMeeting2023.html#notes_6","title":"Notes","text":"<ul> <li>flatpak was our previous attempt</li> <li>it added too much complexity in WebKit's tooling</li> <li>it's immutable sandbox prevents working on system libs</li> <li>the SDK image was too complex to produce</li> <li>flatpak is mostly for desktop apps</li> <li>new attempt is a container based on Ubuntu 23.04</li> <li>goal, be out of the way as much as possible</li> <li>provides scripts to interact with the podman container</li> <li>3, commands, wkdev-create, wkdev-enter, and then in the new shell, usual scripts like build-webkit can be used</li> <li>no modification required in WebKit's tooling</li> <li>SDK provides support for distributed builds with sccache</li> <li>also VSCode integration</li> <li>also NVIDIA tooling, and scripts to build system libs using jhbuild</li> <li>the plan is to deploy this SDK in EWS bots (no timeline yet)</li> <li>the SDK source will be soon published online</li> <li>Github Codespaces could be used for WPE development from the browser as well</li> <li>Could be used in VMs as well, on macOS for instance</li> </ul>"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2023.html#questions_6","title":"Questions","text":"<ul> <li>N/A</li> </ul>"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2023.html#status-of-lbse","title":"Status of LBSE","text":""},{"location":"Other/Contributor%20Meetings/ContributorMeeting2023.html#slides_6","title":"Slides","text":""},{"location":"Other/Contributor%20Meetings/ContributorMeeting2023.html#notes_7","title":"Notes","text":"<ul> <li>Presenting challenges, current status, and roadmap</li> <li>Started working on KHTML since 2001</li> <li>LBSE is a new SVG engine in WebKit, utilizing the layer tree</li> <li>\u2026which used to be inaccessible to SVG, leading to a long trail of bugs</li> <li>LBSE is meant to integrate SVG, HTML, and CSS</li> <li>Trying to embed HTML into SVG caused some really nasty bugs; LBSE should resolve these</li> <li>Developed by Igalia and funded by Igalia, Vorwerk, and now WIX</li> <li>We want to re-use the WebKit codepaths that give us hardware acceleration, particularly perspective transformations</li> <li>The paths are presently mutually exclusive</li> <li>When we started, only SVG had transformations and other features CSS has picked up over the years from SVG</li> <li>SVG rendering stayed as it was while HTML+CSS was developed</li> <li>So the only reasonable way to update is to remove the old SVG engine and replace it with a better one, like LBSE</li> <li>Performance was about on par, with a 2-4% drop on MotionMark</li> <li>Some individual tests were much faster than before, others regressed</li> <li>How is this achieved?</li> <li>SVG is allowed to participate in the layer tree</li> <li>Remove SVG-only transform, clipping, marker, masking, filter code</li> <li>Rework SVG\u2019s render tree to be accessible by CSS</li> <li>Re-use as much code in RenderLayout as possible</li> <li>The more we use RenderLayout, the more things we get for free (z-index, etc.)</li> <li>LBSE was designed as a drop-in replacement for the old SVG engine</li> <li>This wasn\u2019t well-received; too much change too quickly</li> <li>So we had to rename the entire legacy engine so it could live in parallel with LBSE</li> <li>Last we checked, the LBSE patch is about 40MB</li> <li>We provide a compile-time flag allowing to pick between engines</li> <li>Issue was opened in 2021, hadn\u2019t been finished yet</li> <li>Overall progress status is around 77%</li> <li>The main thing missing is resources</li> <li>Many patches landed since last year\u2019s WKCM</li> <li>November 2022, we landed a patch that unified geometry and transform computations with CSS</li> <li>The key for performance is to bypass WebCore</li> <li>Access already-composited images and move them instead.</li> <li>This let us finally switch on accelerated transforms, this year</li> <li>That patch landed June 2023</li> <li>We still had repainting issues; if you animated inside an SVG with CSS, looking at it Web Inspector caused a lot of repaints</li> <li>This was patched out October 2023</li> <li>We started to upstream resources in September 2023</li> <li>In SVG, resources are clipping, masks, filters, etc.</li> <li>In our next steps, we want to finish upstreaming, including many patches adding missing layout tests</li> <li>Resource invalidation is broken in WebKit since forever</li> <li>(shows example, slide 20/32)</li> <li>How invalidations actually work: if you change a circle radius to 20, the clipper has to notify the masker that the rect it\u2019s using has changed</li> <li>Before, we handled this via layout</li> <li>So changes to radius would force a re-render of the mask</li> <li>Do it in the right order, and you could get lucky with correct invalidation</li> <li>Get them out of order, and invalidation wouldn\u2019t happen properly</li> <li>We suffered a lot from unnecessary repaints and relayouting as a result of this</li> <li>(shows example, slide 22/32)</li> <li>Here\u2019s a typical code graph</li> <li>At some point, we\u2019ll tell core the layout is dirty</li> <li>We have to invalidate it</li> <li>Afterwards, something is marking it for invalidation. Why?</li> <li>At some point in this recursive calling, the element gets a style change notification</li> <li>We have to mark the user of that clip path and \u201cneeds layout\u201d</li> <li>Resources in the chain are notified</li> <li>If you process in the right order but swap elements, then after layout, things are still dirty and so layout isn\u2019t finished when we think it is</li> <li>We think we can avoid this mess if we stop abusing layout</li> <li>All this is a fundamental flaw we need to avoid</li> <li>Invalidation depends on element order, and it should not</li> <li>Our plan: LBSE will do masking and clipping statelessly</li> <li>No more temporary buffers or caching of individual render objects</li> <li>This is being implemented via RenderSVGResourceContainer, a new base class, which landed this month (Oct 2023)</li> <li>The next target after that is the resource clipper</li> <li>We\u2019re currently rewriting the whole of the resource invalidation logic</li> <li>The SVG resource cache will be moved to LegacySVGResourcesCache</li> <li>Mid-term plans: finish support for , then add , linear and radial gradients, , and  <li>All of those become trivial after finishing with  <li>First basic implementations should be done this year</li> <li>Long-term plans: finish LBSE, such that all layout tests pass and are not slow</li> <li>We have to reduce the core RenderLayer overhead and cache more SVG subtree information</li> <li>Also selectively construct render layers only when necessary</li> <li>Target for switching over LBSE by default and remove the legacy engine is 2024</li> <li>Igalia is working with all the power we can, getting everything we have upstream so we get everything upstream though maybe not as performant as legacy</li> <li>Then we will work with Apple to reach performance targets and equal or exceed the legacy engine in performance</li> <li>MotionMark is a good example of something that doesn\u2019t benefit from the new engine</li> <li>On other tests, LBSE is outperforming the legacy engine by orders of magnitude</li>"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2023.html#questions_7","title":"Questions","text":"<ul> <li>Sayeed: you mentioned you were going to get rid of the buffer. HOw can you make performance faster without those?</li> <li>Nikolas: The idea is you don\u2019t have to paint those things at all</li> <li>\u2026If there is something where we have to do more work, we\u2019ll do that, but this is no difference than HTML or CSS</li> <li>???: You said MotionMark is testing something that doesn\u2019t benefit, so you\u2019re using it just to make sure you\u2019re not regressing</li> <li>\u2026Do any existing benchmarks that show the gains here, or something you use?</li> <li>Nikolas: We don\u2019t have anything public yet</li> <li>\u2026It would be good to put such a benchmark out for people to see</li> <li>???: I wonder if you would see  benefits in other existing benchmarks</li> </ul>"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2023.html#vertical-form-controls","title":"Vertical Form Controls","text":""},{"location":"Other/Contributor%20Meetings/ContributorMeeting2023.html#slides_7","title":"Slides","text":""},{"location":"Other/Contributor%20Meetings/ContributorMeeting2023.html#notes_8","title":"Notes","text":"<ul> <li>Form controls where writing mode is specified as vertical-rl or vertical-lr</li> <li>Results in an alternate CSS block model form</li> <li>Motivation: lack of internationalization, interop 2023 form focus area (WK has lowest score)</li> <li>Implementation: not as simple as specifying writing mode</li> <li>Example: the progress bar, which is forced to have horizontal style even with vertical mode</li> <li>We need to make sure we are using CSS logical properties, updating our user agent stylesheet</li> <li>Layout: reimplementing custom renderers, baseline adjustment (custom baselines updated to work with vertical text)</li> <li>Any port can detect vertical writing mode by checking render style or control style state</li> <li>Implementation: Rendering: Rotation</li> <li>we simply rotate the control to match the writing mode</li> <li>Implementation: Rendering: Logical Coordinates:</li> <li>transpose rects, sizes, and coordinates</li> <li>Demo of select multiple</li> <li>Complete rewrite of the select control to support horizontal scrolling, and keyboard selecting, and scroll-left property. All ports benefit from this</li> <li>Enabling this in ports: VerticalFormControlsEnabled setting (planning on enabling in macOS and iOS)</li> <li>Still some small rendering issues to address in the short term, and in the long term, support for pop-up/native UI</li> </ul>"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2023.html#questions_8","title":"Questions","text":"<ul> <li>Q: Do other browsers also not rotate the checkbox?</li> <li>A: No browsers rotate checkboxes, radio buttons, etc.</li> </ul>"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2023.html#masonry-layout","title":"Masonry Layout","text":""},{"location":"Other/Contributor%20Meetings/ContributorMeeting2023.html#slides_8","title":"Slides","text":""},{"location":"Other/Contributor%20Meetings/ContributorMeeting2023.html#notes_9","title":"Notes","text":"<ul> <li>Jen Simmons gave an overview of Masonry layout at WWDC.</li> <li>Grid: consists of rows and columns. This might leave a lot of space in individual cells.</li> <li>Masonry: solves this by getting rid of rows.</li> <li>[Shows example of 7 sibling elements where the first three elements are next to each other on the first conceptual row. The elements after that are put in the shortest subsequent conceptual rows.]</li> <li>Worked on updating the specification to finalize it. Worked on Align Tracks &amp; Justify Tracks and because there was a lot of complexity and a11y issues these were removed.</li> <li>Intrinsic Track Sizing Changes: we used to use the first conceptual row to determine the sizes. That made it hard for web developers to use.</li> <li>In the new specification all elements participate in determining the track size.</li> </ul>"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2023.html#questions_9","title":"Questions","text":"<ul> <li>N/A</li> </ul>"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2023.html#speedometer-3","title":"Speedometer 3","text":""},{"location":"Other/Contributor%20Meetings/ContributorMeeting2023.html#slides_9","title":"Slides","text":""},{"location":"Other/Contributor%20Meetings/ContributorMeeting2023.html#notes_10","title":"Notes","text":"<ul> <li>Used to optimize and compare browser perf</li> <li>1.0 in 2014 by apple</li> <li>2.0 2018 collab with Apple and Google</li> <li>1.2 in 2022</li> <li>2.0 was mostly updating frameworks, not much to runner or test harness</li> <li>... but it's often considered one of the most popular benchmarks</li> <li>Speedometer 3</li> <li>open governance model</li> <li>collab between apple, google, mozilla</li> <li>update frameworks, but also include other types of content</li> <li>instead of contributions governed by webkit contribution policies, based on consensus model</li> <li>updated framework list, new (web components, svelte, lit). retired (ember, inferno, elm, flight)</li> <li>New News sites tests (Next.js, Nuxt.js), emulating a website like cnn</li> <li>Editor test cases for rich text and code</li> <li>Charting (observable plot, chart.js, stockcharts, perf dsashboard)</li> <li>working for a bit over a year</li> <li>webkit optimizations. improvements starting July of this year</li> <li>continuing until early next year for scheduled release (spring 2024)</li> </ul>"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2023.html#questions_10","title":"Questions","text":"<ul> <li>question: when running speedometer on browserbench, there's an effect of testing the end users connection. is there work being done to avoid that?</li> <li>answer: we'd have to look into that</li> <li>q: anything you'd like to get into a future version, or another benchmark?</li> <li>answer: biggest one is async tasks for promises / workers</li> <li>q: is it correct that charting is based on SVG?</li> <li>a: yes, some are using SVG and others are using Cavnas</li> <li>q: how do you select frameworks?</li> <li>for the network speed question, filing a GitHub issue on the Speedometer 3 repo is a great way to record that feedback</li> <li>a: we look at data based on what's being used in real websites and use currently prominent</li> <li>or those that appear to be becoming more prominent</li> <li>@Nikolas Zimmermann we have some some SVG optimizations (mostly attributeChanged stuff) based on speedo3</li> <li>the two charting SVG tests are Observable Plot and React Stockcharts SVG. ChartJS and Perf Dashboard are canvas</li> <li>there are SVGs scattered throughout other tests (like icons in editor toolbar), though I wouldn't expect much of that to get measured</li> </ul>"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2023.html#all-fonts-feature-proposal","title":"All Fonts Feature (Proposal)","text":""},{"location":"Other/Contributor%20Meetings/ContributorMeeting2023.html#slides_10","title":"Slides","text":""},{"location":"Other/Contributor%20Meetings/ContributorMeeting2023.html#notes_11","title":"Notes","text":"<ul> <li>recent contributor to web inspector. I have a lot experience as a front end engineer</li> <li>Today I'm going to be presenting a \"all fonts\"  feature for the web.</li> <li>Frances makes a joke about bad font usage.</li> <li>Some fonts are not appropriate for.</li> <li>The New York Time uses a particular font to convey their identity.</li> <li>I created a site with different font for each word.</li> <li>How can a web dev find out which font is being used for each word?</li> <li>You can do that by inspecting each element?</li> <li>You can look at font-family under style... but you can't know which one has been actually selected. It's unclear.</li> <li>And then you have Times New Roman, but it's a default so it's still confusing.</li> <li>You can also look at the CSS itself to see what is being applied being ids</li> <li>and via classes</li> <li>Your last option is to go to Firefox and their dev tools. But that still doesn't tell you which one WebKit chose.</li> <li>Proposals: what is you had a way of seeing which font is selected via a new fonts tab.</li> <li>Challenges: what if you have a slide show, for instance, and each slide is using a different font, or scripts are changing things, and what if the computed style is different for different devices?</li> <li>Right now, I've managed to get the right information to show in console.log().</li> </ul>"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2023.html#questions_11","title":"Questions","text":"<ul> <li>Question: can you track which font is being used by an element?</li> <li>Right now I would just present all the fonts computed for an element on a web page</li> </ul>"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2023.html#playwright-and-the-state-of-modern-e2e-testing","title":"Playwright and the state of modern e2e testing","text":""},{"location":"Other/Contributor%20Meetings/ContributorMeeting2023.html#slides_11","title":"Slides","text":""},{"location":"Other/Contributor%20Meetings/ContributorMeeting2023.html#notes_12","title":"Notes","text":"<ul> <li>Will talk about capabilities, trends, challenges etc</li> <li>the team started at Google, worked on web inspector before fork, chrome devtools after fork</li> <li>later node.js debugging</li> <li>at some point puppeteer emerged as browser automation interface</li> <li>later switched focus to web testing</li> <li>playwright test example, similar to layout test, isolated environment</li> <li>see test execution step by step in the trace viewer</li> <li>unique features: single command installation</li> <li>works on 3 oses, in cloud, headed and headless</li> <li>whitebox instrumentation of the browser, network interception, ignore tls, workers etc</li> <li>reliable tests is a priority</li> <li>each test runs in clean, isolated environment (new ephemeral context per test)</li> <li>parallel test execution</li> <li>chromium, webkit, firefox are supported out of the box</li> <li>Linux dominates CI</li> <li>as it is cheaper in cloud</li> <li>Mostly Node.js, with python distant second</li> <li>also supprot .net and java</li> <li>download stats for major players in the space</li> <li>cypress: tests run within page, oopifs not supported</li> <li>great user experience makes it the leader</li> <li>puppeteer: out of process CDP client of Chromium, general puprose API for the browser</li> <li>playwright: upwards trajectory, relatively new</li> <li>selenium-webdriver: steady download rate</li> <li>each tool uses its own way to connect to the browsers</li> <li>only playwright is cross-browser</li> <li>default browsers download stats for playwright installations</li> <li>firefox and webkit have a big share each</li> <li>wholistic approach to the integration: from browser instumentation to retries</li> <li>responsible for all the bugs, no finger pointing to other components, if there is a bug in browser protocol it is considered a bug in playwright</li> <li>browser fixes either upstream or downstream</li> <li>in chromium everything is upstream</li> <li>webkit: wincairo, linux wpe(headless) and gtk (headed), macos</li> <li>support headless execution on all platforms</li> <li>implemented as web inspector agents</li> <li>the protocol is reach-enough, talks about all the entities we need</li> <li>couldn't find another protocol that provide comparable capabilities</li> <li>when testing how web site works on mobile safari we need to emulate touch</li> <li>the code is ios specific</li> <li>same for fixed layout emulation</li> <li>taking screenshots after compositing is a challenge across platforms</li> <li>same problem in web driver</li> <li>network stack is different across platfors</li> <li>no good rationale for upstreaming web inspector protocol changes as many of them are not directly usable by web inspector front-end</li> </ul>"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2023.html#questions_12","title":"Questions","text":"<ul> <li>N/A</li> </ul>"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2023.html#standards-positions-page-and-contributing","title":"Standards Positions Page and Contributing","text":""},{"location":"Other/Contributor%20Meetings/ContributorMeeting2023.html#slides_12","title":"Slides","text":""},{"location":"Other/Contributor%20Meetings/ContributorMeeting2023.html#notes_13","title":"Notes","text":"<ul> <li>there is a repo on GitHub that WebKit uses to be asked and provide feedback for web standards</li> <li>intended to be separate from actual implementation</li> <li>sometimes have to implement things WebKit doesn't like</li> <li>ideally allows for evaluation of standards with a more principled/conceptual viewpoint</li> <li>collaboration can be async</li> <li>to request a position, discuss with colleagues, post a comment asking, give a deadline, and (usually) WebKit will do something before then</li> <li>there's also a basic website that visualizes the state of the standards position repo on GitHub with some extra features (e.g. search, filtering, etc.)</li> <li>would love to have more contributors.  often only Apple employees</li> <li>could use help with triaging of applying labels to issues.  some is automated, but not all.  all contributors should be able to do so</li> <li>room for improvement in tooling and the website</li> </ul>"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2023.html#questions_13","title":"Questions","text":"<ul> <li>Q: opinions from real use cases often are missing from standards positions</li> <li>A: more of this would definitely be useful.  there are a few cases where this has happened, but not often.  could be that the web developers themselves arent involved.  WebKit should encourage them to</li> <li>Q: are there any examples where there's disagreement on standards positions?  there might be a little bit of hesitancy around \"internal\" disagreement stopping momentum of standards work</li> <li>A: there are a few issues with disagreement from the wider web, but not often from within WebKit.  possibly the mailing list used to have more \"internal\" disagreement.  there is the #standards/#standards-github slack channels for less public discussion if you'd prefer to \"poll the room\" or verify opinions or etc. before posting publicly</li> </ul>"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2023.html#implementing-the-webassembly-gc-proposal-in-jsc","title":"Implementing the WebAssembly GC proposal in JSC","text":""},{"location":"Other/Contributor%20Meetings/ContributorMeeting2023.html#slides_13","title":"Slides","text":""},{"location":"Other/Contributor%20Meetings/ContributorMeeting2023.html#notes_14","title":"Notes","text":"<ul> <li>What is this proposal and why do you are?</li> <li>Wasm is a target language for compiling web programs. It\u2019s an alternative to JS where you want to transpile</li> <li>it\u2019s been used for a number of interesting applications - bringing photoshop to the web, things that use plugins written this way - flight simulator, games</li> <li>you might wonder: is that low level stuff all it can be used for?</li> <li>wasm - can it be used by language that use GC?</li> <li>Kludgy solutions, but Key missing piece was is some kind of allocatable memory - that\u2019s what this GC proposal is for</li> <li>it has datatypes like structs and arrays</li> <li>and also new kinds of reference types - you get new type casts and advanced types from this</li> <li>at least in the browser context it is designed to take advantage of the built in already</li> <li>(shows a concrete example with type declaration)</li> <li>(and globals initialized with new types)</li> <li>(and writing operations which access that data)</li> <li>(and new reference types)</li> <li>there are a bunch more features, ut this is a high level overview of what it is</li> <li>let\u2019s talk about the implementation progress in JSC</li> <li>most of the features of the proposals are already implemented</li> <li>but it\u2019s not in a shippable state - a few more months \u2014 bulk array operations aren\u2019t implemented, misc missing instructions \u2014 it needs much more testing and optimization but look how much (a lot) is already implemented</li> <li>Basically: It\u2019s well underway and hopefully we can get it to shipping soon.</li> <li>The big takeaway is that this is an exciting time for web assembly \u2014 this proposal enables WASM to be used for a whole bunch more languages.  It\u2019s stage 4 pretty much the last stage \u2014 it\u2019s shipped in other browsers</li> <li>it enables targeting wasm for Java, OCaml and many others</li> </ul>"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2023.html#questions_14","title":"Questions","text":"<ul> <li>N/A</li> </ul>"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2023.html#enabling-webassembly-and-ftl-jit-on-windows-port","title":"Enabling WebAssembly and FTL JIT on Windows Port","text":""},{"location":"Other/Contributor%20Meetings/ContributorMeeting2023.html#slides_14","title":"Slides","text":""},{"location":"Other/Contributor%20Meetings/ContributorMeeting2023.html#notes_15","title":"Notes","text":"<ul> <li>Hello everyone</li> <li>wasm work on windows</li> <li>ask questions about recourse center</li> <li>why improve the windows port?</li> <li>firstly, playwright gives people experience to test webkit on windows</li> <li>second, bun uses webkit windows port</li> <li>good for port alignment</li> <li>compared to gtk or wpe, different features on win</li> <li>would be great for the project to get cross-platform desktop apps working, currently corned by Electron</li> <li>which means Chromium</li> <li>other projects are using the system webview for this</li> <li>so webkit limited on windows limits many developers from choosing this</li> <li>more projects want to bring a browser to windows</li> <li>started by working on signal handlers on windows</li> <li>has landed in may, using exception handlers</li> <li>next step: enable low level interpreter</li> <li>currently has an open PR, uses some dll magic</li> <li>PR is open for feedback!</li> <li>current status roughly matches Safari's support for wasm</li> <li>with exception of JIT</li> <li>got JIT working to the point of supporting ARES-6</li> <li>there are some known issues left</li> <li>as a bonus, will unlock BBQ and OMG JIT</li> <li>ARES-6 shows a performance improvement with JIT</li> <li>next steps:</li> <li>land wip PRs</li> <li>work on static asserts, which will make it easier to enable some features</li> <li>more work needed on performance</li> <li>there is a large discrepancy between windows port performance and mac port</li> <li>cross-compilation, potentially running under wine</li> <li>interested in getting paid for this work, reach me on slack/email if interested!</li> <li>thanks to everyone who helped along the way!</li> </ul>"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2023.html#questions_15","title":"Questions","text":"<ul> <li>N/A</li> </ul>"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2023.html#shipping-web-apis-to-customers","title":"Shipping Web APIs to Customers","text":""},{"location":"Other/Contributor%20Meetings/ContributorMeeting2023.html#slides_15","title":"Slides","text":""},{"location":"Other/Contributor%20Meetings/ContributorMeeting2023.html#notes_16","title":"Notes","text":"<ul> <li>It's important to get APIs right on the web the first time \u2014 you can't unship something. Once web developers start using it, it stays</li> <li>There's no version 2 of the web. \"there's web, and there's more web\". It's an additive platform</li> <li>\"Web 2.0 isn't really a thing. There's web, and then there's more web.\" Classic</li> <li>Consider developer experience when designing API. Is it easy / intuitive? How consistent is it with other APIs? How does it interact with other APIs? Does the API have unexpected behavior for end users?</li> <li>Providing feedback on WIP (or complete) specs is really helpful for spec editors \u2014 please provide feedback</li> <li>As implementors, this is part of the job, not just translating spec to code</li> <li>When building web API, building demos (which are different than testcases) is helpful. Show how developers will actually use the API. Show what the end user experience would be like.</li> <li>This helps communicate the value of what you're building</li> <li>Building demos help discover flaws in your API design</li> <li>Step 2: Test your implementation</li> <li>Web platform tests are great and should be part of the process, but you should determine if they match the spec, and whether they are exhaustive. Are all cases covered?</li> <li>Example: for layout, there are sometimes lots of edge cases not covered, like putting something in too small of a container</li> <li>Add a feature flag</li> <li>See UnifiedWebPreferences.yaml</li> <li>These are recently overhauled, with documentation available to help guide usage</li> <li>Measure the quality of your implementation<ul> <li>WPTs</li> <li>Interop</li> <li>Performance and power</li> <li>How well do your demos work?</li> </ul> </li> <li>Test in STP, not just minibrowser. Things like Autofill can introduce surprising interactions not present in Minibrowser</li> <li>Test on iOS (and work with Apple if needed)</li> <li>Ensure spec + implementations are aligned. If implementations are not aligned, and nobody speaks up, users and developers suffer</li> <li>Test your demo</li> <li>You should strive to ship three things:<ul> <li>Feature</li> <li>Tests</li> <li>Spec changes / improvements</li> </ul> </li> <li>How to toggle feature flags in Safari?<ul> <li>Enable debug menu (docs online to do this)</li> <li>Go to Feature Flags section, should find list of flags with toggles</li> </ul> </li> <li>Some possible follow-ups after your feature ships:<ul> <li>Tools for web developers (e.g. in Web Inspector)</li> <li>Tools for WebKit debugging for WebKit developers</li> </ul> </li> </ul>"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2023.html#questions_16","title":"Questions","text":"<ul> <li>Demos are not only useful for guiding your feature development, but also for evangelism and helping teach developers about new web technology. You can ping @Jen Simmons or @jond if you have something you feel is worth showing</li> <li>Q: Is there a place where we can put such demos on webkit.org?</li> <li>A: webkit.org/demos. Also often embedded into webpages / posts / release articles. Reach out to Jen or Jon for help</li> <li>Q: What if the spec is under incubation? Would it be good to start upstreaming implementation? i.e. the spec isn't ready yet, changes are still happening</li> <li>A: It depends. How much is the spec expected to change?</li> <li>As a concrete example, CSS grid went through many iterations. There was an initial implementation from Microsoft that helped guide improvements. Once spec eventually stabilized, more implementors came on board</li> <li>Having the early implementation was really helpful in this case</li> <li>Different implementors might interpret specs differently, which can help expose ambiguity, ultimately creating a stronger web platform for everyone (edited)</li> </ul>"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2023.html#quirks-kintsugi-for-the-web","title":"Quirks: Kintsugi for the Web","text":""},{"location":"Other/Contributor%20Meetings/ContributorMeeting2023.html#slides_16","title":"Slides","text":""},{"location":"Other/Contributor%20Meetings/ContributorMeeting2023.html#notes_17","title":"Notes","text":"<ul> <li>A lot of things on the web are broken for many reasons, causing a negative user experience. This can be due to websites/libraries targeting a specific browser, a new feature, an old feature being removed, a bug fix that websites rely on, or tracking prevention.</li> <li>These problems can be rectified with UA overrides (lying about who you are, e.g., modifying the user agent or HTTP header), C++ quirks (working at code-level implementation), and hot-fixes on websites.</li> <li>The first browser to push website hotfixes was Opera BrowserJS (pre-Blink) by modifying JS/CSS content after it loaded.</li> <li>Source/WebCode/page/Quirks.cpp is a mostly true/false file which is based on domain and simple heuristics. The actual Quirks logic is elsewhere in the C++ codebase.</li> <li>Example of a quirk in practice: ikea.com experienced an issue where images at the bottom of the page would not load after a WebKit change. WebKit pushed a quirk that performed a specific, different behavior for that specific website. Ikea subsequently patched the issue, at which point the quirk was removed.</li> <li>Quirk implemented by @Brandon at: https://github.com/WebKit/WebKit/pull/6595</li> <li>Quirk removed by @Karl Dubost at: https://github.com/WebKit/WebKit/pull/14058</li> <li>For UA overrides: HTTP User-Agent + navigator.userAgent.</li> <li>Source locations:<ul> <li>GTK: Source/WebCore/platform/glib/UserAgentQuirks.cpp</li> <li>Firefox iOS: https://github.com/mozilla-mobile/firefox-ios/blob/main/Shared/UserAgent.swift</li> <li>Safari (via WebKit):<ul> <li>WebKit User Agent Overrides API</li> <li>See this bug for more information</li> </ul> </li> </ul> </li> <li>Issues:<ul> <li>Quirks aren't clean! They modify the C++ codebase.</li> <li>They're \"all or nothing\", meaning they're either all on or all off.</li> <li>Long delay from coding quirks to their release (user experience suffers) -- outreach takes time and is sometimes unsuccessful.</li> <li>Websites can end up being blocked by a Quirk if/when they want to change.</li> <li>We don't always get notified if/when a site is fixed -- is it still needed?</li> </ul> </li> <li>How can we minimize the users' pain?<ul> <li>A simpler configuration (plist/JSON/declarative?)</li> <li>Have special power primitives for webpages (could be catastrophic for security and code injection attacks)</li> <li>Dynamic quirks/UA overrides (ones that deactivate automatically)</li> <li>Allowing WebKit GTK/Firefox iOS to use them?</li> <li>Allow specificity further than domain name</li> <li>Update mechanism independent of releases (Opera BrowserJS)</li> <li>Manual QA/testing suites for removing quirks (e.g., Firefox/Opera)</li> </ul> </li> </ul>"},{"location":"Other/Contributor%20Meetings/ContributorMeeting2023.html#questions_17","title":"Questions","text":"<ul> <li>N/A</li> </ul>"},{"location":"Ports/Introduction.html","title":"What Are WebKit Ports?","text":"<p>WebKit has been ported to various platforms and operating systems. The platform-specific code for each port is maintained by different teams that collaborate on the cross-platform code. The code for upstream ports is maintained directly in the WebKit GitHub repository. The following upstream ports are available:</p> <ul> <li>Apple maintains the WebKit ports for macOS, iOS, and other Apple operating systems.</li> <li>Igalia maintains two WebKit ports for Linux, WebKitGTK and WPE WebKit.</li> <li>Sony maintains the WebKit port for PlayStation.</li> <li>The Windows port facilitates development and testing of WebKit using Windows.</li> <li>The JSCOnly port facilitates development and testing of JavaScriptCore without the rest of WebKit.</li> </ul> <p>There are also several downstream ports of WebKit, which are maintained entirely separately. Because they are not developed upstream, downstream ports are based on different versions of WebKit. Some downstream ports may be old and insecure.</p> <p>There are no cross-platform releases of WebKit. Each WebKit port is responsible for creating their own separate releases, if desired. The same applies for security advisories. Currently the ports maintained by Apple and Igalia have regular releases and security advisories, while other upstream ports do not.</p>"},{"location":"Ports/WindowsPort.html","title":"Windows port","text":"<p>It is using cairo for the graphics backend, libcurl for the network backend. It supports only 64 bit Windows.</p>"},{"location":"Ports/WindowsPort.html#installing-development-tools","title":"Installing Development Tools","text":"<p>Install the latest Visual Studio with \"Desktop development with C++\" workload.</p> <p>Activate Developer Mode. build-webkit script creates a symlink to a generated compile_commands.json.</p> <p>Install CMake, Perl, Python, Ruby, gperf (GnuWin32 Gperf), LLVM, and Ninja. Python 3.12 has a problem for WebKit at the moment. Use Python 3.11.</p> <p>You can use Chocolatey to install the tools. ActivePerl chocolatey package has a problem and no package maintainer now. XAMPP includes Perl, and running layout tests needs XAMPP. Install XAMPP instead.</p> <pre><code>choco install -y xampp-81 python311 ruby git cmake gperf llvm ninja\n</code></pre> <p>Install pywin32 Python module for run-webkit-tests and git-webkit.</p> <pre><code>python -m pip install pywin32\n</code></pre> <p>Windows Git enables <code>autocrlf</code> by default. But, some layout tests files have to be checked out as LF line end style. See Bug 240158.</p> <pre><code>git config --global core.autocrlf input\n</code></pre>"},{"location":"Ports/WindowsPort.html#using-winget","title":"Using WinGet","text":"<p>If you prefer WinGet to Chocolatey, you can use it. Invoke the following command in an elevated PowerShell or cmd prompt.</p> <pre><code>winget install --scope=machine --id Git.Git Kitware.CMake Ninja-build.Ninja Python.Python.3.11 RubyInstallerTeam.Ruby.3.2 ApacheFriends.Xampp.8.2 LLVM.LLVM\nwinget install --id GnuWin32.Gperf\n</code></pre> <p>If <code>--scope=machine</code> isn't specified, Python is installed under your user profile directory.</p> <p>WinGet may not append the path into your PC. If some errors occered, please check your path settings, including LLVM and GnuWin32(Gperf).</p>"},{"location":"Ports/WindowsPort.html#webkit-command-prompt","title":"WebKit command prompt","text":"<p>To compile, run programs and run tests, you need to set some environment variables. For ease of development, it's recommended to create a batch file to set environment variables and open PowerShell. Create a batch file with the following content with adjusting it to your PC. And put it in the top WebKit source directory. And double-click it to open PowerShell, we call this opened shell as \"WebKit command prompt\" hereafter.</p> <pre><code>@echo off\ncd %~dp0\n\npath C:\\xampp\\apache\\bin;%path%\npath C:\\xampp\\perl\\bin;%path%\npath %ProgramFiles%\\CMake\\bin;%path%\npath %ProgramFiles(x86)%\\Microsoft Visual Studio\\Installer;%path%\nfor /F \"usebackq delims=\" %%I in (`vswhere.exe -latest -property installationPath`) do set VSPATH=%%I\n\nrem set WEBKIT_LIBRARIES=%~dp0WebKitLibraries\\win\npath %~dp0WebKitLibraries\\win\\bin;%path%\nset WEBKIT_TESTFONTS=%~dp0Tools\\WebKitTestRunner\\fonts\nset DUMPRENDERTREE_TEMP=%TEMP%\n\nset CC=clang-cl\nset CXX=clang-cl\n\nrem set http_proxy=http://your-proxy:8080\nrem set https_proxy=%http_proxy%\n\nrem You can pass necessary JSC options https://github.com/WebKit/WebKit/blob/main/Source/JavaScriptCore/runtime/OptionsList.h#L83-L607.\nrem set JSC_dumpOptions=1\nrem set JSC_useJIT=0\nrem set JSC_useDFGJIT=0\nrem set JSC_useRegExpJIT=0\nrem set JSC_useDOMJIT=0\n\nrem You can show check TextureMapper FPS via WEBKIT_SHOW_FPS.\nrem set WEBKIT_SHOW_FPS=1\n\ncall \"%VSPATH%\\VC\\Auxiliary\\Build\\vcvars64.bat\"\ncd %~dp0\nstart powershell\n</code></pre> <p>You can replace <code>powershell</code> with <code>cmd</code> or <code>wt</code> (Windows Terminal) if you like.</p>"},{"location":"Ports/WindowsPort.html#building","title":"Building","text":"<p>In the WebKit command prompt, invoke <code>build-webkit</code> to start building.</p> <pre><code>perl Tools/Scripts/build-webkit --release\n</code></pre> <p>You will get required libraries WebKitRequirements downloaded automatically when you perform a <code>build-webkit</code>. It checks the latest WebKitRequirements every time. I'd like to recommend to use <code>--skip-library-update</code> for incremental build to speed up for the next time.</p> <pre><code>python Tools\\Scripts\\update-webkit-win-libs.py\nperl Tools\\Scripts\\build-webkit --release --skip-library-update\n</code></pre> <p>The build succeeded if you got <code>WebKit is now built</code> message. Run your <code>MiniBrowser</code>.</p> <pre><code>WebKitBuild/Release/bin64/MiniBrowser.exe\n</code></pre> <p>You can run programs under a debugger with this instruction.</p>"},{"location":"Ports/WindowsPort.html#building-from-within-visual-studio","title":"Building from within Visual Studio","text":"<p>You can use CMake Visual Studio generator instead of Ninja generator. Install the LLVM extension of MSBuild. It bundles a Clang compiler. But, if the bundled compiler is too old, you might need to set a custom LLVM location and toolset. Instead of creating a Directory.build.props file, you can set LLVMInstallDir and LLVMToolsVersion environment variables.</p> <p>In the WebKit command prompt,</p> <pre><code>perl Tools/Scripts/build-webkit --release --no-ninja --generate-project-only\n</code></pre> <p>Open the generated solution file by invoking devenv command from a WebKit command prompt.</p> <pre><code>devenv WebKitBuild\\Release\\WebKit.sln\n</code></pre> <p>Build \"MiniBrowser\" project.</p>"},{"location":"Ports/WindowsPort.html#running-the-tests","title":"Running the tests","text":"<p>WebKit test runner run-webkit-tests is using a command line debugger NTSD to get crash logs. However, Windows SDK installer doesn't install it by default.</p> <ol> <li>Right-click the Windows start menu</li> <li>Select \"Apps and Features\" menu item</li> <li>Click \"Windows Software Development Kit\" from the apps list</li> <li>Click \"Modify\" button</li> <li>Select \"Change\" and push \"Next\" button</li> <li>Select \"Debugging Tools for Windows\" and proceed the installation</li> </ol> <p>Install XAMPP as described above.</p> <p>Install required Python and Ruby modules.</p> <pre><code>python -m pip install pywin32\ngem install webrick\n</code></pre> <p>If Apache service is running, stop it.</p> <pre><code>net stop apache2.4\n</code></pre> <p>Some extensions need to be registered as CGI. Modify the following commands for your Perl and Python paths, and run them as administrator.</p> <p>An example using Chocolatey</p> <pre><code>reg add HKEY_CLASSES_ROOT\\.pl\\Shell\\ExecCGI\\Command /ve /d \"c:\\xampp\\perl\\bin\\perl.exe -T\"\nreg add HKEY_CLASSES_ROOT\\.cgi\\Shell\\ExecCGI\\Command /ve /d \"c:\\xampp\\perl\\bin\\perl.exe -T\"\nreg add HKEY_CLASSES_ROOT\\.py\\Shell\\ExecCGI\\Command /ve /d \"c:\\Python311\\python.exe -X utf8\"\n</code></pre> <p>An example using WinGet</p> <pre><code>reg add HKEY_CLASSES_ROOT\\.pl\\Shell\\ExecCGI\\Command /ve /d \"c:\\xampp\\perl\\bin\\perl.exe -T\"\nreg add HKEY_CLASSES_ROOT\\.cgi\\Shell\\ExecCGI\\Command /ve /d \"c:\\xampp\\perl\\bin\\perl.exe -T\"\nreg add HKEY_CLASSES_ROOT\\.py\\Shell\\ExecCGI\\Command /ve /d \"\\`\"C:\\Program Files\\Python311\\python.exe\\`\" -X utf8\"\n</code></pre> <p>You need openssl.exe in your PATH to run wpt server. XAMPP contains openssl.exe in C:\\xampp\\apache\\bin directory. Append the directory to your PATH.</p> <p>Open the WebKit command prompt as administrator because http tests need to run Apache service.</p> <p>Invoke <code>run-webkit-tests</code>.</p> <pre><code>python Tools/Scripts/run-webkit-tests --release\n</code></pre> <p>If you are using Japanese Windows, some layout tests fail due to form control size differences. <code>GetStockObject(DEFAULT_GUI_FONT)</code> returns <code>MS UI Gothic</code> on it. Remove <code>GUIFont.Facename</code> of <code>HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\\GRE_Initialize</code>. And, replace <code>MS UI Gothic</code> with <code>Microsoft Sans Serif</code> in <code>HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\\FontSubstitutes\\MS Shell Dlg</code>. Also, change system locale to English, because Python script crashes non-English system locale. See Bug 273060.</p> <p>If http tests fail as flaky failures due to the socket count limit, increase the user port range. See Bug 224523</p> <pre><code>netsh int ipv4 set dynamicport tcp start=1025 num=64511\n</code></pre>"},{"location":"Ports/WindowsPort.html#running-the-tests-in-docker","title":"Running the tests in Docker","text":"<p>You can use Docker to run LayoutTests by mounting the host directory.</p> <pre><code>docker run -it --rm --cpu-count=8 --memory=16g -v %cd%:c:\\repo -w c:\\repo webkitdev/msbuild\n</code></pre>"},{"location":"Ports/WindowsPort.html#downloading-build-artifacts-from-buildbot","title":"Downloading build artifacts from Buildbot","text":"<ul> <li>Go to Windows-64-bit-Release-Build Buildbot builder page.</li> <li>Click any \"Build #\" which is green.</li> <li>Click the \"Archive\" link under \"compile-webkit\" to download the zip</li> <li>Download the corresponding release of WebKitRequirements.</li> <li>Unpack them, copy all DLL of WebKitRequirements to the directory of MiniBrowser.exe</li> <li>Install the latest vc_redist.x64.exe of Microsoft Visual C++ Redistributable for Visual Studio</li> </ul>"},{"location":"Ports/WebKitGTK%20and%20WPE%20WebKit/DependenciesPolicy.html","title":"Dependencies Policy","text":"<p>WebKitGTK's dependencies policy is simple:</p> <ul> <li>We support each major Debian version until one year after the release of the next major version.</li> <li>We support each Ubuntu LTS until one year after the release of the next Ubuntu LTS.</li> </ul> <p>During the support period, we intend for WebKit to remain buildable on these distributions using some system-provided compiler -- not necessarily the default system compiler -- and with the default system libstdc++. The purpose of this policy is to ensure distributions can provide updated versions of WebKit during the support period to ensure users receive security updates.</p> <p>For more information on compiler requirements, see GCC Requirement.</p> <ul> <li>\u200bDebian Releases</li> <li>Debian Packages</li> <li>Ubuntu Releases</li> <li>Ubuntu Packages Search</li> </ul> Operating System Release Date WebKit Support End Ubuntu 20.04 (Focal Fossa) 2020-04-23 2023-04-21 Debian 11 (Bullseye) 2021-08-14 2024-06-10 Ubuntu 22.04 (Jammy Jellyfish) 2022-04-21 2025-04-25 Debian 12 (Bookworm) 2023-06-10 June 2026 Ubuntu 24.04 (Noble Numbat) 2024-04-25 April 2027"},{"location":"Ports/WebKitGTK%20and%20WPE%20WebKit/GCCRequirement.html","title":"GCC Requirement","text":"<p>WebKit is a complex C++ library that often requires relatively new versions of GCC to build. Older versions of GCC tend to have bugs that prevent code from compiling successfully, which tend to be fixed in newer versions of GCC, and which are often encountered in practice by WebKit developers. Apple also likes to be able to use new C++ features, which are often not present in older versions of GCC, relatively early.</p> <p>We are aware that Linux distributions and embedded systems vendors like to be able to compile everything with the same system build toolchain, which may be very old, so we have compromised with Apple to develop a dependencies policy that attempts to balance the interests of both WebKit distributors and developers. We intend to support some system compilers in the latest Ubuntu LTS and Debian stable releases until one year after the subsequent release, but, depending on the age of the release, this might be some version of Clang rather than GCC. We no longer provide any timeline as to how long a particular version of GCC may be supported, as this decision will be made by WebKit developers on a case-by-case basis. However, we will always support the version of libstdc++ available in these releases until one year after the subsequent release. Preserving compatibility with older standard library versions is necessary to allow updates on older systems, even if a newer compiler is used to build those updates. In general, you can expect a new standard library version to be supported by WebKit for approximately three to four years, in accordance with the dependencies policy.</p> <p>This policy means that WebKit may require a new version of GCC sooner than you might prefer. Because Apple does not use GCC to develop WebKit, and because of the relatively high number of compatibility issues caused by supporting GCC, we should be grateful that it is still possible to build WebKit with GCC at all. Unless you are able to upgrade your systems to newer GCCs on a regular basis, it is expected that your system GCC may not be new enough to build WebKit. We urge you to consider building with a different compiler if your system compiler is too old. Embedded systems vendors may choose to deploy WebKit in a container if updating the host's libstdc++ is undesirable. If you choose to stop building new versions of WebKit as a result of an increased GCC or libstdc++ version requirement, your users will be left vulnerable to numerous security vulnerabilities that are fixed in newer versions.</p>"},{"location":"Ports/WebKitGTK%20and%20WPE%20WebKit/Multimedia.html","title":"Multimedia","text":"<p>The WPE and GTK ports depend on the GStreamer multimedia framework for their multimedia-related features, such as video playback (with or without MediaSource Extensions and Encrypted Media Extensions), WebRTC, WebAudio, WebCodecs and MediaRecorder.</p>"},{"location":"Ports/WebKitGTK%20and%20WPE%20WebKit/Multimedia.html#gathering-logs","title":"Gathering logs","text":"<p>GStreamer logs are often useful to help diagnose issues. Depending on the browser used, the procedure might slightly change, but the general idea is to set a few environment variables (mainly <code>GST_DEBUG</code> and <code>GST_DEBUG_FILE</code>) as shown below for a couple runtime scenarios.</p> <p>GStreamer pipeline graph dumps can also be useful for debugging purposes. They can be enabled by setting the <code>GST_DEBUG_DUMP_DOT_DIR</code> environment variable to an existing filesystem folder path.</p> <p>Once gathered, the log file and pipeline graph dumps can be zipped together and uploaded online. Assuming the commands are executed as shown in the next sections, the log file will be <code>$HOME/gst.log</code> and the pipeline graph dumps will be present in the <code>$HOME/dots</code> folder.</p>"},{"location":"Ports/WebKitGTK%20and%20WPE%20WebKit/Multimedia.html#flatpak-apps","title":"Flatpak apps","text":"<p>In this section we take the example of the GNOME Web app, a.k.a. Epiphany.</p> <p>Epiphany runs with its Web content process sandboxed, meaning that filesystem access is restricted, so you need to make sure the GStreamer log file will be located in a folder accessible in read-write mode by the Web content process, using the <code>WEBKIT_DISABLE_SANDBOX_THIS_IS_DANGEROUS</code> environment variable and the <code>--filesystem=home</code> flatpak option.</p> <pre><code>mkdir -p $HOME/dots\nflatpak run --filesystem=home --env=\"WEBKIT_DISABLE_SANDBOX_THIS_IS_DANGEROUS=1\" \\\n    --env=\"GST_DEBUG=3,webkit*:6\" --env=\"GST_DEBUG_FILE=$HOME/gst.log\" \\\n    --env=\"GST_DEBUG_DUMP_DOT_DIR=$HOME/dots\" org.gnome.Epiphany -p \"https://...\"\n</code></pre> <p>GNOME Web has three different flavours. The command above is for the stable version (<code>org.gnome.Epiphany</code>). The Tech Preview application name is <code>org.gnome.Epiphany.Devel</code> and the Canary version is called <code>org.gnome.Epiphany.Canary</code>. So depending on which version you test, the command line will need to be adapted accordingly.</p>"},{"location":"Ports/WebKitGTK%20and%20WPE%20WebKit/Multimedia.html#minibrowser","title":"MiniBrowser","text":"<p>The WPE and GTK ports ship a sample web browser application called MiniBrowser. Its availability might depend on your Linux distro. If you built a development version of the WPE or GTK ports, you can start MiniBrowser as shown below, with the necessary GStreamer environment variables:</p> <pre><code>mkdir -p $HOME/dots\nexport GST_DEBUG=\"3,webkit*:6\" GST_DEBUG_FILE=$HOME/gst.log GST_DEBUG_DUMP_DOT_DIR=$HOME/dots\nTools/Scripts/run-minibrowser --gtk \"https://...\"\n</code></pre>"},{"location":"Ports/WebKitGTK%20and%20WPE%20WebKit/Multimedia.html#layout-tests","title":"Layout tests","text":"<p>When debugging multimedia layout tests on a developer build of the WPE or GTK port, the procedure is similar:</p> <pre><code>mkdir -p $HOME/dots\nexport GST_DEBUG=\"3,webkit*:6\" GST_DEBUG_FILE=$HOME/gst.log GST_DEBUG_DUMP_DOT_DIR=$HOME/dots\nTools/Scripts/run-webkit-tests --gtk --no-retry-failures --no-show-results http/tests/media/video-play-stall.html\n</code></pre>"},{"location":"Ports/WebKitGTK%20and%20WPE%20WebKit/ReleasesAndVersioning.html","title":"Releases and Versioning","text":"<p>WebKitGTK and WPE WebKit follow a 6-month development cycle and the releases for both ports are usually synced.</p> <p>Here is an overview of what the version numbers mean:</p> <ul> <li>Version numbers follow the major.minor.patch numbering scheme.</li> <li>Changes to the major version are very rare and signify considerable architectural changes.</li> <li>The minor version number changes throughout the development cycle and it is possible to identify if a release is stable or not by looking at this number<ul> <li>An even minor version number means the release is stable and ready for production.</li> <li>An odd minor version number means the release is a development (beta) release for testing or preview of new features.</li> </ul> </li> <li>The patch number is incremented for each bugfix release and is just an incremental number.</li> </ul> <p>There are two stable features releases every year (where the minor number is increased to an even number and the patch number is zero), typically in March and September. There may be any number of bugfix releases (where the patch number is increased).</p> <p>Read more about the WPE WebKit release process and versioning scheme.</p>"},{"location":"Ports/WebKitGTK%20and%20WPE%20WebKit/SecurityUpdates.html","title":"Security Updates","text":"<p>Before reading this document, please also learn about WebKit ports and WebKitGTK and WPE WebKit releases and versioning.</p>"},{"location":"Ports/WebKitGTK%20and%20WPE%20WebKit/SecurityUpdates.html#webkitgtk-and-wpe-webkit-security-updates","title":"WebKitGTK and WPE WebKit Security Updates","text":"<p>Developers actively backport security fixes from the WebKit main development branch into the latest stable release.</p> <p>Any stable release may contain security fixes. The concept of \"stable release\" is any release where the minor number is an even number. Developers periodically release security advisories detailing which security issues have been found and which releases were affected. Developers issue these security advisories shortly after creating a new stable release fixing the problem.</p> <p>Developers don't generally backport security fixes for older stable releases. Security updates are only available for the latest stable release branch, i.e. the latest branch with an even minor version number.</p> <p>For more information about the security advisories, see:</p> <ul> <li>WebKitGTK Security Advisories</li> <li>WPE WebKit Security Advisories</li> </ul>"},{"location":"Ports/WebKitGTK%20and%20WPE%20WebKit/SecurityUpdates.html#recommended-practices","title":"Recommended Practices","text":"<p>These are the recommended practices for users that would like to incorporate security and privacy updates from WebKit into their app in a timely manner:</p> <ul> <li> <p>Use the latest stable version of WebKitGTK or WPE WebKit.</p> <ul> <li>Even if a specific stable release doesn't mention that it contains a security fix, it is still a very good idea to update.</li> <li>Stable releases may fix dangerous crashes or issues that may be not tagged as a security issue at the moment of the release.</li> <li>Updating to the latest stable versions of WebKitGTK and WPE WebKit is always recommended: it is the best way of ensuring you are running a safe version of WebKit.</li> </ul> </li> <li> <p>Subscribe to the mailing lists to get notifications about new releases and security advisories.</p> <ul> <li>Security advisories are sent to the port mailing list, so it is recommended to subscribe to it:<ul> <li>WebKitGTK mailing list</li> <li>WPE WebKit mailing list</li> </ul> </li> </ul> </li> <li> <p>Verify the tarballs of the releases.</p> <ul> <li>The release tarballs include checksums and are also signed with PGP signatures.</li> <li>After downloading the release, it is recommended to check the checksums or verify the PGP signature. Verifying the PGP signature is the best way to ensure your download was not compromised.</li> <li>See:<ul> <li>Verifying WebKitGTK releases</li> <li>Verifying WPE WebKit releases</li> </ul> </li> </ul> </li> </ul>"},{"location":"Ports/WebKitGTK%20and%20WPE%20WebKit/SecurityUpdates.html#considerations-when-applying-security-updates","title":"Considerations When Applying Security Updates","text":"<p>Some considerations to take into account when applying security updates:</p> <ul> <li>The WebKitGTK and WPE WebKit API aims to be compatible between minor versions, so if the application was using an older minor version of WebKitGTK or WPE WebKit, it should also run with the newer version of WebKitGTK or WPE WebKit without issues. (Recompiling the application is not needed.)</li> <li>The major version rarely changes, but if it does then you may need to check if the application code still builds and works fine with the new major version. In that case there should be a guide explaining how to port the code of the application.</li> </ul>"},{"location":"Ports/WebKitGTK%20and%20WPE%20WebKit/TipsForMaintainers.html","title":"Tips for Maintainers","text":"<p>Maintaining a WebKit port is a lot of work. Here are some suggestions that may help.</p>"},{"location":"Ports/WebKitGTK%20and%20WPE%20WebKit/TipsForMaintainers.html#watching-the-project","title":"Watching the Project","text":"<p>Bug watching is one of the most important tasks for any software project. All WebKitGTK and WPE WebKit developers should watch the <code>bugs-noreply@webkitgtk.org</code> user in Bugzilla email preferences. This address is automatically CCed on all bugs created in the WebKitGTK or WPE WebKit product. If reporting a bug against other components, you should manually CC this address. This way, you can watch what other developers are doing. Watching bugs can be very time-consuming, so it often makes sense to skim bugs that are of peripheral interest to you, but if you're not watching the shared address at all then it will be impossible to be aware of important bugs and issues of the day and you will not be an effective developer.</p> <p>It's also useful to view saved searches from time to time. From the saved searches preferences in Bugzilla, you can subscribe to searches shared by others, or create your own. Most developers would benefit from subscribing to the GTK/WPE Bugs search, for example, and reviewing it from time to time. Other useful searches there exist for subcategories of bugs, including accessibility bugs, font bugs, multimedia bugs, and networking bugs. Subscribing to searches here will not affect the email you receive; it will just create a link to the search in your Bugzilla footer, where you can manually review the search results from time to time.</p>"},{"location":"Ports/WebKitGTK%20and%20WPE%20WebKit/TipsForMaintainers.html#zero-warnings","title":"Zero Warnings","text":"<p>If there are compiler warnings during the build process, it can be very difficult for developers to notice when programming mistakes introduce new warnings. Ensuring the build remains clean of spammy warnings is therefore very important to avoid missing real warnings that could indicate serious bugs. Because warnings differ based on compiler used, compiler version, build type (debug vs. release), and build options, only you can be responsible for fixing warnings that occur with your particular build configuration. You have three options to fix the warnings: (a) by changing the code so as to no longer trigger the warning (preferred); (b) by suppressing the warning using the <code>IGNORE_WARNINGS_BEGIN()</code> and <code>IGNORE_WARNINGS_END()</code> macros defined in Source/WTF/wtf/Compiler.h, or one of the related macros defined there (the next-best option); or (c) adding compiler flags to disable the warnings to a relevant CMakeLists.txt (as a last resort). For example, the Source/WebKit subproject is built using -Wno-unused-parameter, specified in Source/WebKit/CMakeLists.txt.</p> <p>If you see a new warning appear, please take the time to fix it. The developer who introduced the warning is probably building with a different compiler or build configuration than you are, and other developers will surely appreciate your taking the time to keep the build clean.</p> <p>When a new version of a compiler is released, there will usually be many new warnings. It may take a day or two of effort to make the build warning-free again. This is effort well-spent to ensure the quality of WebKit.</p> <p>Warnings are fatal by default in developer mode since 255961@main. To disable this, use <code>build-webkit --no-fatal-warnings</code> or pass <code>-DDEVELOPER_MODE_FATAL_WARNINGS=OFF</code> to CMake. Warnings should never be fatal for non-developer builds because this would be extremely annoying to users.</p>"},{"location":"Ports/WebKitGTK%20and%20WPE%20WebKit/TipsForMaintainers.html#zero-unreviewed-pull-requests","title":"Zero Unreviewed Pull Requests","text":"<p>You are ultimately responsible for finding reviewers to ensure your pull requests are reviewed, whether by pinging reviewers on Bugzilla, IRC, or email. Nobody else can do this for you. You should examine your open pull requests on GitHub from time to time and ensure you aren't accumulating a backlog of unreviewed pull requests. The ideal size of your request queue is zero pull requests older than a few days.</p>"},{"location":"Ports/WebKitGTK%20and%20WPE%20WebKit/TipsForMaintainers.html#zero-regressions","title":"Zero Regressions?","text":"<p>WebKit has a zero regressions policy, meaning any committer can revert any commit if it's found to introduce a regression. That said, apply common sense. If buildbots are broken, that's an emergency and it makes sense to revert the offending commit now and then think about how to fix the problem later. If WebKitGTK no longer works at all or has suffered some other severe regression, then again, revert the offending commit now and ask questions later. But usually the issue is more minor, and it would make more sense to talk to the developer who introduced the issue before reverting it, or to not revert it at all. Developers usually don't enjoy seeing work reverted, and you won't make friends by reverting commits unnecessarily. It's not unusual for a commit to fix a major issue while also introducing a less-serious issue; it wouldn't make sense to revert a commit in blind adherence to zero regressions if that would reduce the quality of WebKit overall. Generally, cross-platform commits should be reverted only if the regression is severe. Platform-specific WPE/GTK commits can be reverted more aggressively.</p>"},{"location":"Ports/WebKitGTK%20and%20WPE%20WebKit/TipsForMaintainers.html#security","title":"Security","text":"<p>Security is very important for a web engine. As a rule, any issue in which web content can crash WebKit is a security issue. In fact, almost every crash or assertion failure is a security issue. The only crashes which are not security issues are crashes that cannot be triggered by web content, but such crashes are few and far between in WebKit. Fortunately, not all crashes are equally-severe. E.g. a null pointer dereference or a release assert is merely a denial of service issue, whereas a use-after-free or buffer overflow is a code execution vulnerability.</p> <p>There is a saved search in Bugzilla to display open bugs in the Security component, which will be visible to you if you are a member of the WebKit Security Team. However, because almost all crashes are security issues, most security issues are actually reported publicly instead of against the Security component.</p> <p>Also, beware that <code>[ Crash ]</code> expectations are public in our TestExpectations. Crashing layout tests are thus an easy blueprint for attackers to start crafting exploits against WebKit. The acceptable number of crashing layout tests is zero.</p>"},{"location":"Ports/WebKitGTK%20and%20WPE%20WebKit/TipsForMaintainers.html#cve-requests","title":"CVE Requests","text":"<p>Because WebKit developers regularly fix a high volume of crash reports, it would be impractical to request a CVE each time a security issue is resolved. Instead, CVEs are generally only issued for vulnerabilities discovered by third-party security researchers. This is a cynical approach to security advisory, but to request a CVE for every vulnerability would be implausible. Still, we have occasionally requested CVEs for unusually-noteworthy issues. Previous examples have included TLS certificate verification issues, message validation issues in WebKit's IPC framework, or proxy bypass issues where WebKit fails to respect the user's configured proxy settings. To request a CVE for issues that do not affect Apple ports, use MITRE's web form and ignore all the instructions telling you not to use the form and to use other CNAs instead. If you try to get a CVE from another CNA instead of using MITRE's request form, you're just going to waste your time. In particular, do not use the DWF CNA.</p>"},{"location":"Ports/WebKitGTK%20and%20WPE%20WebKit/TipsForMaintainers.html#advisories","title":"Advisories","text":"<p>The time to issue a new security advisory is right after Apple has issued a Safari security advisory. If you are responsible for security advisories, then you need to follow the security-announce@lists.apple.com mailing list to know when it's time for this, in addition to becoming a member of the WebKit Security Team. There is a script to generate advisories in the webkitgtk.org GitHub repo.</p>"},{"location":"Ports/WebKitGTK%20and%20WPE%20WebKit/TipsForMaintainers.html#stable-branches","title":"Stable Branches","text":"<p>WebKitGTK and WPE WebKit share stable branches, maintained in GitHub with names beginning with <code>webkitglib/</code>. Maintaining a stable branch is a lot of work, and deciding which commits to backport is not always easy. Our goal is to backport fixes for bugs without accidentally backporting commits that introduce new bugs, but this is sometimes easier said than done. In general, backporting more commits increases the risk of regressions, so it requires care. In addition to backporting commits proposed for backport on the stable branch wiki page, we've successfully used two different strategies to identify other commits that should be backported.</p>"},{"location":"Ports/WebKitGTK%20and%20WPE%20WebKit/TipsForMaintainers.html#carloss-strategy","title":"Carlos's Strategy","text":"<p>The first strategy, Carlos's strategy, is to simply review all commits to main since the last stable release and backport everything that looks important. This is the most comprehensive strategy to identify as many bugfix commits for backporting as possible, but it's very time-consuming and it's easy to miss important commits even if you are extremely careful and skillful. It can also be difficult to know for sure whether a particular commit is suitable for backporting without a high level of expertise in highly-specialized areas of the codebase. Michael thinks this strategy works better at the beginning of a new release cycle, especially before the .0 or .1 releases.</p>"},{"location":"Ports/WebKitGTK%20and%20WPE%20WebKit/TipsForMaintainers.html#michaels-strategy","title":"Michael's Strategy","text":"<p>Later on in the lifetime of a stable branch, consider switching to Michael's strategy, which focuses on reviewing the commits that are most likely to be important candidates for backporting. Those are (a) commits that were backported to a Safari stable branch, (b) commits associated with resolved security bugs, and of course (c) platform-specific commits.</p> <p>Safari backports are a good place to start because these commits have been identified as good candidates for backporting by Apple developers. Each webkitglib branch has a corresponding Safari stable branch. The corresponding stable branch is one branched shortly before or shortly after a webkitglib branch. For example, for webkit-2.22 the corresponding Safari branch was safari-606-branch. For webkit-2.24, the corresponding Safari branch was safari-607-branch. It's worth examining every commit on the corresponding Safari branch to consider whether it would be a good backport for the webkitglib branch. Most commits backported to Safari stable branches are also good candidates for webkitglib branches, except commits that are Mac-specific or address features that are not yet enabled in the webkitglib branch. For example, WebKitGTK 2.24 does not yet support service workers, WebRTC, EME, or PSON, so fixes for these features should be ignored.</p> <p>Platform-specific commits generally have prefixes like <code>[GTK]</code>, <code>[WPE]</code>, <code>[SOUP]</code>, <code>[FreeType]</code>, <code>[GStreamer]</code>, and <code>[GLib]</code>. These are often important for backporting. Don't rely on developers to request backport when their commit is important; that often doesn't happen.</p>"},{"location":"Ports/WebKitGTK%20and%20WPE%20WebKit/TipsForMaintainers.html#other-backporting-tips","title":"Other Backporting Tips","text":"<p>If a commit from main does not backport cleanly to the webkitglib stable branch, it's possible the corresponding Safari branch commit may backport cleanly, or more easily.</p> <p>Otherwise, if a commit is not backporting cleanly, consider whether it would be advisable to backport other commits from main in order to allow a clean backport. For example, if a bugfix commit depends on a refactoring commit, you should consider backporting the refactoring commit as well. But do so carefully. You have to consider the risk that the refactor will introduce a new bug in the stable branch, versus the risk that you would introduce a bug yourself in trying to backport a commit with conflicts.</p> <p>Whenever a commit doesn't backport cleanly, you should be looking at the revision history of the affected file using trac, or consider doing a blame of the file, to see what has happened to make the backport unclean.</p> <p>JavaScriptCore security fixes often involve very large diffs. Backporting these manually when there are conflicts is often quite risky. Instead, be aggressive in backporting whatever other commits are necessary from main in order to make the security fix backport more cleanly.</p> <p>Always search for the revision number in the git log of the commit you are backporting to see if it is mentioned in other commits. If the revision number is mentioned in subsequent commits, it's probably because the revision introduced a regression. If you forget to check, Murphy's Law guarantees you will backport a commit introducing a known regression.</p>"}]}